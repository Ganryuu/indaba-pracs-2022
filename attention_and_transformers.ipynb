{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Paying Attention to Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0RWJRsNiFHX"
      },
      "source": [
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"40%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/intro-pos_encoding-tokenizing/attention_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [THIS SHOULD STILL CHANGE TO OUR PRAC]\n",
        "\n",
        "Â© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "The transformer architecture, introduced in Vaswani et al. 2017 .'s paper Attention is All You Need, has significantly impacted the deep learning field. It has arguably become the de-facto architecture for complex Natural Language Processing (NLP) tasks; and can outperform benchmarks in various domains, including computer vision and reinforcement learning.\n",
        "\n",
        "Transformers, as the title of the original paper implies, are almost entirely based on a concept known as attention. Attention allows models to \"focus\" on different parts of an input; while considering the entire context of the input versus an RNN, that operates on the data sequentially.\n",
        "\n",
        "In this practical, we will introduce attention in greater detail and build the entire transformer architecture block by block to see why it is such a robust and powerful architecture.\n",
        "\n",
        "**Topics:** \n",
        "\n",
        "Content: <font color='blue'>`Attention mechanisms, Transformers, NLP`</font>  \n",
        "Level: <font color='grey'>`Advanced`</font>\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Learn how different attention mechanisms can be implemented.\n",
        "- Learn and create the basic building blocks from scratch for the most common transformer architectures.\n",
        "- Learn how to train a sequence-sequence model.\n",
        "- Create and train a small GPT inspired model.\n",
        "- Learn how to use the [Hugging Face](https://huggingface.co/) library for quicker development cycles.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Basic understanding of Jax and Haiku\n",
        "- Basic understanding linear algebra\n",
        "- RNN based sequence-sequence models\n",
        "- Token/Word embedding techniques\n",
        "\n",
        "**Outline:** \n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell. \n",
        "#@title Install and import required packages. (Run Cell)\n",
        "\n",
        "!pip install git+https://github.com/deepmind/dm-haiku flax\n",
        "!pip install transformers==4.12.1 datasets \n",
        "!pip install seaborn umap-learn\n",
        "\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "  print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "  print(\"A TPU is connected.\")\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "\n",
        "import haiku as hk\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "\n",
        "import optax\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# download images used in notebook\n",
        "urllib.request.urlretrieve(\n",
        "  'https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80',\n",
        "   \"cat.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions. (Run Cell)\n",
        "\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "  plt.figure(figsize=(20,np.min([8,max_tokens])))\n",
        "  im = plt.imshow(P, aspect=\"auto\", cmap='Blues_r')\n",
        "  plt.colorbar(im, cmap='blue')\n",
        "\n",
        "  if d_model<=64:\n",
        "    plt.xticks(range(d_model))\n",
        "  if max_tokens <=32:\n",
        "    plt.yticks(range(max_tokens))\n",
        "  plt.xlabel('Embedding index')\n",
        "  plt.ylabel('Position index')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "  axes=[]\n",
        "  fig=plt.figure(figsize=(25,25))\n",
        "  for a in range(patches.shape[1]):\n",
        "      axes.append(fig.add_subplot(1, patches.shape[1], a+1) ) \n",
        "      plt.imshow(patches[0][a])\n",
        "  fig.tight_layout()    \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "  import umap\n",
        "  import seaborn as sns\n",
        "\n",
        "  projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "  plt.figure(figsize=(15,8))\n",
        "  plt.title('Projected text embeddings')\n",
        "  sns.scatterplot(\n",
        "      x=projected_embeddings[:,0],\n",
        "      y=projected_embeddings[:,1],\n",
        "      hue=labels\n",
        "  )\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfwoGkW3cLuk"
      },
      "outputs": [],
      "source": [
        "#@title Check what device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5MqDkvKiFHb"
      },
      "source": [
        "[Content on why we need attention, how something like RNN tried to tackle the problem, and how we got to attention.]\n",
        "\n",
        "[Roughly 30 to 35 minutes]\n",
        "\n",
        "[Currently, we still need some math tasks (if we can not think of anything, leave out)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii__Bc27epiJ"
      },
      "source": [
        "### Initial attention mechanisms - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "[Can talk about how an RNN auto-encoder used attention between current states and previous hidden-states, and code up some of those attention mechanisms (such as dot product, additive and multiplicative attention)]\n",
        "\n",
        "[ Can use [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) or something similar for guidance]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaJabAkKvjdI"
      },
      "source": [
        "**Dot product attention**\n",
        "\n",
        "[talk about dot product and how it can be used for attention]\n",
        "\n",
        "[Code up the attention implimentation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OXkT8javogj"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(hidden_state, current_state):\n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cthi-B80v7BQ"
      },
      "source": [
        "**Multiplicative attention (or something else)**\n",
        "\n",
        "[talk about multiplicative and how it can be used for attention and how it adds in learnable parameters]\n",
        "\n",
        "[Code up the attention implimentation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9GrXO07w1w4"
      },
      "outputs": [],
      "source": [
        "def multiplicative_attention(hidden_state, current_state):\n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22cVV00c9ZK"
      },
      "source": [
        "### Self-attention - <font color='blue'>`Intermediate`</font>\n",
        "\n",
        "Talk about how we progressed to self-attention mechanisms and deep dive into scaled dot product attention. Not important yet to know how it fits into MHA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvbKqrRniVDa"
      },
      "source": [
        "#### **Scaled dot product attention**\n",
        "\n",
        "[Deep dive here, with intuition for what query, keys and values can be, why we scale it etc.]\n",
        "\n",
        "[Focus on query, value and key matrices code in MHA section when we build the MHA block of code to be used in transformer models]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRO-fhwmodb6"
      },
      "source": [
        "**Code Task:** Can you code up scaled dot product attention?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNQ8Dgx2Wl8Q"
      },
      "outputs": [],
      "source": [
        "# we need to code up from scratch the function\n",
        "def scd_attention(query, key, value):\n",
        "\n",
        "  # allow then to code up the formula on their own\n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVETJPtRyXDO"
      },
      "outputs": [],
      "source": [
        "# run to test your function\n",
        "\n",
        "def check_scd_attention_function(scd_attention_function):\n",
        "  # we still need to impliment this\n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lDq7CAcAd49i"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "\n",
        "def scd_attention(query, key, value):\n",
        "  emb_dim = query.shape[-1]\n",
        "  logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "  scaled_logits = logits/jnp.sqrt(emb_dim)\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  values = jnp.matmul(attention_weights, value)\n",
        "  return values, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7xAVznfBQU"
      },
      "source": [
        "#### **Masked scaled dot attention** \n",
        "\n",
        "[Talk about how in some cases we are not allowed to see into the future or other inputs, so now we add in the masked attention]\n",
        "\n",
        "[Build upon the function above and add in mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJtf6sCfBQY"
      },
      "source": [
        "**Code Task:** Try and implement the masking operation for your SCD function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LICrxTN5fBQZ"
      },
      "outputs": [],
      "source": [
        "# Code to be implemented during practical\n",
        "def scd_with_mask_attention(query, key, value, mask=None):\n",
        "  # CHANGE ME \n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iE1YNtLkfBQZ"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "def scd_with_mask_attention(query, key, value, mask=None):\n",
        "  emb_dim = query.shape[-1]\n",
        "  logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "\n",
        "  if mask is not None:\n",
        "    logits = jnp.where(mask, logits, -1e30) # same big negative value used in Haiku\n",
        "\n",
        "  scaled_logits = logits/jnp.sqrt(emb_dim)\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  attention = jnp.matmul(attention_weights, value)\n",
        "  return attention, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZe6TDuNfBQa"
      },
      "source": [
        "**Group Task:**\n",
        "\n",
        "- Play with the mask you provide to your function and tell your friend what you see.\n",
        "- Ask your friend if they think it's fair that we don't allow specific models to look into the future using self-attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrT6swYgCm9"
      },
      "source": [
        "### Multihead Attention - <font color='green'>`Advanced`</font>\n",
        "\n",
        "[Introduce the concept of MHA and why it can be useful in a models.]\n",
        "\n",
        "[Talk about projecting Q,K,V to smaller dimensions to make training more efficient etc.]\n",
        "\n",
        "[Code up haiku multi-head attention module that will be used in future transformer models]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZPL9qVci0I"
      },
      "source": [
        "**Code Task:** Code up a Haiku module that implements the entire multi-head attention mechanism. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px2JQqOnYs17"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(hk.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_heads,\n",
        "      key_size,\n",
        "      model_size = None,\n",
        "      name = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.key_size = key_size\n",
        "    self.model_size = model_size or key_size * num_heads\n",
        "\n",
        "  def __call__(self, query, key, value, mask = None):\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ozNp_pfhc12I"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# TODO: ADD IN CORRECT ANSWER\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1gJBDviYsHX"
      },
      "source": [
        "**MHA vs other sequence methods (optional)**\n",
        "\n",
        "[Talk about how it differs and the complexity differs and other differences. Like what was done in this [practical](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILOYJH4gCnD"
      },
      "source": [
        "### Section Quiz \n",
        "\n",
        "[Test knowledge on all the previous material of attention]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5iFeOKOgCnE"
      },
      "outputs": [],
      "source": [
        "#@title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **Transformers**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjEsrhhVhZ2J"
      },
      "source": [
        "### Processing the input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq0kPkVphbFG"
      },
      "source": [
        "#### Tokenisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd4hPASBiEGe"
      },
      "source": [
        "Before data can be fed into transformers, it has to be transformed into an acceptable format, in other words, a sequence of tokens. Below, we briefly discuss how we can transform text and vision data into tokens that transformers can process [source](https://huggingface.co/docs/transformers/preprocessing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8GylOMmBW4n"
      },
      "source": [
        "##### **Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS_jLXJw0kdN"
      },
      "source": [
        "\n",
        "Transformers can not handle raw strings of text. So to process text, the text is first split up into tokens, where after these tokens are indexed and each token is assigned an embedding of size $d_{model}$. These embeddings can be learned during training or can come from a pretrained vocablay of embeddings. This new sequence of token embeddings is then fed into the transformer architecture. This idea is visualised below. \n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16euh4LADP_mcXywFwKKY3QQQkVplepiI\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        "\n",
        "These token IDs are typically predicted when a model generates text, fills in missing words, etc.\n",
        "\n",
        "This process of splitting up text into tokens and assigning an ID to each token is called [tokenisation](https://huggingface.co/docs/transformers/tokenizer_summary). There are various ways to tokenise text, with some methods being trained directly from the data. When using pre-trained transformers, it is crucial to use the same tokeniser that was used to train the model. The previous link has in-depth descriptions of many widely known techniques.\n",
        "\n",
        "Below we show how the [Bert](https://arxiv.org/abs/1810.04805) models tokeniser tokenises a sentence. We use [Hugging Face](https://huggingface.co/) for this part, where we have a deep dive into Hugging Face later in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0C-3v0y0iZe"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = bert_tokenizer(\"The practical is so much fun\")\n",
        "print(f\"Token IDs: {encoded_input['input_ids']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-408xpj794l"
      },
      "source": [
        "Here we can see that the tokeniser returns the IDs for each token, as shown in the figure. But counting the number of IDs, we see that it is larger than the number of words in the sentence. Let's print the tokens associated with each ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dYMGIgi8p-q"
      },
      "outputs": [],
      "source": [
        "print(f\"Tokens: {bert_tokenizer.decode(encoded_input['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phjOR7ftAVt_"
      },
      "source": [
        "We can see the tokeniser attaches new tokens, `[CLS]` and `[SEP]`, to the start and end of the sequence. This is a Bert specific requirement for training and inference. Adding special tokens is a very common thing to do. \n",
        "\n",
        "For instance, to pretrain specific transformers, they perform what is known as masked prediction. For this, random tokens in a sequence are replaced by the `[MASK]` token, and the model is trained to predict the correct token ID for the token replaced with that token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syL_uNAMVHQB"
      },
      "source": [
        "**Group task**:\n",
        "\n",
        "- Discuss with a friend what you think the current issue is when feeding these raw tokens into a transformer architecture? Think about the difference in meaning between a sentence and that same sentence where the word orders are random.\n",
        "- How would you fix that issue? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jWreoeTBUCD"
      },
      "source": [
        "##### **Images** (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcyHr8a_BdaT"
      },
      "source": [
        "As mentioned, transformers are versatile and can be applied to roughly any data which can be converted into a sequence of tokens.\n",
        "\n",
        "For example, to use the transformers encoder architecture with images, one can split an image into different patches, flatten these image patches and project each image patch into a fixed-sized embedding using any projection method. By doing this, the image has been converted into a sequence of image tokens, and the transformer will be able to process the data. \n",
        "\n",
        "This process is shown in the image below. \n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ERF0f3Y_0wNb4kQ07xMYUysqNYIPkQMD\" alt=\"drawing\" width=\"550\"/>\n",
        "\n",
        "\n",
        "**Code task (OPTIONAL and ADVANCED):** Write a function that can take in a batch of images with shape (Batch, Height, Widht, Channels) and divide it into equal-sized patches. You can use the output of `image_to_patch` and `plot_image_patches` functions to visualise and test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htWcVGlDHcba"
      },
      "outputs": [],
      "source": [
        "def image_to_patch(image, patch_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - array of shape [B, H, W, C]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "    \"\"\"\n",
        "    B, H, W, C = image.shape # HINT: You will need these\n",
        " \n",
        "    image  # FINISH ME\n",
        "\n",
        "    return image_patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBXMQInfTuJK"
      },
      "outputs": [],
      "source": [
        "# do not change these lines, only run them to test your function\n",
        "print('Original image:')\n",
        "image = np.array(Image.open(\"cat.png\"))\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print(\"Image broken into patches\")\n",
        "img = jnp.array(image)\n",
        "img = jax.image.resize(img, (512,512,3), \"nearest\")\n",
        "img = jnp.expand_dims(img, 0)\n",
        "patches = image_to_patch(img, 256)\n",
        "plot_image_patches(patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqFn3peRTqm4"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "def image_to_patch(image, patch_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - array of shape [B, H, W, C]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "    \"\"\"\n",
        "    B, H, W, C = image.shape\n",
        "    image = image.reshape(B, H//patch_size, patch_size, W//patch_size, patch_size, C)\n",
        "    image = image.transpose(0, 1, 3, 2, 4, 5)    # [B, H', W', p_H, p_W, C]\n",
        "    image_patches = image.reshape(B, -1, *image.shape[3:])   # [B, H'*W', p_H, p_W, C]\n",
        "    return image_patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln7esAMHhdaz"
      },
      "source": [
        "\n",
        "#### Positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8MrN6YJXUM_"
      },
      "source": [
        "In most domains where a transformer can be utilised, there is an underlying order to the tokens produced, be it the order of words in a sentence, the location from which patches are taken in an image or even the steps taken in an RL environment. This order is very important in all cases; just imagine you interpret the sentence \"I have to read this book.\" as \"I have this book to read.\". Both sentences contain the exact same words, yet they have completely different meanings based on the order. \n",
        "\n",
        "As both the encoder and the decoder blocks process all tokens in parrel, the order of tokens is lost in these calculations. To cope with this, the sequence order has to be injected into the tokens directly. This can be done by adding *positional encodings* to the tokens at the start of the encoder and decoder blocks (though some of the latest techniques add positional information in the attention blocks). An example of how positional encodings alter the tokens are shown below.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eSgnVN2hnEsrjdHygDGwk1kxEi8-dcFo\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Ideally, these encodings should have these characteristics ([source](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)):\n",
        "* Each time-step should have a unique value\n",
        "* The distance between time steps should stay constant.\n",
        "* It should be able to generalise to longer sequences than seen during training.\n",
        "* It must be deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WHx-9m9h5JA"
      },
      "source": [
        "\n",
        "##### **Sine and cosine functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrkqm8gC90GU"
      },
      "source": [
        "\n",
        "In Attention is All you Need, the authors used a method that can satisfy all these requirements. This involves summing a combination of sine and cosine waves at different frequencies, with the formula for a position encoding at position $D$ shown below, where $i$ is the embedding index and $d_m$ is the token embedding size. \n",
        "\n",
        "\\\\\n",
        "\n",
        "$P_{D}= \\begin{cases}\\sin \\left(\\frac{D}{10000^{i/d_{m}}}\\right), & \\text { if } i \\bmod 2=0 \\\\ \\cos \\left(\\frac{D}{10000^{((i-1)/d_{m}}}\\right), & \\text { otherwise } \\end{cases}$\n",
        "\n",
        "\\\\ \n",
        "\n",
        "Assuming our model as $d_m=8$, the position embedding will look like this:\n",
        "\n",
        "$P_{D}=\\left[\\begin{array}{c}\\sin \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{8/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{8/8}}\\right)\\end{array}\\right]$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Let's first create a function that can return these encodings to understand why this will work. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKobhuyj9-74"
      },
      "outputs": [],
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "  assert token_embedding%2==0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "  i = jnp.arange(0, token_embedding, 2)\n",
        "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "  frequencies = positions*frequency_steps\n",
        "\n",
        "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "  return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4_9-Oh4yZuF"
      },
      "outputs": [],
      "source": [
        "token_sequence_length = 50 # Number of tokens the model will need to process\n",
        "token_embedding = 10000 # token embedding (and positional encoding) dimensions, ensure it is divisible by two\n",
        "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
        "plot_position_encodings(P, token_sequence_length, token_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw8nOF2Ra93B"
      },
      "source": [
        "Looking at the graph above, we can see that for each position index, their is a unique pattern forming, where each position index will always have the same encoding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHZXceUVNWy9"
      },
      "source": [
        "**Group task**:\n",
        "\n",
        "- Discuss with your friend why we are seeing that specific pattern when `token_sequence_length` is 1000, and `token_embedding` is 768.\n",
        "- You can try playing around with smaller values for `token_sequence_length` and  `token_embedding` to get a better intuition for the above discussion.\n",
        "- Ask your friend why they think the 10000 constant is used in the functions above. \n",
        "- Make `token_sequence_length` to be 50 and `token_embedding` something large, like 10000. What do you notice? Is a large token embedding always needed?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFiRpEaGd6TB"
      },
      "source": [
        "**Math task (optional):** Notice in our function that we do not directly implement the equation we describe above for numerical stability when calculating the frequency steps. See if you can derive by hand how we got to this new equation. Hint: Think about log rules.\n",
        "\n",
        "Original equation:\n",
        "\n",
        "$\\text{frequencies} = \\frac{D}{x^{i/d_{m}}}$\n",
        "\n",
        "Code equation:\n",
        "\n",
        "$\\text{frequencies} = D\\left(\\text{exp} \\left( \\frac{-i\\log(x)}{d_m} \\right)\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21sD_xKtd6TB"
      },
      "source": [
        "###### **Answer (but first try yourself)**\n",
        "\n",
        "Expand this section to see the answer. \n",
        "\n",
        "What we did here is an essential aspect of computer science, specifically in machine learning, when big numbers appear. We usually rewrite equations to utilise logs, such that multiplications become additions and large numbers get suppressed. This ensures better numerical stability.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQPI_Sqnd6TB"
      },
      "source": [
        "\\begin{align}\n",
        "    \\frac{ð·}{x^{ð/ðð}} &= D\\left( x^{-i/dm}\\right) \\\\\n",
        "    &= D\\left( \\text{exp}\\left(\\log{x^{-i/dm}}\\right)\\right) \\\\\n",
        "    &= D\\left( \\text{exp}\\left(\\frac{-i}{dm}\\log{x}\\right)\\right) \\\\\n",
        "    &= D\\left( \\text{exp}\\left(\\frac{-i\\log{x}}{dm}\\right)\\right) \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GeFt_xwh722"
      },
      "source": [
        "##### **Learned positional embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTNW1kNyZ75"
      },
      "source": [
        "\n",
        "Another method which is commonly used is to allow the model to learn the positional information required. In this method, the model learns a lookup table, where each index in the table refers to a positional embedding. Whereas the previous method allowed for an infinite amount of tokens, this method caps the maximum token sequence length as the lookup table has to be set beforehand. \n",
        "\n",
        "Using Haiku, this method is relatively straightforward as we can use [Haiku's embed function](https://dm-haiku.readthedocs.io/en/latest/api.html#embed) or [Haiku's get parameter function](https://dm-haiku.readthedocs.io/en/latest/api.html#get-parameter)\n",
        "\n",
        "**Code Task:** Try and implement a lookup table, which can be learned, for using Haikus get parameter function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXbQ_-RnXaNY"
      },
      "outputs": [],
      "source": [
        "class PositionEmbeddingsLookup(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [max_sequence_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, max_sequence_len, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        \n",
        "        assert sequence.shape[0]<=self.max_sequence_len, f\"Sequence to long, max lenght={self.max_sequence_len}\"\n",
        "\n",
        "        lookup_table = hk.get_parameter( \n",
        "            name=\"position_embedding\",\n",
        "            shape=, # FILL ME IN\n",
        "            init=jnp.zeros\n",
        "        )\n",
        "\n",
        "        return lookup_table[:sequence.shape[0], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1_WpVc0Y3Hk"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "class PositionEmbeddingsLookup(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [max_sequence_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, max_sequence_len, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        \n",
        "        assert sequence.shape[0]<=self.max_sequence_len, f\"Sequence to long, max lenght={self.max_sequence_len}\"\n",
        "\n",
        "        lookup_table = hk.get_parameter(\n",
        "            name=\"position_embedding\", \n",
        "            shape=(self.max_sequence_len, self.d_model),\n",
        "            init=jnp.zeros\n",
        "        )\n",
        "\n",
        "        return lookup_table[:sequence.shape[0], :]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE6hEihPhAhK"
      },
      "source": [
        "### Section Quiz \n",
        "\n",
        "Optional end of section quiz. Below is an example of an assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L03B3HKwhAhK"
      },
      "outputs": [],
      "source": [
        "#@title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rectkTs9iFHg"
      },
      "source": [
        "## **Hugging Face**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFBw8kRx-4Mk"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"data:image/png;base64,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\" width=\"10%\" />\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is a startup founded in 2016 and, in their own words: \"are on a mission to democratize good machine learning, one commit at a time.\"\n",
        "\n",
        "They have developed various open-source packages and allow users to easily interact with a large corpus of pretrained transformer models (across all modalities) and datasets to train or fine-tune pre-trained transformers.\n",
        "\n",
        "Their software is used widely in industry and research. The following sections show how one can interact with their various features, access SOTA models, and train your own model.\n",
        "\n",
        "This is an optional section for the practical. Still, working through it in the practical or afterwards is highly recommended. Understanding Hugging Face will allow you to build a very quick proof of concept system to test out various hypotheses, whereafter, the system you build can simply be used, or a new system can be developed with the knowledge that the original idea has merit. \n",
        "\n",
        "It should also be noted that various languages are still severely under-resourced, even in this ecosystem. See it as an opportunity also to see where the gaps are and how we as a community can reduce this gap. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets Package - <font color='blue'>`Beginner`</font>"
      ],
      "metadata": {
        "id": "Wq_pX3XTefu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Along with all the models being availible, datasets are also hosted on the hugginface hub. One can visit the [dataset hub](https://huggingface.co/datasets), and browse for interesting datasets and very easilly access it through the [datasets](https://github.com/huggingface/datasets) package.\n",
        "\n",
        "Lets say for instance we want to build an text intent classification model. What we do then is to go to the link above, and use the search tags to find a dataset that seems like a good fit for us. Doing this, we find the [`banking77`](https://huggingface.co/datasets/banking77) dataset. Below we then load in this dataset.\n"
      ],
      "metadata": {
        "id": "Tn-vfzXyehPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just for notebook cleanliness\n",
        "from datasets.utils.logging import set_verbosity_error\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"banking77\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "4StYbstjeitO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that the API returns a variable of DatasetDict type, which containins our dataset that has split into two datasets, i.e. train and test splits. We also see that each of these datasets contain `text` and `label` features. \n",
        "\n",
        "Lets investigate how the data looks."
      ],
      "metadata": {
        "id": "pMh4aox5eh8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "id": "9qFr15bkemIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that our dataset splits are of type Dataset."
      ],
      "metadata": {
        "id": "EzyH1lNWen3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_intents = dataset['train']\n",
        "test_intents = dataset['test']\n",
        "\n",
        "print('Text: ', train_intents['text'][0])\n",
        "print('Label: ', train_intents['label'][0])"
      ],
      "metadata": {
        "id": "zEQFsm0HepIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes we want to work with only a subset of the dataset and we thus need to filter out the rest. Luckilly, the datasets have an easy to use filter functionality. Below we filter to only see text which relates to label 11. "
      ],
      "metadata": {
        "id": "N7--kHR7eqej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we are only the first 5 text samples\n",
        "train_intents.filter(lambda data: data['label']==1)"
      ],
      "metadata": {
        "id": "ehbEghPSetvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also apply a function to each data item by mapping the function to each \"row\" in the dataset."
      ],
      "metadata": {
        "id": "IvcEMvEDevSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating small dataset as example of using select\n",
        "example_dataset = train_intents.select(range(10))\n",
        "\n",
        "# printing first character of a text example\n",
        "example_dataset.map(lambda example: print(example['text'][0]));"
      ],
      "metadata": {
        "id": "q-0tUb6sewij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the map function, we can also add a new collumn."
      ],
      "metadata": {
        "id": "xgt4MFR4ex14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_sentence_len(example):\n",
        "    example['lenght'] = len(example['text'])\n",
        "    return example\n",
        "\n",
        "# add new column\n",
        "example_dataset = example_dataset.map(lambda example: add_sentence_len(example))\n",
        "\n",
        "print(example_dataset)\n",
        "print('New data:', example_dataset['lenght'])"
      ],
      "metadata": {
        "id": "6gtdQeKVezT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task**: \n",
        "- We want to build a classifier out of this. We need to investigate the distrubution of classes to see if our dataset is balanced or not. Write code that generates a dictionary containing the count of each class for both the train and test dataset.\n",
        "- Filter out classes with less than 150 classes in the training set, and ensure only the remaining classes are in the test test. "
      ],
      "metadata": {
        "id": "nZ_qP_Jqe0r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# task 1\n",
        "unique_labels = # FINISH ME\n",
        "total_unique_labels = # FINISH ME\n",
        "\n",
        "train_counts = # FINISH ME\n",
        "test_counts = # FINISH ME\n",
        "\n",
        "# task 2\n",
        "passed_labels = [label for label, count in train_counts.items() if count>=150]\n",
        "train_intents = # FINISH ME\n",
        "test_intents = # FINISH ME"
      ],
      "metadata": {
        "id": "42qddlWve2Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# # task 1\n",
        "unique_labels = np.unique(train_intents['label'])\n",
        "total_unique_labels = len(unique_labels)\n",
        "\n",
        "train_counts = {label:sum(label==train_intents['label']) for label in unique_labels}\n",
        "test_counts = {label:sum(label==train_intents['label']) for label in unique_labels}\n",
        "\n",
        "# task 2\n",
        "passed_labels = [label for label, count in train_counts.items() if count>=150]\n",
        "train_intents = train_intents.filter(lambda data: data['label'] in passed_labels)\n",
        "test_intents = test_intents.filter(lambda data: data['label'] in passed_labels)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ABrM5iAIe4YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other modalities**\n",
        "\n",
        "Text is not the only data that can be accessed, audio and image data can be accessed just as easilly. "
      ],
      "metadata": {
        "id": "hz4U0IZ8e34k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "dataset = load_dataset(\"cgarciae/cartoonset\")\n",
        "Image.open(BytesIO(dataset['train'][\"img_bytes\"][231]))\n",
        "\n",
        "# # free up a bit of space\n",
        "del dataset"
      ],
      "metadata": {
        "id": "aPCHECJQiFU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers Package"
      ],
      "metadata": {
        "id": "iL4j1JZse8wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a way in accessing data, lets shift our intention to accessin the hundreds of pretrained transformers."
      ],
      "metadata": {
        "id": "4bKe1WDEe9eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipeline - <font color='blue'>`Beginner`</font>"
      ],
      "metadata": {
        "id": "Po9R6qiSe_HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest method to access a vast range of pre-trained models and use tasks is through the `pipeline` API.\n",
        "\n",
        "Pipelines group together a pretrained model found on their models hub with the preprocessing that was used during that model's training. To use the pipeline, one must import it from the [transformers](https://github.com/huggingface/transformers) library and specify the task and model you want.\n",
        "\n",
        "For a list of models, visit [this](https://huggingface.co/models) page that allows you to search through all models currently on the hub."
      ],
      "metadata": {
        "id": "tcK8-KpZfA7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When calling the function for the first time, the model, and its tokenizer, will be automatically downloaded\n",
        "sentiment_model = pipeline(task='sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "print(sentiment_model(\"I love this practical!\"))\n",
        "print(sentiment_model(\"I hate this practical!\"))\n",
        "\n",
        "# passing more than one sentence\n",
        "sentence_batch = [\n",
        "  'This is much quicker and easier to build a POC with than training everything from scratch',\n",
        "  'It really hurts when I stub my toe',\n",
        "  'I want to get ice cream'\n",
        "]\n",
        "print('\\nBatch output:')\n",
        "sentiment_model(sentence_batch)"
      ],
      "metadata": {
        "id": "ZjTFOuylfDKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the first sentence we process in our batch of sentences is predicted to be Negative, with a relative low score of $0.51$, even though we feel this should be more neutral? The low score indicates this and we can interepet that when scores are low the actual label was meant to be Neutral, but this model was trained to do a binary prediction only.\n",
        "\n",
        "This model you just used is a Distellbert model, which was trained on *8 16GB V100s for 90 hours*, and you could use it as quickly as that.\n",
        "\n",
        "**Code Task:** Apply the zero shot model to all of our test intent chatbot examples, extracting the predicted senitment label into a new collumn, using the map function."
      ],
      "metadata": {
        "id": "qNYlFKWNfCL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(example):\n",
        "  # FINISH ME\n",
        "  return sentiment\n",
        "\n",
        "train_intents = # FINISH ME\n",
        "test_intents = # FINISH ME"
      ],
      "metadata": {
        "id": "00lgxVY8fKPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "def get_sentiment(example):\n",
        "  example['sentiment'] = sentiment_model(example['text'])[0]['label']\n",
        "  return example\n",
        "\n",
        "test_intents = test_intents.map(lambda example: get_sentiment(example))"
      ],
      "metadata": {
        "id": "KY_pOGR6fLYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Count of positive versus negative text inputs')\n",
        "sns.countplot(test_intents['sentiment']);"
      ],
      "metadata": {
        "id": "txcTUTbUfMnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group code task (optional depending on time)**: (Hint: use the tags when searching the [model hub](https://huggingface.co/models))\n",
        "- Search for other pipeline tasks available, and dicuss with your friend what you did and found. ([Hint](https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/pipelines#pipelines))\n",
        "- Play with different language models and see how they perform"
      ],
      "metadata": {
        "id": "Tg3ti5iEfGIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_pipeline = pipeline(\n",
        "    task='text-generation', # CHANGE ME TO OTHER STUFF\n",
        "    model='gpt2' # CHANGE ME AS WELL\n",
        ")\n",
        "text = 'I like ice-cream and '\n",
        "your_pipeline(text)"
      ],
      "metadata": {
        "id": "jeMfN56EfS7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeing up memory for future tasks\n",
        "del your_pipeline\n",
        "del sentiment_model"
      ],
      "metadata": {
        "id": "DSu5FY7CfRJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training a chatbot intent model  - <font color='blue'>`Advanced`</font>"
      ],
      "metadata": {
        "id": "QSJ-Fh49fU-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want more controll than the pipeline API provides you, you can also use the predefined model classes. \n",
        "\n",
        "To showcase this, we will be training a custom model on top of a large transformer.\n",
        "\n",
        "To set the scene, lets say for instance that we want to train an intent model that can be used along with a chatbot. This intent model will be responsible to predict the true underlying intent found within the text. \n",
        "\n",
        "Your imaganiry friend has built an intent model using TF-IDF techniques, but you think that you can use transformers for this task and that it will perform better. \n",
        "\n",
        "You have heard about the famous [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf) model that was trained to extract text features, and yout think this model can be a perfect fit to extract features for your intent model, as it is an encoder only transformer architecture that produces strong token representations.\n",
        "\n",
        "To start your training process, you have two steps to follow:\n",
        "\n",
        "* Get the tokenizer\n",
        "* Get the model"
      ],
      "metadata": {
        "id": "LHzc2lpOfWVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Getting the tokenizer**\n",
        "\n",
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. It is very important to use the same tokenizer as the model you will be finetuning.  "
      ],
      "metadata": {
        "id": "-MBCwSG5i38v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want the Distilbert model, thus we import the correct tokenizer the model train\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# we specify a specif distilbert \n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print('Tokenizer output:')\n",
        "output = tokenizer('This is example text')\n",
        "print(output)\n",
        "\n",
        "print('Tokens converted back to string')\n",
        "print(tokenizer.decode(output['input_ids']))"
      ],
      "metadata": {
        "id": "f7ufSoUxi4uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the output from the above, we that the special tokens are the `[CLS]` and `[SEP]` tokens. This is important to note, as we will be using the final output of the model for the `[CLS]` token when predicting the intent. \n",
        "\n",
        "This is a very common thing to do, where the token that indicates the start of the sentence is used when making predictions on the sentence. Seeing as our system is built in Jax, we will need to tell the tokenizer to return the data in the correct format.\n"
      ],
      "metadata": {
        "id": "zW_VglCQfXfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_batch = tokenizer(\n",
        "    [\"We are very happy to show you the ð¤ Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"jax\",\n",
        ")\n",
        "print(token_batch)"
      ],
      "metadata": {
        "id": "eR0yqg-mfZjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeing as we want to use these tokens throughout our training process and for processing the embeddings, lets map the tokenizer output to new collumn for our train and test splits."
      ],
      "metadata": {
        "id": "pqBOZ4YLfa0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Getting the transformer and generating embeddings**"
      ],
      "metadata": {
        "id": "hyDoAqXvfc_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No that we can encode our text quickly into data that our transformer can processes, let gather and download the pretrained transformer model from the hub and generate representations for each text example.\n",
        "\n",
        "As was mentioned above, we are interested for now only in the `[CLS]` token embedding. One can ofcourse look at averaging over all token (being sure to only use tokens who are note masked) and see how it compares. We leave this as an excersise for the reader to compare how it changes the performance of the model.\n"
      ],
      "metadata": {
        "id": "sxSkOij_fexe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FlaxDistilBertModel\n",
        "\n",
        "# we use FlaxDistillBertModel because we are in the JAX world\n",
        "distell_bert_model = FlaxDistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "tokens = tokenizer([train_intents[0]['text']])\n",
        "embeddings = distell_bert_model(**tokens)[0][:,0]\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "DFhSfCOKfgA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying one by one is extremely slow, so lets rather infer in batches, using the `batch` flag in the map function. It is important here that the function that is being mapped works with batches and return the data in the correct format, i.e a dictionary with the new column name."
      ],
      "metadata": {
        "id": "GKgHz1_tfh10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(batch):\n",
        "\n",
        "  text = batch['text']\n",
        "  tokens = tokenizer(\n",
        "      text,\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=50,\n",
        "      return_tensors=\"jax\",\n",
        "  )\n",
        "\n",
        "  cls_embeddings = np.array(distell_bert_model(**tokens)[0][:,0])\n",
        "  return {'embedding':cls_embeddings}\n",
        "\n",
        "train_intents = train_intents.map(lambda batch: get_embedding(batch), batched=True)\n",
        "test_intents = test_intents.map(lambda batch: get_embedding(batch), batched=True)"
      ],
      "metadata": {
        "id": "hPkm_JdsfjIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see whether these embeddings are in anyway usefull, lets plot a few projected embeddings and their labels and see how it looks. "
      ],
      "metadata": {
        "id": "dDcAZzm3fkfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling to get a clear picture with less data.\n",
        "sample_labels = [5,11,20,28,34,45,51,76]\n",
        "plot_data = train_intents.filter(lambda data: data['label'] in sample_labels)\n",
        "plot_projected_embeddings(plot_data['embedding'], [str(l) for l in plot_data['label']])"
      ],
      "metadata": {
        "id": "bbCy-UK7flz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see there are clear clusters forming, but there is defnitely still some work that can be done here.\n",
        "\n",
        "Lets thus train a non linear model on this data to try and find something that seperate this intents.\n",
        "\n",
        "We have a choice really of what we want to do, given data our new dataset can just be interpeted as a database. We can either train another neural netwok, or we can train anything ranging from a logistic regression model to a XGBoost model.\n",
        "\n",
        "Lets start by training a neural network, as we are at the deep learning indaba ;)"
      ],
      "metadata": {
        "id": "Z8agDZwufoFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting data into more usable state for all methods\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder() # labels should be from 0 - N\n",
        "\n",
        "train_embeddings = train_intents['embedding']\n",
        "train_labels = jnp.array(le.fit_transform(train_intents['label']))\n",
        "test_embeddings = test_intents['embedding']\n",
        "test_labels = jnp.array(le.transform(test_intents['label']))"
      ],
      "metadata": {
        "id": "KohN7yfbfmOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load batches of embedding, we feed our extracted embeddings and labels into tensorflow datasets."
      ],
      "metadata": {
        "id": "7LxKYDyYfr3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# creating tensorflow dataset loaders\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_embeddings,train_labels))\n",
        "train_ds = train_ds.\\\n",
        "  shuffle(buffer_size=len(train_embeddings),reshuffle_each_iteration=True).\\\n",
        "  batch(64)\n",
        "\n",
        "# we do not want to shuffle test data\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_embeddings,test_labels))\n",
        "test_ds = test_ds.batch(64)"
      ],
      "metadata": {
        "id": "og_F4Hyrfquy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Training intent model**"
      ],
      "metadata": {
        "id": "TW5-9GbNf1nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the methods below only show us updating the weights or parameters of downstream models, there is now reason that one can not finetune the entire Distilbert model on the new dataset and task. This will just require much more compute and in many cases the extra costs are not linearly correlated with improved performance. This is why in this practical, why only train downstream models utilising the pretrained embeddings."
      ],
      "metadata": {
        "id": "F-UfWogVf3zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### MLP"
      ],
      "metadata": {
        "id": "ekey34T1f5jK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our intent model will be 2 layer MLP. \n",
        "\n",
        "**Code task:** Finish the 2 layer MLP Haiku Module."
      ],
      "metadata": {
        "id": "mFetHB7tf641"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build a training model\n",
        "\n",
        "class IntentClassifier(hk.Module):\n",
        "  \"\"\"A MLP which predicts intent from transformer embeddings\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._init_scale = .5\n",
        "    self.number_classes = #FILL ME IN\n",
        "\n",
        "  def __call__(self, embeddings):\n",
        "    embedding_size = # FILL ME IN\n",
        "    initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "    projection_layer = hk.Linear(embedding_size, w_init=initializer)\n",
        "    classification_layer = # FILL ME IN\n",
        "\n",
        "    projections = jax.nn.relu(projection_layer(embeddings))\n",
        "\n",
        "    logits = # FILL ME IN\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Ie5_Hf8Ff75r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# build a training model\n",
        "\n",
        "class IntentClassifier(hk.Module):\n",
        "  \"\"\"A MLP which predicts intent from transformer embeddings\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._init_scale = .5\n",
        "    self.number_classes = 38\n",
        "\n",
        "  def __call__(self, embeddings):\n",
        "    embedding_size = embeddings.shape[-1] \n",
        "    initializer = hk.initializers.VarianceScaling(\n",
        "        self._init_scale)\n",
        "    projection_layer = hk.Linear(\n",
        "        embedding_size, \n",
        "        w_init=initializer\n",
        "    )\n",
        "    classification_layer = hk.Linear(\n",
        "        self.number_classes,\n",
        "        w_init=initializer\n",
        "    )\n",
        "\n",
        "    projections = jax.nn.relu(\n",
        "        projection_layer(embeddings)\n",
        "    )\n",
        "    logits = classification_layer(projections)\n",
        "    return logits"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Kjq62LOFf9Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we build the Haiku training loop."
      ],
      "metadata": {
        "id": "bS8bmMangAOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiliase model and optmiser\n",
        "\n",
        "def classify_intent(embeddings):\n",
        "  model = IntentClassifier()\n",
        "  return model(embeddings)\n",
        "\n",
        "classify_intent = hk.transform(classify_intent)\n",
        "\n",
        "# initialise model\n",
        "rng = jax.random.PRNGKey(42)\n",
        "x = np.zeros([1, 768])\n",
        "params = classify_intent.init(rng, x)\n",
        "\n",
        "optimiser = optax.adam(1e-3)\n",
        "opt_state = optimiser.init(params)\n"
      ],
      "metadata": {
        "id": "FZp6hQMNf_Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate loss \n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "def loss(params, batch):\n",
        "  \"\"\"Cross-entropy classification loss\"\"\"\n",
        "  batch_size = len(batch['labels'])\n",
        "  logits = classify_intent.apply(params,key, batch['embeddings'])\n",
        "  labels = jax.nn.one_hot(batch['labels'], num_classes=38)\n",
        "  log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "  return -log_likelihood / batch_size"
      ],
      "metadata": {
        "id": "UwY8XsRMgEET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update weights\n",
        "@jax.jit\n",
        "def update(params, opt_state, batch):\n",
        "  # get data neded for training\n",
        "  grads = jax.grad(loss)(params, batch)\n",
        "  updates, opt_state = optimiser.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state"
      ],
      "metadata": {
        "id": "pvoc6MS1gM7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy per batch\n",
        "@jax.jit\n",
        "def accuracy(params, batch):\n",
        "    predictions = classify_intent.apply(params, key, batch['embeddings'])\n",
        "    print(predictions)\n",
        "    return jnp.mean(jnp.argmax(predictions, axis=-1) == batch[\"labels\"])"
      ],
      "metadata": {
        "id": "o2EtwmKTgSpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Training & evaluation loop.\n",
        "for epoc in range(10):\n",
        "\n",
        "  train_accs = 0\n",
        "  total_calcs = 0\n",
        "  for batch in tqdm(train_ds, desc='Train steps', leave=False):\n",
        "    batch = {\n",
        "        'embeddings':jnp.array(batch[0]),\n",
        "        'labels':jnp.array(batch[1])\n",
        "    }\n",
        "    params, opt_state = update(params, opt_state, batch)\n",
        "    train_accs+=accuracy(params, batch)\n",
        "    total_calcs+=1\n",
        "\n",
        "  if epoc%5==0:\n",
        "    print(f'At epoch: {epoc}') \n",
        "    train_accs /= round(total_calcs,2)\n",
        "\n",
        "    test_accs = 0\n",
        "    total_calcs = 0\n",
        "\n",
        "    for batch in tqdm(test_ds, desc='Test steps', leave=False):\n",
        "      batch = {\n",
        "          'embeddings':jnp.array(batch[0]),\n",
        "          'labels':jnp.array(batch[1])\n",
        "      }\n",
        "      test_accs+=accuracy(params, batch)\n",
        "      total_calcs+=1\n",
        "    \n",
        "    test_accs /= round(total_calcs,2)\n",
        "\n",
        "    print(f'\\nTrain accuracy:{train_accs}')\n",
        "    print(f'Test accuracy:{test_accs}')"
      ],
      "metadata": {
        "id": "4UZ4x66OgUef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a trained MLP that can predict the intent, we need to writ code that given new text, will classify an intent. "
      ],
      "metadata": {
        "id": "PCAs9gLVgDnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_text_mlp(text):\n",
        "  embedding = get_embedding({'text':[text]})['embedding']\n",
        "  logits = classify_intent.apply(params, key, embedding)\n",
        "  predicted_intent = jnp.argmax(jax.nn.softmax(logits))\n",
        "  converted_back_intent = le.inverse_transform([predicted_intent])[0]\n",
        "  print(f'Predicted intent for inpy \"{text}\" is {int(converted_back_intent)}')\n",
        "  \n",
        "  index = jnp.where(train_labels==predicted_intent)[0]\n",
        "  print(f'Example training data from this class: {train_intents[index][\"text\"][0]}')"
      ],
      "metadata": {
        "id": "OiCROb4lgZhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_new_text_mlp('Can I get a refund please')"
      ],
      "metadata": {
        "id": "j9o8sN2-gbIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Logistic regression"
      ],
      "metadata": {
        "id": "XvyQtDmggcvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final experiment, we will see how a logistic regression model performs against our MLP, and how quick it can be to get very quick results."
      ],
      "metadata": {
        "id": "g5TgD99zgeQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "model = LogisticRegression(max_iter=6000)\n",
        "model.fit(train_embeddings, train_labels)\n",
        "y_hat_train = model.predict(train_embeddings)\n",
        "y_hat_test = model.predict(test_embeddings)\n",
        "\n",
        "train_accuracy = jnp.sum(y_hat_train == train_labels)/len(y_hat_train)\n",
        "test_accuracy = jnp.sum(y_hat_test == test_labels)/len(y_hat_test)\n",
        "\n",
        "print(f'Train accuracy:{train_accuracy}')\n",
        "print(f'Test accuracy:{test_accuracy}')"
      ],
      "metadata": {
        "id": "HxVGB2PGggQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, in just those few lines of code we have coded a logistic regression model that can classify the intent of a user given "
      ],
      "metadata": {
        "id": "ZQvBBHhYgkv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_text_lr(text):\n",
        "  embedding = get_embedding({'text':[text]})['embedding']\n",
        "  predicted_intent = model.predict(embedding)\n",
        "  converted_back_intent = le.inverse_transform([predicted_intent])[0]\n",
        "  print(f'Predicted intent for inpy \"{text}\" is {int(converted_back_intent)}')\n",
        "  \n",
        "  index = jnp.where(train_labels==predicted_intent)[0]\n",
        "  print(f'Example training data from this class: {train_intents[index][\"text\"][0]}')"
      ],
      "metadata": {
        "id": "ovi4HNFfgiUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_new_text_lr('Can I please get a refund')"
      ],
      "metadata": {
        "id": "cJMvLsDEgmj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:** \n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:** \n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "#@title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6EqhIg1odqg0",
        "-ZUp8i37dFbU",
        "ii__Bc27epiJ",
        "7SrT6swYgCm9",
        "WILOYJH4gCnD",
        "hE6hEihPhAhK",
        "fV3YG7QOZD-B",
        "o1ndpYE50BpG"
      ],
      "name": "attention_and_transformers.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}