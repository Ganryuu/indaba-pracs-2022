{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Intro to Reinforcement Learning\n",
        "\n",
        "\n",
        "[Need to update this picture]\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/Indaba_2022_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "Claude Formanek\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "Reinforcement Learning\n",
        "\n",
        "**Topics:** \n",
        "\n",
        "Content: Reinforcement Learning\n",
        "\n",
        "Level: Beginner\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "[Points on the exact learning outcomes from the prac. This should probably be <=5 things.]\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "[Knowledge required for this prac. You can link a relevant parallel track session, blogs, papers, courses, topics etc.]\n",
        "\n",
        "**Outline:** \n",
        "\n",
        "[Points that link to each section.]\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Setup\n",
        "\n",
        "For this practical, it will help to use a GPU runtime to speed up training. To do this, go to the `Runtime` menu in Colab, select `Change runtime type` and then in the popup menu, choose `GPU` in the `Hardware accelerator` box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "# @title Install required packages (Run Cell)\n",
        "%%capture\n",
        "!pip install jaxlib\n",
        "!pip install jax\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install gym\n",
        "!pip install gym[box2d]\n",
        "!pip install rlax\n",
        "!pip install optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwbqggmcRjMy"
      },
      "outputs": [],
      "source": [
        "# @title Import required packages (Run Cell)\n",
        "%%capture\n",
        "import random\n",
        "import collections # useful data structures\n",
        "import numpy as np\n",
        "import gym # reinforcement learning environments\n",
        "import jax\n",
        "import jax.numpy as jnp # jax numpy\n",
        "import haiku as hk # jax neural network library\n",
        "import optax # jax optimizer library\n",
        "import rlax # jax reinforcement learning library\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## Section 1: Key Concepts in Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning (RL) is a subfield of Machine Learning (ML). RL algorithms try to learn the optimal actions to take in an environment in order to maximise some reward signal. More precicely, in RL we have an **agent** which percieves an observation $o_t$ of the current state $s_t$ of the **environment** and must choose an action $a_t$ to take. The environment then transitions to a new state $s_{t+1}$ in response to the agents action and also gives the the agent a scalar reward $r_t$ to indicate how good or bad the chosen action was given the environment's state. The goal in RL is for the agent to maximise the ammount of reward it receives from the environment over time. The subscipt $t$ is used to indicate the timestep number, i.e., $s_0$ is the state of the environment at the initial timestep and $a_{99}$ is the agent's action at the $99th$ timestep. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghgy69hFRjMz"
      },
      "source": [
        "### OpenAI Gym\n",
        "OpenAI has provided a Python package called Gym that includes implementations of popular environments and a simple interface for an RL agent to interact with. To create a gym environment, all you need to do is pass the name of the environment to the function `gym.make(<environment_name>)`. In this tutorial we will be using a simple environment called CartPole. In **CartPole** the task is for the agent to learn to balance the pole for as long as possible by moving the cart *left* or *right*.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/1*v8KcdjfVGf39yvTpXDTCGQ.gif\" width=\"30%\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfxzajMYRjMz"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_BbftaJj3zu"
      },
      "source": [
        "### States and observations\n",
        "In RL the agent perceives an observation of environments state. In some settings the observation may include all the information underlying the environment's state. Such an envrionment is called **fully observed**. In other settings the the agent may only receive partial information about the environment's state in its observation. Such an environment is called **partially observed**. For the rest of this tutorial we will assume the environment is fully observed and so we will use state $s_t$ and observation $o_t$ interchangeably. In Gym we get the initial observation from the environment by calling the function `env.reset()`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdS8nqOgRjM0"
      },
      "outputs": [],
      "source": [
        "# Reset the environment\n",
        "s_0 = env.reset()\n",
        "print(\"Initial State::\", s_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL1Nkgy7nUfn"
      },
      "source": [
        "In CartPole the state of the environment is represented by four numbers; *angular position of the pole, angular velocity of the pole, position of the cart, velocity of the cart*. \n",
        "\n",
        "### Actions\n",
        "In RL actions are usually either **discrete** or **continuous**. Continous actions are given by a vector of real numbers. Discrete actions are given by an integer value. In environments where we can count out the finite set of actions we usually use discrete actions. In CartPole there are only two actions; *left and right*. As such, the actions can be represented by integers $0$ and $1$. In gym we can easily get the list of possible actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOLZqU_LpIXh"
      },
      "outputs": [],
      "source": [
        "# Get action space\n",
        "print(f\"Environment action space: {env.action_space}\")\n",
        "\n",
        "# Get num actions\n",
        "num_actions = env.action_space.n\n",
        "print(f\"Number of actions: {num_actions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRsflxbDpoPm"
      },
      "source": [
        "### The agent's policy\n",
        "In RL the agent choses actions based on the observation it receives. We can think of the agent's action selection process as a function that takes an observation as input and returns an action as output. In RL we usually call this function the agent's **policy** and denote it $\\pi(s_t)=a_t$. \n",
        "\n",
        "In some cases we may want our policy to be stocastic, rather than deterministic. In such cases, actions are actiually randomly sampled from a probability distribution that is conditiond on the observation. We denote stocastic policies $a_t\\sim\\pi(\\cdot\\ |\\ s_t)$, where the symbol $\\cdot$ is simply a shorthand for *all actions* and $~$ means \"*sampled from*\".\n",
        "\n",
        "**Exercise 1:** If Bob has a deterministic policy $\\pi$ and the first time Bob uses his policy on some observation $o_t$ he choses action $0$. What will Bob's chosen action be if he uses his policy a second time on the exact same observation $o_t$. Chose from the options below and assume there are only two possible actions:\n",
        "1.   Bob will chose action 0 again.\n",
        "2.   Bob will chose action 1.\n",
        "3.   You can't say.\n",
        "\n",
        "**Exercise 2:**  If Alice has a stocastic policy $\\pi$ and the first time Alice uses her policy on some observation $o_t$ she choses action $0$. What will Alice's chosen action be if she uses her policy a second time on the exact same observation $o_t$. Chose from the options below and assume there are only two possible actions:\n",
        "1.   Alice will chose action 0 again.\n",
        "2.   Alice will chose action 1.\n",
        "3.   You can't say."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8peH76H7wBKN"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 1 & 2 solution\n",
        "%%capture\n",
        "\n",
        "\"\"\" \n",
        "Exercise 1: Since the policy is deterministic, Bob will always choose the same \n",
        "action given the same observation.\n",
        "\n",
        "Exercise 2: Since the policy is stocastic, the result from Alice's policy will be\n",
        "random. So you can't say.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZdmz2Wqv_NN"
      },
      "source": [
        "As an exercise we will implement a stocastic policy as well as a deterministic policy for CartPole. \n",
        "\n",
        "**Exercise 3:** Complete the function below which should take an observation `obs` as input, compute the dot product between the `obs` and parameter vector `params=[1,-2,2,-1]` and then return a `0` if the result is less than or equal to zero and a `1` if the result is greater than zero. Is this a deterministic or stocastic policy?\n",
        "\n",
        "**Usefull methods:** \n",
        "*   [Numpy dot product](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)\n",
        "\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "*   We already imported `numpy` as `np`.\n",
        "*   Assume `obs` is also a vector of four numbers like `params`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSGCd7XB1z8k"
      },
      "outputs": [],
      "source": [
        "def choose_action(obs):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OCzRmXbz2HfY"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 3 solution\n",
        "\n",
        "def choose_action(obs):\n",
        "\n",
        "  weights = np.array([1,-2,2,-1])\n",
        "  dot_product = np.dot(obs, weights)\n",
        "\n",
        "  if dot_product >= 0:\n",
        "    action = 0\n",
        "  else:\n",
        "    action = 1\n",
        "\n",
        "  return action\n",
        "\n",
        "# TESTS\n",
        "fixed_obs = [1,1,2,4]\n",
        "print(f\"Fixed observation: {fixed_obs}\")\n",
        "for i in range(10):\n",
        "  print(f\"Result of policy call number {i}: {choose_action(fixed_obs)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI_-xUHL0qz-"
      },
      "source": [
        "**Exercise 4:** Complete the function below which should take an observation `obs` as input, compute the dot product between the `obs` and parameter vector `params=[1,-2,2,-1]` and then return a `0` 20% of the time and a `1` 80% of the time if the result is less than or equal to zero. If the result of the dot-product is greater than zero the function should return a `0` 100% of the time. Is this a deterministic or stocastic policy?\n",
        "\n",
        "**Usefull methods:** \n",
        "*   [Numpy random choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypsMyq_S_jwv"
      },
      "outputs": [],
      "source": [
        "def choose_action(obs):\n",
        "\n",
        "  weights = np.array([1,-2,2,-1])\n",
        "  action = None # you will need to overwrite this\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action\n",
        "\n",
        "# TESTS\n",
        "fixed_obs = [1,1,2,4]\n",
        "print(f\"Fixed observation: {fixed_obs}\")\n",
        "for i in range(10):\n",
        "  print(f\"Result of policy call number {i}: {choose_action(fixed_obs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmWivlcK4WlC"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 4 solution\n",
        "\n",
        "def choose_action(obs):\n",
        "\n",
        "  weights = np.array([1,-2,2,-1])\n",
        "  dot_product = np.dot(obs, weights)\n",
        "\n",
        "  if dot_product <= 0:\n",
        "    action = np.random.choice([0,1], p=[0.2, 0.8])\n",
        "  else:\n",
        "    action = 0\n",
        "\n",
        "  return action\n",
        "\n",
        "# TESTS\n",
        "fixed_obs = [1,1,2,4]\n",
        "print(f\"Fixed observation: {fixed_obs}\")\n",
        "for i in range(10):\n",
        "  print(f\"Result of policy call number {i}: {choose_action(fixed_obs)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkuvT-jf6Ieh"
      },
      "source": [
        "### The environment transition function\n",
        "Now that we have a policy we can pass actions from the agent to the environment. The environment will the transition into a new state in response to the agent's action. In RL we model this process by a **state transition function** $P$ which takes the current state $s_t$ and an action $a_t$ as input and returns the next state $s_{t+1}$ as output. Like with policies, the state transiton function can either be deterministic $s_{t+1}=P(s_t, a_t)$ or it can be stocastic $s_{t+1}\\sim P(\\cdot\\ |\\ s_t, a_t)$. In gym we can pass actions to the environment by calling the `env.step(<action>)` function. The function will then return four values; the next observation, the reward for the action taken, a boolean flag to indicate if the game is done and finally some extra information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh0j9-Tk7olb"
      },
      "outputs": [],
      "source": [
        "# Get the initial obs by resetting the env\n",
        "initial_obs = env.reset()\n",
        "\n",
        "# Use the policy to chose an action\n",
        "action = choose_action(initial_obs)\n",
        "\n",
        "# Step the environment\n",
        "next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(\"Observation:\", initial_obs)\n",
        "print(\"Action:\", action)\n",
        "print(\"Next observation:\", next_obs)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Game is done:\", done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX9iZtu48UYn"
      },
      "source": [
        "### Episode return\n",
        "In RL we usually break the agents interactions with the environment up into **episodes**. The sum of all reward collected during an episode is what we call the episode's **return**. The goal in RL is for the agent to chose actions which maximise the expected future return. In CartPole the agent receives a reward of `1` for every timestep the pole is still upright. If the pole falls over, the game is over and the agent receives a reward of `0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6KZA1Nq9p47"
      },
      "source": [
        "### Agent-environment Loop\n",
        "Now that we know what a policy is and we know how to step the environment, lets close the agent-environment loop.\n",
        "\n",
        "**Exercise 5:** Write a function which runs one episode of CartPole by sequentially choosing actions and stepping the environment. You should use the stocastic policy we defined earlier to chose actions. The function should keep track of the reward received and output the return at the end of the episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buy0X7mi-gHP"
      },
      "outputs": [],
      "source": [
        "def run_episode(env):\n",
        "  episode_return = 0\n",
        "\n",
        "  ## YOUR CODE\n",
        "\n",
        "  # HINT: reset environment\n",
        "\n",
        "  # HINT: while loop until episode is done\n",
        "\n",
        "    # HINT: choose action\n",
        "\n",
        "    # HINT: step environment\n",
        "    \n",
        "    # HINT: add reward to episode_return\n",
        "\n",
        "  ## END CODE\n",
        "\n",
        "  return episode_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JAfPqaeS_Krs"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 5 solution\n",
        "\n",
        "def run_episode(env):\n",
        "  episode_return = 0\n",
        "  \n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = choose_action(obs)\n",
        "\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "    episode_return += reward\n",
        "\n",
        "    # Critical\n",
        "    obs = next_obs\n",
        "\n",
        "  return episode_return\n",
        "\n",
        "# TEST\n",
        "print(\"Episode return:\", run_episode(env))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGdzHxJnZGl"
      },
      "source": [
        "In CartPole, the environment is considered solved when the agent can reliably achieve an episode return of 500. As you can see, our current policy is nowhere near optimal yet. Lets learn a way to find an optimal policy.\n",
        "\n",
        "One way we can find an optimal policy is by randomly searching for it. Obviously in a complex environment finding an optimal policy by randomly trying differnet strategies could take forever. But CartPole is a sufficiently simple environment that it might just work.\n",
        "\n",
        "Before we implement random policy search, lets take a look at the following environment loop function that we implemented for you. Its the environmentloop we will be using for the rest of the notebook. We will use a [NamedTuple](https://www.geeksforgeeks.org/namedtuple-in-python/) to bundle `obs`, `action`, `reward`, `next_obs` and the done flag into a **transition** object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpxNZxbORjM0"
      },
      "outputs": [],
      "source": [
        "# Named tuple to store transition\n",
        "Transition = collections.namedtuple(\"Transition\", [\"obs\", \"action\", \"reward\", \"next_obs\", \"done\"])\n",
        "\n",
        "# TEST\n",
        "transition = Transition(\n",
        "    obs=[1,2,-1,2],\n",
        "    action=0,\n",
        "    reward=10,\n",
        "    next_obs=[1,2,2,1],\n",
        "    done=True\n",
        ")\n",
        "\n",
        "print(\"Transition obs:\", transition.obs)\n",
        "print(\"Transition action:\", transition.action)\n",
        "print(\"Transition reward:\", transition.reward)\n",
        "print(\"Transition next obs:\", transition.next_obs)\n",
        "print(\"Transition done:\", transition.done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSCMHsULvlky"
      },
      "source": [
        "Below is the environment loop function we have implemented for you. We reccomend reading through it and trying to understand it, but if anything is unclear, don't worry about it. It should all make more sense as we work through this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWBwz3zMRjM0"
      },
      "outputs": [],
      "source": [
        "# Environment loop\n",
        "def run_environment_loop(rng, env, agent_params, agent_select_action_func, \n",
        "    agent_actor_state=None, agent_learn_func=None, agent_learner_state=None, \n",
        "    agent_memory=None, num_episodes=1000, evaluator_period=100, \n",
        "    evaluation_episodes=32, learner_steps_per_episode=1):\n",
        "    \"\"\"\n",
        "    This function runs several episodes in an environment and periodically does \n",
        "    some agent learning and evaluation.\n",
        "    \n",
        "    Args:\n",
        "        rng: a random number generator. This is for jax.\n",
        "        env: a gym environment.\n",
        "        agent_params: an object to store parameters that the agent uses.\n",
        "        agent_select_func: a function that does action selection for the agent.\n",
        "        agent_actor_state (optional): an object that stores the internal state \n",
        "            of the agents action selection function.\n",
        "        agent_learn_func (optional): a function that does some learning for the \n",
        "            agent by updating the agent parameters.\n",
        "        agent_learn_state (optional): an object that stores the internal state \n",
        "            of the agent learn function.\n",
        "        agent_memory (optional): an object for storing an retrieving historical \n",
        "            experience.\n",
        "        num_episodes: how many episodes to run.\n",
        "        evaluator_period: how often to run evaluation.\n",
        "        evaluation_episodes: how many evaluation episodes to run.\n",
        "\n",
        "    Returns:\n",
        "        episode_returns: list of all the episode returns.\n",
        "        evaluator_episode_returns: list of all the evaluator episode returns.\n",
        "    \"\"\"\n",
        "    episode_returns = [] # List to store history of episode returns.\n",
        "    evaluator_episode_returns = [] # List to store history of evaluator returns.\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # Reset environment.\n",
        "        obs = env.reset()\n",
        "        episode_return = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # Agent select action.\n",
        "            action, agent_actor_state = agent_select_action_func(\n",
        "                                            next(rng), \n",
        "                                            agent_params, \n",
        "                                            agent_actor_state, \n",
        "                                            np.array(obs)\n",
        "                                        )\n",
        "\n",
        "            # Step environment.\n",
        "            next_obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "            # Pack into transition.\n",
        "            transition = Transition(obs, action, reward, next_obs, done)\n",
        "\n",
        "            # Add transition to memory.\n",
        "            if agent_memory: # check if agent has memory\n",
        "              agent_memory.push(transition)\n",
        "\n",
        "            # Add reward to episode return.\n",
        "            episode_return += reward\n",
        "\n",
        "            # Set obs to next obs before next environment step. CRITICAL!!!\n",
        "            obs = next_obs\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "        # At the end of every episode we do a learn step.\n",
        "        if agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "\n",
        "            for _ in range(learner_steps_per_episode):\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng), \n",
        "                                                        agent_params, \n",
        "                                                        agent_learner_state, \n",
        "                                                        memory\n",
        "                                                    )\n",
        "\n",
        "        if (episode % evaluator_period) == 0: # Do evaluation\n",
        "\n",
        "            evaluator_episode_return = 0\n",
        "            for eval_episode in range(evaluation_episodes):\n",
        "                obs = env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action, _ = agent_select_action_func(\n",
        "                                    next(rng), \n",
        "                                    agent_params, \n",
        "                                    agent_actor_state, \n",
        "                                    np.array(obs), \n",
        "                                    evaluation=True\n",
        "                                )\n",
        "\n",
        "                    obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "                    evaluator_episode_return += reward\n",
        "\n",
        "            evaluator_episode_return /= evaluation_episodes\n",
        "\n",
        "            evaluator_episode_returns.append(evaluator_episode_return)\n",
        "\n",
        "            logs = [\n",
        "                    f\"Episode: {episode}\",\n",
        "                    f\"Episode Return: {episode_return}\",\n",
        "                    f\"Average Episode Return: {np.mean(episode_returns[-20:])}\",\n",
        "                    f\"Evaluator Episode Return: {evaluator_episode_return}\"\n",
        "            ]\n",
        "\n",
        "            print(*logs, sep=\"\\t\") # Print the logs\n",
        "\n",
        "    return episode_returns, evaluator_episode_returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDhgij_UwWRn"
      },
      "source": [
        "Before we can test this environment loop function with our policy function we implemented earlier, we will need to modify it so that its interface matches the way our environment loop expects it. The `select_action` function should take a random seed in the first argument position, then parameters, then the actors internal state (more on this later), then the observation and finally a `evaluation` boolean flag that indicates if the function is being called during the evaluation loop or not (more on this later). The function should return the chosen action and the next state of the actor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zujdaD_HwqZ3"
      },
      "outputs": [],
      "source": [
        "def select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    \"\"\"\n",
        "    This function assums params is a vecotor of same size as obs.\n",
        "    It computes the dot product between params and obs. If the result is\n",
        "    less than zero it returns actions zero. If the result is greater than\n",
        "    or equal to zero, it returns action 1.\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(params, obs)\n",
        "    \n",
        "    if dot_product >= 0:\n",
        "      action = 1\n",
        "    else:\n",
        "      action = 0\n",
        "\n",
        "    return action, actor_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v6VIqYozHDX"
      },
      "source": [
        "**Exercise 6:** Can you convert this into a function that only uses Jax methods so that we can jit the function?\n",
        "\n",
        "\n",
        "**Useful functions:** \n",
        "*   [Jax dot product](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html)\n",
        "*   [Jax select](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html#jax.lax.select)\n",
        "\n",
        "**Note:**\n",
        "\n",
        "We already imported `jax.numpy` as `jnp`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG3-qk4X0pyE"
      },
      "outputs": [],
      "source": [
        "def select_action(key, params, actor_state, obs, evaluation=False):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action, actor_state\n",
        "\n",
        "# TEST: the result of your function should be action 1 in this test\n",
        "key = None # not used\n",
        "actor_state = None # not used\n",
        "select_action_jit = jax.jit(select_action) # jit the function\n",
        "params = np.array([1,1,-1,-1], \"float32\")\n",
        "obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "action, actor_state = select_action_jit(key, params, actor_state, obs)\n",
        "print(\"Action:\", action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cRAQiMPwzZFw"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 6 solution\n",
        "\n",
        "def select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    dot_product = jnp.dot(params, obs)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        dot_product >= 0.0,\n",
        "        1,\n",
        "        0,\n",
        "    )\n",
        "\n",
        "    return action, actor_state\n",
        "\n",
        "# TEST: the result of your function should be action 1 in this test\n",
        "key = None # not used\n",
        "actor_state = None # not used\n",
        "select_action_jit = jax.jit(select_action) # jit the function\n",
        "\n",
        "params = np.array([1,1,-1,-1], \"float32\")\n",
        "obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "action, actor_state = select_action_jit(key, params, actor_state, obs)\n",
        "print(\"Action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpAqVqVY_DcR"
      },
      "source": [
        "We can now test our `select_action` function in the `run_environment_loop` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ2uN-kh_Qpm"
      },
      "outputs": [],
      "source": [
        "# Jax random number generator\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(0)) # don't worry about this for now\n",
        "\n",
        "# Some arbitrary parameters\n",
        "params = np.array([1,1,-1,-1], \"float32\")\n",
        "\n",
        "episode_returns, evaluator_returns = run_environment_loop(\n",
        "                                        rng, \n",
        "                                        env, \n",
        "                                        params,\n",
        "                                        select_action_jit,\n",
        "                                        num_episodes=1001,\n",
        "                                      )\n",
        "\n",
        "print(\"Average episode returns:\", np.mean(episode_returns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zu51ep7Sh0M"
      },
      "source": [
        "### Agent memory\n",
        "\n",
        "In many RL algorithms the agent uses a kind of memory to store some of the experiences it had in the environment. The interface we will use for the agent's memory is very simple. It will have a function `memory.push(<transition>)` that adds some information about the transition to the memory, a function `memory.is_ready()` to check if the memory is ready to do some learning, and fianlly a function `memory.sample()` that returns some information that the agent learn function can use to do learning.\n",
        "\n",
        "#### Average Episode Return Memory\n",
        "We have built a simple agent memory module for you below. It stores the `epsisode_returns` of the last 20 episodes. Read through our implementation below and see if you can understand it. The `memory.sample()` method returns the average episode return over the last 20 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVkTBIK5RjM1"
      },
      "outputs": [],
      "source": [
        "# A NamedTuple to store the average episode return of the last 20 runs\n",
        "AverageEpisodeReturnMemory = collections.namedtuple(\n",
        "                                \"AverageEpisodeReturnMemory\", \n",
        "                                [\"average_episode_return\"]\n",
        "                            )\n",
        "\n",
        "class AverageEpisodeReturnBuffer:\n",
        "\n",
        "    def __init__(self, num_episodes_to_store=20):\n",
        "        \"\"\"\n",
        "        This class implements an agent memory that stores the average episode \n",
        "        return over the last 20 episodes.\n",
        "        \"\"\"\n",
        "        self.num_episodes_to_store = num_episodes_to_store\n",
        "        self.episode_return_buffer = []\n",
        "        self.current_episode_return = 0\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_return += transition.reward\n",
        "\n",
        "        if transition.done: # If the episode is done\n",
        "            # Add episode return to buffer\n",
        "            self.episode_return_buffer.append(self.current_episode_return)\n",
        "\n",
        "            # Reset episode return\n",
        "            self.current_episode_return = 0\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.episode_return_buffer) == self.num_episodes_to_store\n",
        "\n",
        "    def sample(self):\n",
        "        average_episode_return = np.mean(self.episode_return_buffer)\n",
        "\n",
        "        # Clear episode return buffer\n",
        "        self.episode_return_buffer = []\n",
        "\n",
        "        return AverageEpisodeReturnMemory(average_episode_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPlIq4oDBPY"
      },
      "source": [
        "## Section 2: Random Policy Search (RPS)\n",
        "Now we are ready to implement the random policy search algorithm which will randomly try different policies and keep track of the best policy it has found so far. We will say that policy $A$ is better than policy $B$ if the average episode return policy $A$ achieved over the last 20 episodes is greater than that of policy $B$. We will need to modify the way we store the agents parameters so that we always have access to the latest parameters as well as the best parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DcaC-PQRjM1"
      },
      "outputs": [],
      "source": [
        "# Parameter container for random policy search\n",
        "RandomPolicySearchParams = collections.namedtuple(\"RandomPolicySearchParams\", [\"current\", \"best\"])\n",
        "\n",
        "# TEST: store two different sets of parameters\n",
        "current_params = np.array([1,1,-1,-1])\n",
        "best_params = np.array([0,0,0,0])\n",
        "rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "\n",
        "print(f\"Best params: {rps_params.best}\")\n",
        "print(f\"Current params: {rps_params.current}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tExceGeGNYH"
      },
      "source": [
        "### RPS select action function\n",
        "Now lets once again modify our `select_action` function such that it uses the best parameters when when `evaluation==True` and uses the current parameters when `evaluation==False`.\n",
        "\n",
        "**Exercise 7:** Implement the `random_policy_search_select_action` function as described above. Make sure you use Jax so that we can jit the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kmwT35JRjM1"
      },
      "outputs": [],
      "source": [
        "def random_policy_search_select_action(\n",
        "    key, \n",
        "    params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        "):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  # HINT: best_action = ... (two lines)\n",
        "\n",
        "  # HINT: current_action = ... (two lines)\n",
        "\n",
        "  # HINT: action = best_action if evaluation else current action (one line)\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action, actor_state\n",
        "\n",
        "# TEST:\n",
        "key = None # not used\n",
        "actor_state = None # not used\n",
        "random_policy_search_select_action_jit = jax.jit(\n",
        "                                            random_policy_search_select_action\n",
        "                                            ) # jit the function\n",
        "# Parameters\n",
        "current_params = np.array([-1,-1,-1,-1])\n",
        "best_params = np.array([0,0,0,0])\n",
        "rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "\n",
        "# Observation\n",
        "obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "current_action, actor_state = random_policy_search_select_action_jit(\n",
        "    key, \n",
        "    rps_params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        ")\n",
        "\n",
        "best_action, actor_state = random_policy_search_select_action_jit(\n",
        "    key, \n",
        "    rps_params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=True\n",
        ")\n",
        "\n",
        "print(\"Current action:\", current_action)\n",
        "print(\"Best action:\", best_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH_9qJ08IFg6"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 7 solution\n",
        "\n",
        "def random_policy_search_select_action(\n",
        "    key, \n",
        "    params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        "):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  dot_product = jnp.dot(params.best, obs)\n",
        "  best_action = jax.lax.select(\n",
        "      dot_product >= 0,\n",
        "      1,\n",
        "      0\n",
        "  )\n",
        "\n",
        "  dot_product = jnp.dot(params.current, obs)\n",
        "  current_action = jax.lax.select(\n",
        "      dot_product >= 0,\n",
        "      1,\n",
        "      0\n",
        "  )\n",
        "\n",
        "  action = jax.lax.select(\n",
        "      evaluation,\n",
        "      best_action,\n",
        "      current_action\n",
        "  )\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action, actor_state\n",
        "\n",
        "# TEST:\n",
        "key = None # not used\n",
        "actor_state = None # not used\n",
        "random_policy_search_select_action_jit = jax.jit(\n",
        "                                            random_policy_search_select_action\n",
        "                                            ) # jit the function\n",
        "# Parameters\n",
        "current_params = np.array([-1,-1,-1,-1])\n",
        "best_params = np.array([0,0,0,0])\n",
        "rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "\n",
        "# Observation\n",
        "obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "current_action, actor_state = random_policy_search_select_action_jit(\n",
        "    key, \n",
        "    rps_params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        ")\n",
        "\n",
        "best_action, actor_state = random_policy_search_select_action_jit(\n",
        "    key, \n",
        "    rps_params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=True\n",
        ")\n",
        "\n",
        "print(\"Current action:\", current_action)\n",
        "print(\"Best action:\", best_action)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oXBsSa2KjWE"
      },
      "source": [
        "### RPS learn function\n",
        "Now we need to implement a learn function for our random policy search agent. The learn function is quite simple. All we need to do is check if the current weights are better than the best weights. If they are better, then set the current weights to be the new best weights and randomly sample a new set of current weights. Lets assume our learn function receives a memory from the AverageEpisodeReturnMemory we implmented earlier. We can uses this to compare the current weights to the best weights. We will need to keep track of the best average episode return for the learn function. For that we can use the `learn_state` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cSH4uYmRjM2"
      },
      "outputs": [],
      "source": [
        "# A NamedTuple to store the best average episode return so far\n",
        "LearnerState = collections.namedtuple(\n",
        "                                      \"LearnerState\", \n",
        "                                      [\"count\", \"best_average_episode_return\"]\n",
        "                                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Djxc9j-LzkM"
      },
      "source": [
        "**Exercise 8:** Write a function to randomly sample new weights using jax. The weights should be sampled from the interval [-2,2].\n",
        "\n",
        "**Useful functions:** \n",
        "*   [Jax random uniform sample](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.uniform.html#jax.random.uniform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V2yFM2XMjGW"
      },
      "outputs": [],
      "source": [
        "def get_new_random_weights(random_key, old_weights):\n",
        "    new_weights_shape = old_weights.shape\n",
        "    new_weights_dtype = old_weights.dtype\n",
        "    new_weights = None # you should overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    # new_weights = ...\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "# TEST \n",
        "old_weights = np.array([1,1,1,1], \"float32\")\n",
        "get_new_random_weights_jit = jax.jit(get_new_random_weights) # jit the function\n",
        "random_key = next(rng) # get net random key from the random number generator\n",
        "new_weights = get_new_random_weights_jit(random_key, old_weights)\n",
        "print(\"New weights:\", new_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T05R_AWHLx_I"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 8 solution\n",
        "\n",
        "def get_new_random_weights(random_key, old_weights):\n",
        "    new_weights_shape = old_weights.shape\n",
        "    new_weights_dtype = old_weights.dtype\n",
        "    new_weights = None # you should overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    new_weights = jax.random.uniform(\n",
        "                      random_key, \n",
        "                      new_weights_shape, \n",
        "                      new_weights_dtype,\n",
        "                      minval=-2.0,\n",
        "                      maxval=2.0\n",
        "                  )\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "# TEST \n",
        "old_weights = np.array([1,1,1,1], \"float32\")\n",
        "get_new_random_weights_jit = jax.jit(get_new_random_weights) # jit the function\n",
        "random_key = next(rng) # get net random key from the random number generator\n",
        "new_weights = get_new_random_weights(random_key, old_weights)\n",
        "print(\"New weights:\", new_weights)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxZXdP8bOu6b"
      },
      "source": [
        "Now lets implement the random policy search function.\n",
        "\n",
        "**Exercise 9:** Use the description of the random policy search learn function at the top of this section to complete the function below. Try to use jax (remember `jax.lax.select()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ElX1w1iQTrE"
      },
      "outputs": [],
      "source": [
        "def random_policy_search_learn(key, params, learner_state, memory):\n",
        "    best_weights = params.best \n",
        "    current_weights = params.current\n",
        "\n",
        "    average_episode_return = memory.average_episode_return\n",
        "    best_average_episode_return = learner_state.best_average_episode_return\n",
        "\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # HINT: if current better than best then ...\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    # Generate new random weights\n",
        "    new_weights = get_new_random_weights_jit(key, best_weights)\n",
        "\n",
        "    # Bundle weights in RPS Params NamedTuple\n",
        "    params = RandomPolicySearchParams(current=new_weights, best=best_weights)\n",
        "\n",
        "    # Increment the learn counter by one\n",
        "    learn_count = learner_state.count + 1\n",
        "\n",
        "    return params, LearnerState(learn_count, best_average_episode_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkUcO2efRjM2"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 9 solution\n",
        "def random_policy_search_learn(key, params, learner_state, memory):\n",
        "\n",
        "    best_weights = params.best \n",
        "    current_weights = params.current\n",
        "\n",
        "    current_episode_return = memory.average_episode_return\n",
        "    best_average_episode_return = learner_state.best_average_episode_return\n",
        "\n",
        "    best_weights = jax.lax.select(\n",
        "        current_episode_return > best_average_episode_return,\n",
        "        current_weights,\n",
        "        best_weights\n",
        "    )\n",
        "        \n",
        "    best_average_episode_return = jax.lax.select(\n",
        "        current_episode_return > best_average_episode_return,\n",
        "        current_episode_return,\n",
        "        best_average_episode_return\n",
        "    )\n",
        "\n",
        "    # Generate new random weights\n",
        "    new_weights = get_new_random_weights_jit(key, best_weights)\n",
        "\n",
        "    # Bundle weights in RPS Params NamedTuple\n",
        "    params = RandomPolicySearchParams(current=new_weights, best=best_weights)\n",
        "\n",
        "    # Increment the learn counter by one\n",
        "    learn_count = learner_state.count + 1\n",
        "\n",
        "    return params, LearnerState(learn_count, best_average_episode_return)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ol_AxnMBdgP"
      },
      "source": [
        "Now we can put everything together using the environment loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx57tf7vRjM3"
      },
      "outputs": [],
      "source": [
        "initial_learner_state = LearnerState(0, -float(\"inf\"))\n",
        "\n",
        "# Jit the learn function for some extra speed\n",
        "random_policy_search_learn_jit = jax.jit(random_policy_search_learn)\n",
        "\n",
        "initial_weights = np.array([1,1,1,1], \"float32\")\n",
        "initial_params = RandomPolicySearchParams(initial_weights, initial_weights)\n",
        "\n",
        "memory = AverageEpisodeReturnBuffer(num_episodes_to_store=20)\n",
        "\n",
        "episode_return, evaluator_episode_returns = run_environment_loop(\n",
        "                                        rng, \n",
        "                                        env, \n",
        "                                        initial_params, \n",
        "                                        random_policy_search_select_action_jit, \n",
        "                                        None, # no actor state\n",
        "                                        random_policy_search_learn_jit, \n",
        "                                        initial_learner_state, \n",
        "                                        memory, \n",
        "                                        num_episodes=2001\n",
        "                                    )\n",
        "\n",
        "# Plot graph of evaluator episode returns\n",
        "plt.plot(evaluator_episode_returns)\n",
        "plt.title(\"Random Search Evaluator Episode Return\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG10FG6uS05A"
      },
      "source": [
        "Hopefully you found a set of optimal parameters on CartPole. You will know you have, if the evaluator episode return eventually reaches 500. If you didn't find optimal parameters, try running the environment loop again, you were probably just a little unlucky. That is the big limitation with random policy search after all, if you are unlucky you might not (randomly) stumble on the optimal policy.\n",
        "\n",
        "So, in random policy search there is very little (if any) real learning going on. Next, lets look to implementing a simple RL algorithm instead, that can use its experience to guide the learning process, rather than just randomly sampling new weights. Hopefully this will help us more reliably find an optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEnSjZVESrxc"
      },
      "source": [
        "## Section 3: Policy Gradients (PG)\n",
        "As discussed, the goal in RL is to find a policy which maximise the expected cummulative reward (return) the agent receives from the environment. We can write the expected return of a policy as:\n",
        "\n",
        "$J(\\pi_\\theta)=\\mathrm{E}_{\\tau\\sim\\pi_\\theta}\\ [R(\\tau)]$,\n",
        "\n",
        "where $\\pi_\\theta$ is a policy parametrised by $\\theta$, $\\mathrm{E}$ means *expectation*, $\\tau$ is shorthand for \"*episode*\", $\\tau\\sim\\pi_\\theta$ is shorthand for \"*episodes sampled using the policy* $\\pi_\\theta$\", and $R(\\tau)$ is the return of episode $\\tau$.\n",
        "\n",
        "Then, the goal in RL is to find the parameters $\\theta$ that maximise the function $J(\\pi_\\theta)$. One way to find these parameters is to perform gradient ascent on $J(\\pi_\\theta)$ with respect to the parameters $\\theta$: \n",
        "\n",
        "$\\theta_{k+1}=\\theta_k + \\alpha \\nabla J(\\pi_\\theta)|_{\\theta_{k}}$,\n",
        "\n",
        "where $\\nabla J(\\pi_\\theta)|_{\\theta_{k}}$ is the gradient of the expected return with respect to the policy parameters $\\theta_k$ and $\\alpha$ is the step size. This quantity, $\\nabla J(\\pi_\\theta)$, is also called the **policy gradient** and is very important in RL. If we can comput the policy gradient, theat we will have a means by which to directly optimise our policy.\n",
        "\n",
        "As it turns out, there is a way for us to compute the policy gradient and the mathematical derivation can be found [here](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html). But for this tutorial we will ommit the derivation and just give you the result:\n",
        "\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) R(\\tau)]$\n",
        "\n",
        "Informaly, the policy gradient is equal to the gradient of the log of the probability of the action chosen multiplied by the return of the episode in which the action was taken.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTnTzgtSuy-y"
      },
      "source": [
        "### REINFORCE\n",
        "REINFORCE is a simple RL algorithm that uses the policy gradient to find the optimal policy by increasing the probability of choosing actions that tend to lead to high return episodes.\n",
        "\n",
        "**Exercise 10:** Implement a function that takes the probability of an action and the return of the episode the action was taken in and returns the log of the probability multiplied by the return. Make sure you use jax so that we can jit the function.\n",
        "\n",
        "**Usefull functions:**\n",
        "*   [Jax numpy log](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.log.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJObUsoUrOyV"
      },
      "outputs": [],
      "source": [
        "def compute_weighted_log_prob(action_prob, episode_return):\n",
        "    weighted_log_prob = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return weighted_log_prob\n",
        "\n",
        "# TEST: the result should be -22.314354\n",
        "action_prob = 0.8\n",
        "episode_return = 100\n",
        "print(\"Weighted log prob:\", compute_weighted_log_prob(action_prob, episode_return))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x7dTAlCdrJkf"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 10 solution\n",
        "\n",
        "def compute_weighted_log_prob(action_prob, episode_return):\n",
        "    weighted_log_prob = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    weighted_log_prob = jnp.log(action_prob) * episode_return\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return weighted_log_prob\n",
        "\n",
        "# TEST\n",
        "action_prob = 0.8\n",
        "episode_return = 100\n",
        "print(\"Weighted log prob:\", compute_weighted_log_prob(action_prob, episode_return))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmgW9UJ3tIpl"
      },
      "source": [
        "### Rewards-to-go\n",
        "The gradient of the log of the actions probability, weighted by the return of the episode will tend to push up the probability of actions that were in episodes whith high return, regardless of where in the episode the action was taken. This does not really make much sense because an action near the end of an episode may be reinforced because lots of reward was collected earlier on in the episode, before the action was taken. RL agents should really only reinforce actions on the basis of their *consequences*. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come after. The cummulative rewards received after an action was taken is called the **rewards-to-go** and can be computed as:\n",
        "\n",
        "$\\hat{R}_t=\\sum_t^Tr_t$\n",
        "\n",
        "Compare the rewards-to-go with the episode return:\n",
        "\n",
        "$R(\\tau)=\\sum_{t=0}^Tr_t$\n",
        "\n",
        "Thus, the policy gradient with rewards-to-go is given by:\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\hat{R}_t]$\n",
        "\n",
        "**Exercise 11:** Implement a function that takes a list of all the rewards obtained in an episode and computes the rewards-to-go. Don't worry about using jax in this function. You can use regular Python operations like `for-loops`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV1Hww8E3dUJ"
      },
      "outputs": [],
      "source": [
        "# Implement reward to go\n",
        "def rewards_to_go(rewards):\n",
        "    \"\"\"\n",
        "    This function should take a list of rewards as input and \n",
        "    compute the rewards-to-go for each timestep.\n",
        "    \n",
        "    Arguments:\n",
        "        rewards[t] is the reward at time step t.\n",
        "\n",
        "    Returns:\n",
        "        rewards_to_go[t] should be the reward-to-go at timestep t.\n",
        "    \"\"\"\n",
        "\n",
        "    rewards_to_go = []\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return rewards_to_go\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2qy2F3xRjM3"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 11 solution\n",
        "\n",
        "def rewards_to_go(rewards):\n",
        "    rewards_to_go = []\n",
        "    for i in range(len(rewards)):\n",
        "        r2g = 0\n",
        "        for j in range(i, len(rewards)):\n",
        "            r2g += rewards[j]\n",
        "        rewards_to_go.append(r2g)\n",
        "    return rewards_to_go\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IsJg3PxVV2e"
      },
      "outputs": [],
      "source": [
        "# Faster rewards to go calculation using numpy\n",
        "def rewards_to_go(rewards):\n",
        "    return np.flip(np.cumsum(np.flip(rewards)))\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboxN9MS65i5"
      },
      "source": [
        "Next we will need to make a new agent memory to store the rewards-to-go $R_t$ along with the observation $o_t$ and action $a_t$ at every timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhS4V6auRjM3"
      },
      "outputs": [],
      "source": [
        "# Now we need a new episode memory buffer\n",
        "\n",
        "EpisodeRewardsToGoMemory = collections.namedtuple(\"AverageEpisodeReturnMemory\", [\"obs\", \"action\", \"reward_to_go\"])\n",
        "\n",
        "class EpisodeRewardsToGoBuffer:\n",
        "\n",
        "    def __init__(self, num_transitions_to_store=500, batch_size=500):\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_buffer = collections.deque(maxlen=num_transitions_to_store)\n",
        "        self.current_episode_transition_buffer = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_transition_buffer.append(transition)\n",
        "\n",
        "        if transition.done:\n",
        "\n",
        "            episode_rewards = []\n",
        "            for t in self.current_episode_transition_buffer:\n",
        "                episode_rewards.append(t.reward)\n",
        "\n",
        "            r2g = rewards_to_go(episode_rewards)\n",
        "\n",
        "            for i, t in enumerate(self.current_episode_transition_buffer):\n",
        "                memory = EpisodeRewardsToGoMemory(t.obs, t.action, r2g[i])\n",
        "                self.memory_buffer.append(memory)\n",
        "\n",
        "            # Reset episode buffer\n",
        "            self.current_episode_transition_buffer = []\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.memory_buffer) >= self.batch_size\n",
        "\n",
        "    def sample(self):\n",
        "        random_memory_sample = random.sample(self.memory_buffer, self.batch_size)\n",
        "\n",
        "        obs_batch, action_batch, reward_to_go_batch = zip(*random_memory_sample)\n",
        "\n",
        "        return EpisodeRewardsToGoMemory(\n",
        "            np.stack(obs_batch).astype(\"float32\"), \n",
        "            np.asarray(action_batch).astype(\"int32\"), \n",
        "            np.asarray(reward_to_go_batch).astype(\"int32\")\n",
        "        )\n",
        "\n",
        "\n",
        "# Instantiate Memory\n",
        "REINFORCE_memory = EpisodeRewardsToGoBuffer(num_transitions_to_store=512, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idkav_aSYXvz"
      },
      "source": [
        "### Policy neural network\n",
        "Next, we need to aproximate the policy using a simple neural network. Our policy neural network will have an input layer that takes the observation as input and passes it through two hidden layers and then outputs one scalar value for each of the possible actions. So, in CartPole the output layer will have size `2`.\n",
        "\n",
        "[Haiku](https://github.com/deepmind/dm-haiku) is a library for implementing neural networks is Jax. Below we have implemented a simple function to make the policy network for you. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2XO7VkORjM4"
      },
      "outputs": [],
      "source": [
        "def make_policy_network(num_actions: int, layers=[10, 10]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for the policy.\"\"\"\n",
        "\n",
        "  def policy_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [\n",
        "            hk.Flatten(),\n",
        "            hk.nets.MLP(layers + [num_actions])\n",
        "        ]\n",
        "    )\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(policy_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GR2y8FjaG-G"
      },
      "source": [
        "Haiku networks have two important functions you need to know about. The first is the `<network>.init(<rng>, <input>)`, which returns a set of random initial parameters. The second method is the `<network>.apply(<params>, <input>)` which passes an input through the network using the set of parameters provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJrn9o-Vatkw"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "POLICY_NETWORK = make_policy_network(num_actions=2, layers=[20,20])\n",
        "random_key = next(rng) # get next random key\n",
        "dummy_obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "REINFORCE_params = POLICY_NETWORK.init(random_key, dummy_obs)\n",
        "print(\"Initial params:\", REINFORCE_params.keys())\n",
        "\n",
        "output = POLICY_NETWORK.apply(REINFORCE_params, dummy_obs)\n",
        "print(\"Policy network output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlouUBvoeunz"
      },
      "source": [
        "The outputs of our policy network are [logits](https://qr.ae/pv4YTe). To convert this into a probability distribution over actions we pass the logits to the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function (more on this later).\n",
        "\n",
        "### Action selection\n",
        "\n",
        "**Exercise 12:** Complete the function below which takes a vector of logits and randomly samples an action. \n",
        "\n",
        "**Useful functions:**\n",
        "*   [Jax random categorical](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.categorical.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Z8DxUmeOGJ"
      },
      "outputs": [],
      "source": [
        "def sample_action(random_key, logits):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # END YOUR code\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(\"Action:\", sample_action(next(rng), np.array([0, 1], \"float32\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2huSgyukg1N4"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 12 solution\n",
        "\n",
        "def sample_action(random_key, logits):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    action = jax.random.categorical(random_key, logits)\n",
        "    # END YOUR code\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST: \n",
        "for i in range(10):\n",
        "  print(\"Action:\", sample_action(next(rng), np.array([0, 1], \"float32\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn2yzwDrhmUI"
      },
      "source": [
        "Notice in the tests that the actions are randomly sampled. Ofcourse the action with the higher probability will be chose more often because, but there is always a chance the action with the lower probability will be chosen. This is actually desirable in RL because it mean the agent will always try new things in the environment. We call this **exploring**. Exploring is important because it helps the agent discover new, possibly better, strategies in the environment. When an agent chooses the best possible action (given its current knowledge) we say the agent is being **greedy**.\n",
        "\n",
        "**Exercise 13:** Complete the function below which takes a vector of logits and returns the greedy action. \n",
        "\n",
        "**Useful functions:**\n",
        "*   [Jax numpy argmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.argmax.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG_4qq6RjLCg"
      },
      "outputs": [],
      "source": [
        "def greedy_action(logits):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "    print(\"Action:\", greedy_action(np.array([0, 1], \"float32\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r2sa7wqFjHb-"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 13 solution\n",
        "\n",
        "def greedy_action(logits):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "    action = jnp.argmax(logits)\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "    print(\"Action:\", greedy_action(np.array([0, 1], \"float32\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MffEyZUjyaF"
      },
      "source": [
        "Notice that the greed action selector always chooses the same action, namely the one with the highest probability (or equivalently the largets logit). Next, we will implement the REINFORCE `select_action` function. The function passes the observation through the policy neural network to get loggits and then uses thos to chose an action. If `evaluation` is `True`, the greedy action will be chosen. Otherwise, the action is sampled from the action probability distribution given by the logits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP5UH87VRjM4"
      },
      "outputs": [],
      "source": [
        "def REINFORCE_select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim\n",
        "    logits = POLICY_NETWORK.apply(params, obs)[0] # remove batch dim\n",
        "\n",
        "    sampled_action = sample_action(key, logits)\n",
        "\n",
        "    best_action = greedy_action(logits)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        evaluation,\n",
        "        best_action,\n",
        "        sampled_action\n",
        "    )\n",
        "    \n",
        "    return action, actor_state\n",
        "\n",
        "# TEST\n",
        "action, actor_state = REINFORCE_select_action(\n",
        "    key=next(rng),\n",
        "    params=REINFORCE_params, # we instantiated this earlier\n",
        "    actor_state=None, # not used\n",
        "    obs=np.array([1,1,1,1], \"float32\") # dummy obs\n",
        ")\n",
        "\n",
        "print(\"Action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDhTH3culwqo"
      },
      "source": [
        "Now that we finished the REINFORCE action selection function, all we have left to do is make a REINFORCE learn function. The learn function should use the `weighted_log_prob` function we made earlier to compute the policy gradient and apply the updates to our neural network.\n",
        "\n",
        "### Network Optimiser\n",
        "\n",
        "To apply updates to our neural network we will use a Jax library called [Optax](https://github.com/deepmind/optax). Optax has an implementation of the [Adam optimizer](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/) which we can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxXINlMHP5Ic"
      },
      "outputs": [],
      "source": [
        "REINFORCE_OPTIMIZER = optax.adam(1e-3)\n",
        "REINFORCE_optim_state = REINFORCE_OPTIMIZER.init(REINFORCE_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ALCJESQJ8e"
      },
      "source": [
        "### Policy gradient loss\n",
        "\n",
        "**Exercise 14:** Complete the `pg_loss` function below.\n",
        "\n",
        "**Useful methods:**\n",
        "*   [Jax softmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html)\n",
        "*   [Jax one-hot vector](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.one_hot.html)\n",
        "*   [Jax dot product](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sUKkqx0RjM4"
      },
      "outputs": [],
      "source": [
        "def pg_loss(action, logits, reward_to_go):\n",
        "    chosen_action_prob = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # HINT all_action_probs = ... (convert logits into probs)\n",
        "\n",
        "    # HINT extract the prob of the desired action.\n",
        "    # One way to achieve this is to use a one-hot vector and a dot product...?\n",
        "\n",
        "    # END YOUR CODE\n",
        "    weighted_log_prob = compute_weighted_log_prob(\n",
        "                            chosen_action_prob, \n",
        "                            reward_to_go\n",
        "                        )\n",
        "    \n",
        "    loss = - weighted_log_prob # negative because we want gradient `ascent`\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# TEST \n",
        "print(\"Policy gradient loss:\", pg_loss(0, np.array([0,1], \"float32\"), 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1HAxSFU0qBVo"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 14 solution\n",
        "\n",
        "def pg_loss(action, logits, reward_to_go):\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    all_action_probs = jax.nn.softmax(logits)\n",
        "    action_mask = jax.nn.one_hot(action, logits.shape[0])\n",
        "    chosen_action_prob = jnp.dot(all_action_probs, action_mask)\n",
        "\n",
        "    # END YOUR CODE\n",
        "    \n",
        "    weighted_log_prob = compute_weighted_log_prob(\n",
        "                            chosen_action_prob, \n",
        "                            reward_to_go\n",
        "                        )\n",
        "    \n",
        "    loss = - weighted_log_prob \n",
        "    \n",
        "    return loss\n",
        "\n",
        "# TEST \n",
        "print(\"Policy gradient loss:\", pg_loss(0, np.array([0,1], \"float32\"), 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzuqx1jJrwVx"
      },
      "source": [
        "Now, when we do a policy gradient update step we are going to want to do it using a batch of experience, rather than just a single experience like above. We can use Jax's [vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap) function to easily make our `pg_loss` function work on a batch of experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yq4naLURjM4"
      },
      "outputs": [],
      "source": [
        "def batched_pg_loss(params, obs_batch, action_batch, reward_to_go_batch):\n",
        "    logits_batch = POLICY_NETWORK.apply(params, obs_batch) # network we made earlier\n",
        "    pg_loss_batch = jax.vmap(pg_loss)(action_batch, logits_batch, reward_to_go_batch) # add batch\n",
        "    mean_pg_loss = jnp.mean(pg_loss_batch)\n",
        "    return mean_pg_loss\n",
        "\n",
        "# TEST\n",
        "obs_batch = np.array([[1,0,0,1],[1,0,0,1],[1,0,0,1]])\n",
        "actions_batch = np.array([1,0,0])\n",
        "rew2go_batch = np.array([2.3, 4.3, 2.1])\n",
        "\n",
        "loss = batched_pg_loss(REINFORCE_params, obs_batch, actions_batch, rew2go_batch)\n",
        "\n",
        "print(\"PG loss on batch:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViENrHOALbCw"
      },
      "source": [
        "Now we can make the REINFORCE learn function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQr2Uz5ORjM5"
      },
      "outputs": [],
      "source": [
        "REINFORCELearnerState = collections.namedtuple(\"LearnerState\", [\"optim_state\"])\n",
        "\n",
        "def REINFORCE_learn(key, params, learner_state, memory):\n",
        "    \n",
        "    #Get Policy gradient by using `jax.grad()` on `batched_pg_loss`\n",
        "    grad_loss = jax.grad(batched_pg_loss)(params, memory.obs, memory.action, memory.reward_to_go)\n",
        "\n",
        "    # Get param updates using gradient and optimizer\n",
        "    updates, new_optim_state = REINFORCE_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "    # Apply updates to params\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return params, REINFORCELearnerState(new_optim_state) # update learner state\n",
        "\n",
        "# Lets jit the learn function for some extra speed\n",
        "REINFORCE_learn_jit = jax.jit(REINFORCE_learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5an3U2NhRKgG"
      },
      "source": [
        "Now we can train our REINFORCE agent by putting everything together using the environment loop. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vioIcVGsRjM5"
      },
      "outputs": [],
      "source": [
        "learner_state = REINFORCELearnerState(REINFORCE_optim_state)\n",
        "actor_state = None # not used\n",
        "\n",
        "episode_returns, evaluator_returns = run_environment_loop(rng, env, REINFORCE_params, jax.jit(REINFORCE_select_action), actor_state, \n",
        "    REINFORCE_learn_jit, learner_state, REINFORCE_memory, num_episodes=1001)\n",
        "\n",
        "# Plot the episode returns over time\n",
        "plt.plot(episode_returns)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_HHzFTOc-Qr"
      },
      "source": [
        "## Section 4: Q-Learning\n",
        "Another common aproach to finding an optimal policy in an environment in RL is via Q-learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1nOZrUSzhE"
      },
      "source": [
        "### Value functions\n",
        "In Q-learnig the agent learns a function that aproximates the **value** of states and actions. By *value* we mean return if you start in a particular state $s_t$, take a particular action $a_t$, and then act according to a particular policy $\\pi$ forever after. There are two types of **value functions**:\n",
        "1. The **state value function** which returns the expected value starting from a particular state.\n",
        "\n",
        "  $V_\\pi(s)=\\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s\\right]$\n",
        "\n",
        "2. The **state-action value function** which returns the expected value of starting from a particular state and choosing a particular action.\n",
        "\n",
        "  $Q_\\pi(s,a)=\\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s,\\ a_t=a\\right]$\n",
        "\n",
        "We say that the value function $V_\\pi(s)$ or $Q_\\pi(s,a)$ is the **optimal** value function if the policy $\\pi$ is an optimal policy. We denote the optimal value functions as follows:\n",
        "\n",
        "1.   $V_\\ast(s)=max_\\pi \\ \\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s\\right]$\n",
        "2.   $Q_\\ast(s)=max_\\pi \\  \\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s,\\ a_t=a\\right]$\n",
        "\n",
        "There is an important relationship between the optimal action $a_\\ast$ in a state $s$ and the optimal state-action value function $Q_\\ast$. Namely, the optimal action $a_\\ast$ in state $s$ is equal to the action that maximises the optimal state-action value function. This relationship naturally induces an optimal policy:\n",
        "\n",
        "$\\pi_\\ast(s)=\\mathop{argmax}_a\\ Q_\\ast(s, a)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2x2tqvZSihz"
      },
      "source": [
        "### Greedy action selection\n",
        "\n",
        "**Exercise 15:** Lets implement a function that, given a vector of Q-values, returns the action with the largets Q-value (i.e. the greedy action).\n",
        "\n",
        "**Useful methods:**\n",
        "*   [Jax argmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.argmax.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn9P1YdzTIDU"
      },
      "outputs": [],
      "source": [
        "# Implement a function takes q-values as input and returns the greedy_action\n",
        "def select_greedy_action(q_values):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "q_values = np.array([1, 3])\n",
        "print(\"Greedy action:\", select_greedy_action(q_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GsPv35lRjM6"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 15 solution\n",
        "\n",
        "\n",
        "def select_greedy_action(q_values):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE\n",
        "\n",
        "    action = jnp.argmax(q_values)\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "q_values = np.array([1, 3])\n",
        "print(\"Greedy action:\", select_greedy_action(q_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbHk03VVUHAV"
      },
      "source": [
        "### Random exploration\n",
        "\n",
        "**Exercise 16:** Now implement a function that, given the number of possible actions, returns a random action.\n",
        "\n",
        "**Useful methods:**\n",
        "\n",
        "*   [Jax random int](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.randint.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUKkpMLXUtko"
      },
      "outputs": [],
      "source": [
        "def select_random_action(key, num_actions):\n",
        "    action = None\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(f\"Random action number {i}: {select_random_action(next(rng), 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lrVadhcwRjM6"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 16 solution\n",
        "\n",
        "def select_random_action(key, num_actions):\n",
        "    action = None\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    action = jax.random.randint(\n",
        "        key, \n",
        "        shape=(1,), \n",
        "        minval=0, \n",
        "        maxval=num_actions\n",
        "    )[0] # important to take zeroth element\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(f\"Random action number {i}: {select_random_action(next(rng), 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-kKDFT6XU6y"
      },
      "source": [
        "**Exercise 17:** Implement a function that takes the number of timesteps as input and returns the current epsilon value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTpezAeUYFR7"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 17 solution\n",
        "\n",
        "EPSILON_DECAY_TIMESTEPS = 3000\n",
        "EPSILON_MIN = 0.05 # 5%\n",
        "\n",
        "def get_epsilon(num_timesteps):\n",
        "    epsilon = 1.0 - num_timesteps / EPSILON_DECAY_TIMESTEPS\n",
        "\n",
        "    epsilon = jax.lax.select(\n",
        "        epsilon < EPSILON_MIN,\n",
        "        EPSILON_MIN,\n",
        "        epsilon\n",
        "    )\n",
        "\n",
        "    return epsilon\n",
        "\n",
        "# TEST\n",
        "\n",
        "print(\"Epsilon after 10 timesteps:\", get_epsilon(10))\n",
        "print(\"Epsilon after 10 010 timesteps:\", get_epsilon(10_010))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t56oo58TVQ_s"
      },
      "source": [
        "### Epsilon-greedy action selection\n",
        "\n",
        "**Exercise 18:** Now lets put these functions together to do epsilon-greedy action selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQx8K4vKUXj"
      },
      "outputs": [],
      "source": [
        "def select_epsilon_greedy_action(key, q_values, num_timesteps):\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "dummy_q_values = jnp.array([0,1], jnp.float32)\n",
        "num_timesteps = 1010 # very greedy\n",
        "print(\"Greedy actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")\n",
        "print()\n",
        "\n",
        "num_timesteps = 0 # completly random\n",
        "print(\"Random actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXDRQhORRjM6"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 18 solution\n",
        "\n",
        "# Now make a function that takes an epsilon-greedy action\n",
        "\n",
        "def select_epsilon_greedy_action(key, q_values, num_timesteps):\n",
        "\n",
        "    epsilon = get_epsilon(num_timesteps)\n",
        "\n",
        "    should_explore = jax.random.uniform(key, (1,))[0] < epsilon\n",
        "\n",
        "    num_actions = len(q_values)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        should_explore,\n",
        "        select_random_action(key, num_actions), \n",
        "        select_greedy_action(q_values)\n",
        "    )\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "dummy_q_values = jnp.array([0,1], jnp.float32)\n",
        "num_timesteps = 1010 # very greedy\n",
        "print(\"Greedy actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")\n",
        "print()\n",
        "\n",
        "num_timesteps = 0 # completly random\n",
        "print(\"Random actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H953ClAVsRW"
      },
      "source": [
        "### Q-Network\n",
        "Next we use Haiku to make a simple neural network to aproximate the Q-function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IivPEmC-RjM5"
      },
      "outputs": [],
      "source": [
        "def build_network(num_actions: int, layers=[10, 10]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for approximating Q-values.\"\"\"\n",
        "\n",
        "  def q_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [hk.Flatten(),\n",
        "         hk.nets.MLP(layers + [num_actions])])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(q_network))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLLI4LbvRjM5"
      },
      "outputs": [],
      "source": [
        "# Initialise Q-network\n",
        "Q_NETWORK = build_network(num_actions=2)\n",
        "dummy_obs = jnp.zeros((1,4), jnp.float32)\n",
        "Q_NETWORK_WEIGHTS = Q_NETWORK.init(next(rng), dummy_obs)\n",
        "print(\"Q-Learning params:\", Q_NETWORK_WEIGHTS.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5W23MnobN9x"
      },
      "source": [
        "### DQN action selection\n",
        "\n",
        "We now have everything we need to make the `DQN_select_action function`. We will use the `actor_state` to store a counter which keeps track of the current number of timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81TysLc0RjM6"
      },
      "outputs": [],
      "source": [
        "# Actor state stores the current number of timesteps\n",
        "ActorState = collections.namedtuple(\"ActorState\", [\"count\"])\n",
        "\n",
        "def dqn_select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim\n",
        "    q_values = Q_NETWORK.apply(params.online, obs)[0] # remove batch dim\n",
        "\n",
        "    action = select_epsilon_greedy_action(key, q_values, actor_state.count)\n",
        "    greedy_action = select_greedy_action(q_values)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        evaluation,\n",
        "        greedy_action,\n",
        "        action\n",
        "    )\n",
        "\n",
        "    next_actor_state = ActorState(actor_state.count + 1) # increment timestep counter\n",
        "\n",
        "    return action, next_actor_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tveVMB-4W8aA"
      },
      "source": [
        "### Transition Replay Buffer\n",
        "For the DQN algorithm we will need an agent memory that stores entire transitions: `obs`, `action`, `reward`, `next_obs`, `done`. When we sample the memory, samples should be chosen from the memory randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aojS6wpdRjM5"
      },
      "outputs": [],
      "source": [
        "class TransitionMemory(object):\n",
        "  \"\"\"A simple Python replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, max_size=5000, batch_size=256):\n",
        "    self.batch_size = batch_size\n",
        "    self.buffer = collections.deque(maxlen=max_size)\n",
        "\n",
        "  def push(self, transition):\n",
        "\n",
        "      self.buffer.append(\n",
        "          (transition.obs, transition.action, transition.reward, \n",
        "           transition.next_obs, transition.done)\n",
        "      )\n",
        "\n",
        "  \n",
        "  def is_ready(self):\n",
        "    return self.batch_size <= len(self.buffer)\n",
        "\n",
        "  def sample(self):\n",
        "    random_replay_sample = random.sample(self.buffer, self.batch_size)\n",
        "    obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*random_replay_sample)\n",
        "\n",
        "    return Transition(\n",
        "        np.stack(obs_batch).astype(\"float32\"), \n",
        "        np.asarray(action_batch).astype(\"int32\"), \n",
        "        np.asarray(reward_batch).astype(\"float32\"), \n",
        "        np.stack(next_obs_batch).astype(\"float32\"), \n",
        "        np.asarray(done_batch).astype(\"float32\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZn1tLOjdTsV"
      },
      "source": [
        "### Bellman Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBovYZmKRjM7"
      },
      "outputs": [],
      "source": [
        "# Squared error\n",
        "def compute_squared_error(pred, target):\n",
        "    squared_error = jnp.square(pred - target)\n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEkOu5PfRjM7"
      },
      "outputs": [],
      "source": [
        "# Bellman target\n",
        "def compute_bellman_target(reward, done, target_value):\n",
        "    return reward + (1.0 - done) * target_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_4QwvmvM03p"
      },
      "source": [
        "### Q-learning loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHjF59qhRjM7"
      },
      "outputs": [],
      "source": [
        "def q_learning_loss(q_values, action, reward, done, target_q_values):\n",
        "    q_value = q_values[action]\n",
        "    target_q_value = jnp.max(target_q_values)\n",
        "    bellman_target = compute_bellman_target(reward, done, target_q_value)\n",
        "    squared_error = compute_squared_error(q_value, bellman_target)\n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRRZe8D3NJI2"
      },
      "outputs": [],
      "source": [
        "def batched_q_learning_loss(online_params, obs, actions, rewards, next_obs, dones, target_params):\n",
        "    q_values = Q_NETWORK.apply(online_params, obs)\n",
        "    target_q_values = Q_NETWORK.apply(target_params, next_obs)\n",
        "    squared_error = jax.vmap(q_learning_loss)(q_values, actions, rewards, dones, target_q_values)\n",
        "    mean_squared_error = jnp.mean(squared_error)\n",
        "    return mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deKznSahRjM5"
      },
      "outputs": [],
      "source": [
        "LearnerState = collections.namedtuple(\"LearnerState\", [\"count\", \"optim_state\"])\n",
        "\n",
        "# Initialise Q-network optimizer\n",
        "DQN_OPTIMIZER = optax.adam(1e-3) # learning rate = 0.001\n",
        "DQN_optim_state = optim.init(Q_NETWORK_WEIGHTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z884-1oNRGEr"
      },
      "source": [
        "### Target network update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R24ULfb4Ozyz"
      },
      "outputs": [],
      "source": [
        "DQNParams = collections.namedtuple(\"DQNParams\", [\"online\", \"target\"])\n",
        "\n",
        "TARGET_UPDATE_RATE = 50\n",
        "\n",
        "def update_target(params, trainer_steps):\n",
        "\n",
        "    target_params = jax.lax.cond(\n",
        "        (trainer_steps % TARGET_UPDATE_RATE) == 0,\n",
        "        lambda x: x.online,\n",
        "        lambda x: x.target,\n",
        "        params\n",
        "    )\n",
        "\n",
        "    return DQNParams(params.online, target_params)\n",
        "\n",
        "# TEST\n",
        "update_target_jit = jax.jit(update_target)\n",
        "DQN_params = DQNParams(Q_NETWORK_WEIGHTS, Q_NETWORK_WEIGHTS)\n",
        "print(\"Updated target params:\", update_target_jit(DQN_params, 0).target.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyoog6cURjM7"
      },
      "source": [
        "### DQN learn function\n",
        "\n",
        "Next lets implement the dqn learner step function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjttz4YBRjM8"
      },
      "outputs": [],
      "source": [
        "def dqn_learner_step(rng, params, learner_state, memory):\n",
        "    grad_loss = jax.grad(batched_q_learning_loss)(params.online, memory.obs, \n",
        "                                            memory.action, memory.reward, \n",
        "                                            memory.next_obs, memory.done,\n",
        "                                            params.target)\n",
        "\n",
        "    updates, opt_state = DQN_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "    online_params = optax.apply_updates(params.online, updates)\n",
        "\n",
        "    # Maybe update target network params\n",
        "    params = DQNParams(online_params, params.target)\n",
        "    params = update_target(params, learner_state.count)\n",
        "\n",
        "    # Increment learner step counter\n",
        "    learner_state = LearnerState(learner_state.count + 1, opt_state)\n",
        "\n",
        "    return params, learner_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbdHDbd1RjM8"
      },
      "outputs": [],
      "source": [
        "# DQN params\n",
        "dqn_params = DQNParams(Q_NETWORK_WEIGHTS, Q_NETWORK_WEIGHTS)\n",
        "\n",
        "# Initialise actor\n",
        "actor_state = ActorState(0)\n",
        "dqn_select_action_jit = jax.jit(dqn_select_action)\n",
        "\n",
        "# Initialise learner\n",
        "learner_state = LearnerState(0, DQN_optim_state)\n",
        "dqn_learner_step_jit = jax.jit(dqn_learner_step)\n",
        "\n",
        "# Initialise memory\n",
        "memory = TransitionMemory(5000, 256) # store 5000 transitions\n",
        "\n",
        "# Run environment loop\n",
        "episode_returns, evaluator_returns = run_environment_loop(rng, env, dqn_params, \n",
        "                      dqn_select_action_jit, actor_state, \n",
        "                      dqn_learner_step_jit, learner_state, memory,\n",
        "                      num_episodes=5_000,\n",
        "                      learner_steps_per_episode=4)\n",
        "\n",
        "plt.plot(evaluator_returns)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR_xjOaYTGhv"
      },
      "source": [
        "## Additional Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:** \n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:** \n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bd84AqdSJH1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbcFsfAibBu"
      },
      "source": [
        "Math foundations:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "#@title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "intro_to_rl.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
