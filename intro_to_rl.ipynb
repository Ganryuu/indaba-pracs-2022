{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Intro to Reinforcement Learning\n",
        "\n",
        "\n",
        "[Need to update this picture]\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/feat/intro-rl-section-1-updates/intro_to_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "Claude Formanek, Kale-ab Tessera\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "In this tutorial, we will be learning about Reinforcement Learning, a type of Machine Learning where an agent learns to chose actions in an environment that lead to maximal reward in the long run. RL has seen tremendous success on a wide range of challenging problems such as learning to play complex video games like Atari, StarCraft II and Dota II. In this introductory tutorial we will solve the classic CartPole environment, where an agent must learn to balance a pole on a cart, using several different RL approaches. Along the way you will be introduces to some of the most important concepts and terminology in RL.\n",
        "\n",
        "**Topics:** \n",
        "* Reinforcement Learning\n",
        "* Policy Gradients\n",
        "* Q-Learning\n",
        "\n",
        "**Level:** \n",
        "\n",
        "Beginner\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "* Understand the basic theory behind RL\n",
        "* Implement a simple policy gradient RL algorithm\n",
        "* Implement a simple Q-learning algorithm\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "* Some familiarity with [Jax](https://github.com/google/jax).\n",
        "* Neural network basics.\n",
        "\n",
        "**Outline:** \n",
        "\n",
        "* Section 1: Key Concepts in Reinforcement Learning\n",
        "* Section 2: Random Policy Search\n",
        "* Section 3: Policy Gradient\n",
        "* Section 4: Deep Q-Learning\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install required packages (Run Cell)\n",
        "%%capture\n",
        "!pip install jaxlib\n",
        "!pip install jax\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install gym\n",
        "!pip install gym[box2d]\n",
        "!pip install rlax\n",
        "!pip install optax\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwbqggmcRjMy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Import required packages (Run Cell)\n",
        "%%capture\n",
        "import random\n",
        "import collections # useful data structures\n",
        "import numpy as np\n",
        "import gym # reinforcement learning environments\n",
        "import jax\n",
        "import jax.numpy as jnp # jax numpy\n",
        "import haiku as hk # jax neural network library\n",
        "import optax # jax optimizer library\n",
        "import rlax # jax reinforcement learning library\n",
        "import matplotlib.pyplot as plt # graph plotting library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## Section 1: Key Concepts in Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning (RL) is a subfield of Machine Learning (ML). Unlike fields like supervised learning, where we give examples of expected behaviour to our models, RL focuses on *goal-orientated* learning from interactions, through trial-and-error. RL algorithms learn what to do (i.e. which optimal actions to take) in an environment to maximise some reward signal. In settings like a video game, the reward signal could be the score of the game, i.e., RL algorithms will try to maximise the score in the game by chosing the best actions.  \n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*Ews7HaMiSn2l8r70eeIszQ.png\" width=\"60%\" />\n",
        "</center>\n",
        "\n",
        "\n",
        "More precisely, in RL we have an **agent** which perceives an **observation** $o_t$ of the current state $s_t$ of the **environment** and must choose an **action** $a_t$ to take. The environment then transitions to a new state $s_{t+1}$ in response to the agent's action and also gives the agent a scalar reward $r_t$ to indicate how good or bad the chosen action was given the environment's state. The goal in RL is for the agent to maximise the amount of reward it receives from the environment over time. The subscript $t$ is used to indicate the timestep number, i.e., $s_0$ is the state of the environment at the initial timestep, and $a_{99}$ is the agent's action at the $99th$ timestep. \n",
        "\n",
        "[Image Source](https://towardsdatascience.com/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghgy69hFRjMz"
      },
      "source": [
        "### Environment - OpenAI Gym\n",
        "As mentioned above, an environment receives an action $a_t$ and returns reward $r_t$ and observation $o_t$.\n",
        "\n",
        "OpenAI has provided a Python package called Gym that includes implementations of popular environments and a simple interface for an RL agent to interact with. To use a supported gym [environment](https://www.gymlibrary.ml/), all you need to do is pass the name of the environment to the function `gym.make(<environment_name>)`. \n",
        "\n",
        "In this tutorial, we will be using a simple environment called CartPole. In **CartPole** the task is for the agent to learn to balance the pole for as long as possible by moving the cart *left* or *right*.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/1*v8KcdjfVGf39yvTpXDTCGQ.gif\" width=\"30%\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfxzajMYRjMz"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_BbftaJj3zu"
      },
      "source": [
        "### States and Observations - $s_t$ and $o_t$\n",
        "\n",
        "In RL, an agent perceives an observation of the environment's state. In some settings, the observation may include all the information underlying the environment's state. Such an environment is called **fully observed**. In other settings, the agent may only receive partial information about the environment's state in its observation. Such an environment is called **partially observed**. \n",
        "\n",
        "For the rest of this tutorial, we will assume the environment is fully observed and so we will use state $s_t$ and observation $o_t$ interchangeably. In Gym we get the initial observation from the environment by calling the function `env.reset()`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdS8nqOgRjM0"
      },
      "outputs": [],
      "source": [
        "# Reset the environment\n",
        "s_0 = env.reset()\n",
        "print(\"Initial State::\", s_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUNX6mbotABo"
      },
      "source": [
        "In CartPole, the state of the environment is represented by four numbers; *angular position of the pole, angular velocity of the pole, position of the cart, velocity of the cart*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL1Nkgy7nUfn"
      },
      "source": [
        "### Actions - $a_t$\n",
        "\n",
        "In RL actions are usually either **discrete** or **continuous**. Continuous actions are given by a vector of real numbers. Discrete actions are given by an integer value. In environments where we can count out the finite set of actions we usually use discrete actions. \n",
        "\n",
        "In CartPole there are only two actions; *left and right*. As such, the actions can be represented by integers $0$ and $1$. In gym we can easily get the list of possible actions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOLZqU_LpIXh"
      },
      "outputs": [],
      "source": [
        "# Get action space - e.g. discrete or continuous\n",
        "print(f\"Environment action space: {env.action_space}\")\n",
        "\n",
        "# Get num actions\n",
        "num_actions = env.action_space.n\n",
        "print(f\"Number of actions: {num_actions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRsflxbDpoPm"
      },
      "source": [
        "### The Agent's Policy - $\\pi$\n",
        "\n",
        "In RL the agent chooses actions based on the observations it receives. We can think of the agent's action selection process as a function that takes an observation as input and returns an action as output. In RL we usually call this function an agent's **policy** and denote it $\\pi(s_t)$. \n",
        "\n",
        "Our policies can be **deterministic** ($s_t$ is mapped to a single action $a_t$), as follows: \n",
        "\n",
        "<center>\n",
        "$\\pi(s_t)=a_t$.\n",
        "</center>\n",
        "\n",
        "In other cases, our policy could rather be **stochastic**, where it returns a distribution and actions are sampled from this distribution. We denote stochastic policies as follows:\n",
        "<center>\n",
        "$a_t\\sim\\pi(\\cdot\\ |\\ s_t)$\n",
        "</center>\n",
        "\n",
        ", where the symbol $\\cdot$ is simply a shorthand for *all actions* and \"~\" means \"*sampled from*\".\n",
        "\n",
        "Policies are parameterized by weights $\\theta$. We sometimes write this as a subscript on the policy symbol to highlight the connection as follows - $\\pi_{\\theta}$, but for convenience we will simply refer to policies as $\\pi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRXmAc8PzaGZ"
      },
      "source": [
        "**Exercise 1:** If Bob has a deterministic policy $\\pi$ and the first time Bob uses his policy on some observation $o_t$ he chooses action $0$. What will Bob's chosen action be if he uses the same policy a second time on the exact same observation $o_t$ (same timestep $t$). Chose from the options below and assume there are only two possible actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mippEuENypur"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 1\n",
        "selection = \"Bob will chose action 1.\" #@param [\"Bob will chose action 0 again.\", \"Bob will chose action 1.\", \"You can't say.\"]\n",
        "print(f\"You selected: {selection}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjSz_qfr1dPh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Check Exercise 1\n",
        "correct_answer = \"Bob will chose action 0 again.\"\n",
        "assert selection == correct_answer, \"Incorrect answer, hint ...\"\n",
        "\n",
        "\"\"\" \n",
        "Exercise 1: Since the policy is deterministic, Bob will always choose the same \n",
        "action given the same observation.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xjdYEcJ0trD"
      },
      "source": [
        "**Exercise 2:**  If Alice has a stochastic policy $\\pi$ and the first time Alice uses her policy on some observation $o_t$ she chooses action $0$. What will Alice's chosen action be if she uses her policy a second time on the exact same observation $o_t$. Chose from the options below and assume there are only two possible actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_qRxbzX31FND"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 2\n",
        "selection = \"Alice will chose action 0 again.\" #@param [\"Alice will chose action 0 again.\", \"Alice will chose action 1.\", \"You can't say.\"]\n",
        "print(f\"You selected: {selection}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HKA-ngpb1rFT"
      },
      "outputs": [],
      "source": [
        "#@title Check Exercise 2\n",
        "correct_answer = \"You can't say.\"\n",
        "assert selection == correct_answer, \"Incorrect answer, hint ...\"\n",
        "\n",
        "\"\"\" \n",
        "Exercise 2: Since the policy is stochastic, the result from Alice's policy will be\n",
        "random. So you can't say.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZdmz2Wqv_NN"
      },
      "source": [
        "As an exercise we will implement a stochastic policy as well as a deterministic policy for CartPole. \n",
        "\n",
        "*Note, in this section we are not learning $\\pi$'s weights $\\theta$, we are just using a passed in value for the weights.*\n",
        "\n",
        "**Exercise 3:** Complete the following functions:\n",
        "- `linear_policy` : computes the linear combination of $\\pi$'s weights $\\theta$ and observation $o_t$. This is a linear function approximator. \n",
        "- `choose_action` : [Discretize](https://en.wikipedia.org/wiki/Discretization) the result from the linear policy as follows:\n",
        "    - if the `result is less than or equal to zero` - return a `0`\n",
        "    - if the `result is greater than zero` - return a `1`\n",
        "\n",
        "Is this a deterministic or stochastic policy?\n",
        "\n",
        "**Useful methods:** \n",
        "* [JAX Numpy dot product](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html).\n",
        "* [JAX Numpy where](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) or [Jax select](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html#jax.lax.select).\n",
        "\n",
        "\n",
        "**Notes:**\n",
        "*   We already imported `jax numpy` as `jnp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSGCd7XB1z8k"
      },
      "outputs": [],
      "source": [
        "# params refer to weights of our policy e.g. [1,-2,2,-1]\n",
        "def linear_policy(params,obs):\n",
        "  # YOUR CODE\n",
        "  result = ...\n",
        "  # END YOUR CODE\n",
        "  return result \n",
        "\n",
        "# params refer to weights of our policy e.g. [1,-2,2,-1]\n",
        "def choose_action(params, obs):\n",
        "  result = linear_policy(params,obs)\n",
        "  # YOUR CODE\n",
        "  action = ...\n",
        "\n",
        "  # END YOUR CODE\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9EnnBvceb1f3"
      },
      "outputs": [],
      "source": [
        "# @title Check correctness of implementation (Run me) \n",
        "\n",
        "def check_deterministic_linear_policy(linear_policy,choose_action):\n",
        "  fixed_obs = jnp.array([1,1,2,4])\n",
        "  \n",
        "  # check case1 - negative dot product.\n",
        "  # weights\n",
        "  params = jnp.array([1,-2,2,-1])\n",
        "\n",
        "  assert linear_policy(params,fixed_obs) == -1, \"Incorrect answer, your linear policy is incorrect.\"\n",
        "  assert choose_action(params,fixed_obs) == 0,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # Check deterministic policy\n",
        "  for i in range(100):\n",
        "    assert choose_action(params,fixed_obs) == 0,  \"Incorrect answer, your choose action function is not deterministic.\"\n",
        "\n",
        "\n",
        "  # check case2 - positive dot product\n",
        "  params = jnp.array([1,2,2,1])\n",
        "\n",
        "  assert linear_policy(params,fixed_obs) == 11, \"Incorrect answer, your linear policy is incorrect.\"\n",
        "  assert choose_action(params,fixed_obs) == 1,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # Check deterministic policy\n",
        "  for i in range(100):\n",
        "    assert choose_action(params,fixed_obs) == 1,  \"Incorrect answer, your choose action function is not deterministic.\"\n",
        "\n",
        "  # check case3 - 0 dot product\n",
        "  params = jnp.array([0,0,0,0])\n",
        "\n",
        "  assert linear_policy(params,fixed_obs) == 0, \"Incorrect answer, your linear policy is incorrect.\"\n",
        "  assert choose_action(params,fixed_obs) == 0,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # Check deterministic policy\n",
        "  for i in range(100):\n",
        "    assert choose_action(params,fixed_obs) == 0,  \"Incorrect answer, your choose action function is not deterministic.\"\n",
        "\n",
        "  print(\"Your function is correct!\")\n",
        "check_deterministic_linear_policy(linear_policy,choose_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OCzRmXbz2HfY"
      },
      "outputs": [],
      "source": [
        "# @title  Exercise 3 solution - Answer to code task (Try not to peek until you've given it a good try!')\n",
        "# params refer to weights e.g. [1,-2,2,-1]\n",
        "def linear_policy(params,obs):\n",
        "  result = jnp.dot(params,obs)\n",
        "  return result \n",
        "\n",
        "def choose_action(params, obs):\n",
        "  result = linear_policy(params,obs)\n",
        "  action = jnp.where(result <= 0,0,1)\n",
        "  return action\n",
        "\n",
        "check_deterministic_linear_policy(linear_policy,choose_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI_-xUHL0qz-"
      },
      "source": [
        "**Exercise 4:** Following on exercise 3, let's implement a different way of choosing actions. For this exercise, implement:\n",
        "- `choose_action` : [Discretize](https://en.wikipedia.org/wiki/Discretization) the result from the linear policy as follows:\n",
        "    - if the `result is less than or equal to zero` :  \n",
        "      - return a `0` 20% of the time.\n",
        "      - return a `1` 80% of the time.\n",
        "    - if the `result is greater than zero` : \n",
        "      - return a `0` 100% of the time. \n",
        "\n",
        "Is this a deterministic or stochastic policy?\n",
        "\n",
        "**Useful methods:** \n",
        "*   [JAX random choice](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.choice.html) or [JAX Numpy Uniform](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html).\n",
        "* [JAX Numpy Where](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) or [JAX Conditional](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0GTHXXJeUv-"
      },
      "outputs": [],
      "source": [
        "# function from exercise 3\n",
        "def linear_policy(params,obs):\n",
        "  result = jnp.dot(params,obs)\n",
        "  return result \n",
        "\n",
        "# key refers to our jax random key e.g. jax.random.PRNGKey(42)\n",
        "def choose_action(key, params, obs):\n",
        "  result = linear_policy(params,obs)\n",
        "  # YOUR CODE\n",
        "  action = ...\n",
        "\n",
        "  # END YOUR CODE\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur5Kxsl8i9nL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Check correctness of implementation (Run me) \n",
        "\n",
        "def check_stochastic_linear_policy(choose_action):\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  fixed_obs = jnp.array([1,1,2,4])\n",
        "  \n",
        "  # check case1\n",
        "  # weights\n",
        "  params = jnp.array([1,-2,2,-1])\n",
        "\n",
        "  assert linear_policy(params,fixed_obs) == -1, \"Incorrect answer, your linear policy is incorrect.\"\n",
        "\n",
        "  # Check stochastic policy\n",
        "  actions = []\n",
        "  for i in range(100):\n",
        "    actions.append(choose_action(key,params,fixed_obs))\n",
        "    new_key, _ = jax.random.split(key)\n",
        "    key = new_key\n",
        "  \n",
        "  sum_actions = jnp.sum(jnp.array(actions))\n",
        "  # This is around ~ 80%, we know the exact value since Jax handles PRNG consistently!\n",
        "  assert sum_actions == 84,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # check case2\n",
        "  params = jnp.array([1,2,2,1])\n",
        "\n",
        "  assert linear_policy(params,fixed_obs) == 11, \"Incorrect answer, your linear policy is incorrect.\"\n",
        "\n",
        "  # Check stochastic policy\n",
        "  actions = []\n",
        "  for i in range(100):\n",
        "    actions.append(choose_action(key,params,fixed_obs))\n",
        "    new_key, _ = jax.random.split(key)\n",
        "    key = new_key\n",
        "\n",
        "  sum_actions = jnp.sum(jnp.array(actions))\n",
        "  assert sum_actions == 0,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "check_stochastic_linear_policy(choose_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r6UhHZEMgcNp"
      },
      "outputs": [],
      "source": [
        "# @title  Exercise 4 solution - Answer to code task (Try not to peek until you've given it a good try!')\n",
        "# function from exercise 3\n",
        "def linear_policy(params,obs):\n",
        "  result = jnp.dot(params,obs)\n",
        "  return result \n",
        "\n",
        "# key refers to our jax random key e.g. jax.random.PRNGKey(42)\n",
        "def choose_action(key, params, obs):\n",
        "  result = linear_policy(params,obs)\n",
        "  action = jnp.where(result <= 0,jax.random.choice(key,a=jnp.array([0,1]),p=jnp.array([0.2,0.8])),0)\n",
        "  return action\n",
        "\n",
        "# Inefficient example using all if statements and jax.random.uniform\n",
        "# def choose_action(key, params, obs):\n",
        "#   result = linear_policy(params,obs)\n",
        "#   r = jax.random.uniform(key)\n",
        "#   if result <= 0:\n",
        "#     if r <= 0.8:\n",
        "#       action = 1\n",
        "#     else:  \n",
        "#       action = 0 \n",
        "#   else:\n",
        "#     action = 0\n",
        "#   return action\n",
        "\n",
        "check_stochastic_linear_policy(choose_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkuvT-jf6Ieh"
      },
      "source": [
        "### The Environment Transition Function - $P$\n",
        "\n",
        "Now that we have a policy we can pass actions from the agent to the environment. The environment will then transition into a new state in response to the agent's action. \n",
        "\n",
        "In RL we model this process by using a **state transition function** $P$ which takes the current state $s_t$ and an action $a_t$ as input and returns the next state $s_{t+1}$ as output. Like with policies, the state transition function can either be *deterministic*: \n",
        "<center>\n",
        " $s_{t+1}=P(s_t, a_t)$\n",
        "</center> \n",
        "\n",
        "or it can be *stochastic*: \n",
        "<center>\n",
        " $s_{t+1}\\sim P(\\cdot\\ |\\ s_t, a_t)$\n",
        "</center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzchuH4giVFB"
      },
      "source": [
        "\n",
        "In gym we can pass actions to the environment by calling the `env.step(<action>)` function. The function will then return four values:\n",
        "- the **next observation**\n",
        "- the **reward** for the action taken\n",
        "- a boolean flag to indicate if the game is **done** \n",
        "- some **extra** information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh0j9-Tk7olb"
      },
      "outputs": [],
      "source": [
        "# Get the initial obs by resetting the env\n",
        "initial_obs = env.reset()\n",
        "\n",
        "# Randomly sample actions from env\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# Step the environment\n",
        "next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(\"Observation:\", initial_obs)\n",
        "print(\"Action:\", action)\n",
        "print(\"Next observation:\", next_obs)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Game is done:\", done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX9iZtu48UYn"
      },
      "source": [
        "### Episode Return - $R_t$\n",
        "\n",
        "In RL we usually break an agent's interactions with the environment up into **episodes** (sequence of interactions with an environment, usually ending in a terminal state). \n",
        "\n",
        "The sum of all rewards collected during an episode is what we call the episode's **return** - $R_t$. The simplest formulation is the **finite-horizon, undiscounted return**, which can be formulated as follows:\n",
        "\n",
        "<center>\n",
        "$R_t=\\sum_{t=0}^Tr_t$\n",
        "</center>\n",
        "\n",
        ", where $r_t$ is our reward at time $t$ and $T$ is our terminal state. This return is calculated over a fixed window of time and assumes that terminal state $T$ is always reached and every reward $r_t$ is weighted equally.\n",
        "\n",
        "\n",
        "Generally, in practice, we tend to use the **infinite horizon, discounted rewards** (also referred to as the *expected discounted return*), which sums all rewards *ever obtained over time*, discounted by how far off in the future they are obtained. This is formulated as follows:\n",
        "\n",
        "<center>\n",
        "$R_t =\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t}$\n",
        "</center>\n",
        "\n",
        ", where $\\gamma \\in (0,1) $ is our discount rate. \n",
        "\n",
        "The goal in RL is for the agent to chose actions which maximise this expected future return $R_t$.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxh3FNEnQXpJ"
      },
      "source": [
        "**Group task**: Discuss with your neighbour why we would want to use a discount factor? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6KZA1Nq9p47"
      },
      "source": [
        "### Agent-environment Loop\n",
        "Now that we know what a policy is and we know how to step the environment, let's close the agent-environment loop.\n",
        "\n",
        "**Exercise 5:** Write a function that runs one episode of CartPole by sequentially choosing actions and stepping the environment. You should use the stochastic policy we defined earlier to chose actions (i.e. the `choose_action` function). The function should keep track of the reward received and output the return at the end of the episode. For simplicity, we will use the **finite-horizon, undiscounted return**.\n",
        "\n",
        "In CartPole the agent receives a reward of `1` for every timestep the pole is still upright. If the pole falls over, the game is over and the agent receives a reward of `0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buy0X7mi-gHP"
      },
      "outputs": [],
      "source": [
        "def run_episode(env):\n",
        "  episode_return = 0\n",
        "\n",
        "  ## YOUR CODE\n",
        "  # initial obs\n",
        "  obs = ...\n",
        "  done = False\n",
        "  # policy params\n",
        "  params = jnp.array([1,-2,2,-1])\n",
        "  key = jax.random.PRNGKey(42)\n",
        "\n",
        "  # while loop until episode is done\n",
        "  while not done:\n",
        "    # HINT: You might need to the convert the action from your policy to a np.array\n",
        "    action = ...\n",
        "    # HINT: Step in your environment\n",
        "    next_obs, reward, done, info = ...\n",
        "    # HINT: Update observations\n",
        "    obs = ...\n",
        "    episode_return = ...\n",
        "\n",
        "  return episode_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bA2Orj9PVbKO"
      },
      "outputs": [],
      "source": [
        "# @title Check correctness of implementation (Run me) \n",
        "\n",
        "env.seed(42)\n",
        "assert run_episode(env) == 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h_vu0GImyBT4"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 5 solution\n",
        "def run_episode(env):\n",
        "  episode_return = 0\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  params = jnp.array([1,-2,2,-1])\n",
        "  key = jax.random.PRNGKey(42)\n",
        "\n",
        "  while not done:\n",
        "    action = np.array(choose_action(key, params, obs))\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "    obs = next_obs\n",
        "    episode_return += reward\n",
        "\n",
        "  return episode_return\n",
        "\n",
        "env.seed(42)\n",
        "assert run_episode(env) == 8\n",
        "\n",
        "print(\"Episode return:\", run_episode(env))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGdzHxJnZGl"
      },
      "source": [
        "In CartPole, the environment is considered solved when the agent can reliably achieve an episode return of 500. As you can see, our current policy is nowhere near optimal yet. Let's learn a way to find an optimal policy.\n",
        "\n",
        "One way we can find an optimal policy is by randomly searching for it. Obviously in a complex environment finding an optimal policy by randomly trying different strategies could take forever. But CartPole is a sufficiently simple environment that it might just work.\n",
        "\n",
        "Before we implement Random Policy Search, let's take a look at the following environment loop function that we implemented for you. Its the environmentloop we will be using for the rest of the notebook. We will use a [NamedTuple](https://www.geeksforgeeks.org/namedtuple-in-python/) to bundle `obs`, `action`, `reward`, `next_obs` and the done flag into a **transition** object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpxNZxbORjM0"
      },
      "outputs": [],
      "source": [
        "# Named tuple to store transition\n",
        "Transition = collections.namedtuple(\"Transition\", [\"obs\", \"action\", \"reward\", \"next_obs\", \"done\"])\n",
        "\n",
        "# TEST\n",
        "transition = Transition(\n",
        "    obs=[1,2,-1,2],\n",
        "    action=0,\n",
        "    reward=10,\n",
        "    next_obs=[1,2,2,1],\n",
        "    done=True\n",
        ")\n",
        "\n",
        "print(\"Transition obs:\", transition.obs)\n",
        "print(\"Transition action:\", transition.action)\n",
        "print(\"Transition reward:\", transition.reward)\n",
        "print(\"Transition next obs:\", transition.next_obs)\n",
        "print(\"Transition done:\", transition.done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSCMHsULvlky"
      },
      "source": [
        "Below is the environment loop function we have implemented for you. We recommend reading through it and trying to understand it, but if anything is unclear, don't worry about it. It should all make more sense as we work through this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWBwz3zMRjM0"
      },
      "outputs": [],
      "source": [
        "# Environment loop\n",
        "def run_environment_loop(rng, env, agent_params, agent_select_action_func, \n",
        "    agent_actor_state=None, agent_learn_func=None, agent_learner_state=None, \n",
        "    agent_memory=None, num_episodes=1000, evaluator_period=100, \n",
        "    evaluation_episodes=32, learn_steps_per_episode=1):\n",
        "    \"\"\"\n",
        "    This function runs several episodes in an environment and periodically does \n",
        "    some agent learning and evaluation.\n",
        "    \n",
        "    Args:\n",
        "        rng: a random number generator. This is for jax.\n",
        "        env: a gym environment.\n",
        "        agent_params: an object to store parameters that the agent uses.\n",
        "        agent_select_func: a function that does action selection for the agent.\n",
        "        agent_actor_state (optional): an object that stores the internal state \n",
        "            of the agents action selection function.\n",
        "        agent_learn_func (optional): a function that does some learning for the \n",
        "            agent by updating the agent parameters.\n",
        "        agent_learn_state (optional): an object that stores the internal state \n",
        "            of the agent learn function.\n",
        "        agent_memory (optional): an object for storing an retrieving historical \n",
        "            experience.\n",
        "        num_episodes: how many episodes to run.\n",
        "        evaluator_period: how often to run evaluation.\n",
        "        evaluation_episodes: how many evaluation episodes to run.\n",
        "\n",
        "    Returns:\n",
        "        episode_returns: list of all the episode returns.\n",
        "        evaluator_episode_returns: list of all the evaluator episode returns.\n",
        "    \"\"\"\n",
        "    episode_returns = [] # List to store history of episode returns.\n",
        "    evaluator_episode_returns = [] # List to store history of evaluator returns.\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # Reset environment.\n",
        "        obs = env.reset()\n",
        "        episode_return = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # Agent select action.\n",
        "            action, agent_actor_state = agent_select_action_func(\n",
        "                                            next(rng), \n",
        "                                            agent_params, \n",
        "                                            agent_actor_state, \n",
        "                                            np.array(obs)\n",
        "                                        )\n",
        "\n",
        "            # Step environment.\n",
        "            next_obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "            # Pack into transition.\n",
        "            transition = Transition(obs, action, reward, next_obs, done)\n",
        "\n",
        "            # Add transition to memory.\n",
        "            if agent_memory: # check if agent has memory\n",
        "              agent_memory.push(transition)\n",
        "\n",
        "            # Add reward to episode return.\n",
        "            episode_return += reward\n",
        "\n",
        "            # Set obs to next obs before next environment step. CRITICAL!!!\n",
        "            obs = next_obs\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "        # At the end of every episode we do a learn step.\n",
        "        if agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "\n",
        "            for _ in range(learn_steps_per_episode):\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng), \n",
        "                                                        agent_params, \n",
        "                                                        agent_learner_state, \n",
        "                                                        memory\n",
        "                                                    )\n",
        "\n",
        "        if (episode % evaluator_period) == 0: # Do evaluation\n",
        "\n",
        "            evaluator_episode_return = 0\n",
        "            for eval_episode in range(evaluation_episodes):\n",
        "                obs = env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action, _ = agent_select_action_func(\n",
        "                                    next(rng), \n",
        "                                    agent_params, \n",
        "                                    agent_actor_state, \n",
        "                                    np.array(obs), \n",
        "                                    evaluation=True\n",
        "                                )\n",
        "\n",
        "                    obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "                    evaluator_episode_return += reward\n",
        "\n",
        "            evaluator_episode_return /= evaluation_episodes\n",
        "\n",
        "            evaluator_episode_returns.append(evaluator_episode_return)\n",
        "\n",
        "            logs = [\n",
        "                    f\"Episode: {episode}\",\n",
        "                    f\"Episode Return: {episode_return}\",\n",
        "                    f\"Average Episode Return: {np.mean(episode_returns[-20:])}\",\n",
        "                    f\"Evaluator Episode Return: {evaluator_episode_return}\"\n",
        "            ]\n",
        "\n",
        "            print(*logs, sep=\"\\t\") # Print the logs\n",
        "\n",
        "    return episode_returns, evaluator_episode_returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-JDXqJsaur-"
      },
      "source": [
        "Before we can test this environment loop function with the policy function we implemented earlier, we will need to modify it so that its interface matches the way our environment loop expects it. The `choose_action` function should take a random seed in the first argument position, then parameters, then an actor's internal state (more on this later), then the observation and finally an evaluation boolean flag that indicates if the function is being called during the evaluation loop or not (more on this later). The function should return the chosen action and the next state of the actor.\n",
        "\n",
        "Let's adapt our solution from **exercise 3**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTHFPGWcbAtZ"
      },
      "outputs": [],
      "source": [
        "def choose_action(key, params, actor_state, obs, evaluation=False):\n",
        "  del key, evaluation # not used yet\n",
        "\n",
        "  result = linear_policy(params,obs)\n",
        "  action = jnp.where(result <= 0,0,1)\n",
        "  return action, actor_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWoGrjH2YHMA"
      },
      "source": [
        "Let's [JIT](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) our `choose_actions` function to speed it up. If you are unfamiliar with JIT or other JAX basics, you can review the *Introduction to ML using Jax* practical. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaNN8Lw0Yoic"
      },
      "outputs": [],
      "source": [
        "choose_action_jit = jax.jit(choose_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9hOnME-TeIu"
      },
      "source": [
        "**A quick aside about random number:** when using Jax you will sometimes need to pass a random key to your functions. We will instantiate a Random Number Generator and use that to continuously generate new random keys. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NATerVnjT4_p"
      },
      "outputs": [],
      "source": [
        "# Jax random number generator\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpAqVqVY_DcR"
      },
      "source": [
        "We can now test our `select_action` function in the `run_environment_loop` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ2uN-kh_Qpm"
      },
      "outputs": [],
      "source": [
        "# Some arbitrary parameters\n",
        "params = np.array([1,1,-1,-1], \"float32\")\n",
        "\n",
        "episode_returns, evaluator_returns = run_environment_loop(\n",
        "                                        rng, # random number generator\n",
        "                                        env, # environment\n",
        "                                        params, # parameters used by the action selector\n",
        "                                        choose_action_jit,\n",
        "                                        num_episodes=1001,\n",
        "                                      )\n",
        "\n",
        "print(\"Average episode returns:\", np.mean(episode_returns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zu51ep7Sh0M"
      },
      "source": [
        "### Agent memory\n",
        "\n",
        "In many RL algorithms, the agent uses a kind of memory to store some of the experiences it had in the environment. The interface we will use for the agent's memory is very simple. It will have a function `memory.push(<transition>)` that adds some information about the transition to the memory, a function `memory.is_ready()` to check if the memory is ready to do some learning, and finally a function `memory.sample()` that returns some information that the agent learn function can use to do learning.\n",
        "\n",
        "#### Average Episode Return Memory\n",
        "We have built a simple agent memory module for you below. It stores the `epsisode_returns` of the last 20 episodes. Read through our implementation below and see if you can understand it. The `memory.sample()` method returns the average episode return over the last 20 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVkTBIK5RjM1"
      },
      "outputs": [],
      "source": [
        "# A NamedTuple to store the average episode return of the last 20 runs\n",
        "AverageEpisodeReturnMemory = collections.namedtuple(\n",
        "                                \"AverageEpisodeReturnMemory\", \n",
        "                                [\"average_episode_return\"]\n",
        "                            )\n",
        "\n",
        "class AverageEpisodeReturnBuffer:\n",
        "\n",
        "    def __init__(self, num_episodes_to_store=20):\n",
        "        \"\"\"\n",
        "        This class implements an agent memory that stores the average episode \n",
        "        return over the last 20 episodes.\n",
        "        \"\"\"\n",
        "        self.num_episodes_to_store = num_episodes_to_store\n",
        "        self.episode_return_buffer = []\n",
        "        self.current_episode_return = 0\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_return += transition.reward\n",
        "\n",
        "        if transition.done: # If the episode is done\n",
        "            # Add episode return to buffer\n",
        "            self.episode_return_buffer.append(self.current_episode_return)\n",
        "\n",
        "            # Reset episode return\n",
        "            self.current_episode_return = 0\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.episode_return_buffer) == self.num_episodes_to_store\n",
        "\n",
        "    def sample(self):\n",
        "        average_episode_return = np.mean(self.episode_return_buffer)\n",
        "\n",
        "        # Clear episode return buffer\n",
        "        self.episode_return_buffer = []\n",
        "\n",
        "        return AverageEpisodeReturnMemory(average_episode_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPlIq4oDBPY"
      },
      "source": [
        "## Section 2: Random Policy Search (RPS)\n",
        "In Section 1, we used predefined parameters for our policy, that is to say we didn't learn $\\pi$'s weights $\\theta$, we just simply set them ( `params = jnp.array([1,-2,2,-1])`, `linear_policy(params,...`). There are various algorithms to find and improve our policy's weights.  \n",
        "\n",
        "One such algorithm is Random Policy Search (RPS), which is a method that will randomly try different policies and keep track of the best policy it has found so far. We will say that policy $A$ is better than policy $B$ if the average episode return policy $A$ achieved over the last 20 episodes is greater than that of policy $B$. We will need to modify the way we store the agent's parameters so that we always have access to the latest parameters as well as the best parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DcaC-PQRjM1"
      },
      "outputs": [],
      "source": [
        "# Parameter container for Random Policy Search\n",
        "RandomPolicySearchParams = collections.namedtuple(\"RandomPolicySearchParams\", [\"current\", \"best\"])\n",
        "\n",
        "# TEST: store two different sets of parameters\n",
        "current_params = np.array([1,1,-1,-1])\n",
        "best_params = np.array([0,0,0,0])\n",
        "rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "\n",
        "# How to access the best or current params.\n",
        "print(f\"Best params: {rps_params.best}\")\n",
        "print(f\"Current params: {rps_params.current}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement the following:\n",
        "  - RPS Select Action - How we choose actions given a policy.\n",
        "  - RPS Learn - How we update and improve our policy."
      ],
      "metadata": {
        "id": "v91pDDzGex9a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tExceGeGNYH"
      },
      "source": [
        "### RPS Select Action Function\n",
        "Now let's once again modify our `choose_action` function such that it uses the best parameters when `evaluation==True` and uses the current parameters when `evaluation==False`.\n",
        "\n",
        "> We can still use the `linear_policy` function to calculate the forward pass of our policy and then discretize the result from the linear policy as follows:\n",
        "  - if the `result is less than or equal to zero` - return a `0`\n",
        "  - if the `result is greater than zero` - return a `1`\n",
        "\n",
        "**Exercise 7:** Implement the `random_policy_search_choose_action` function as described above. Make sure you use Jax so that we can jit the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kmwT35JRjM1"
      },
      "outputs": [],
      "source": [
        "def linear_policy(params,obs):\n",
        "  result = jnp.dot(params,obs)\n",
        "  return result \n",
        "\n",
        "def random_policy_search_choose_action(\n",
        "    key, \n",
        "    params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        "):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  # HINT: best_action = ... (two lines)\n",
        "  # 2 steps - (1) Forward pass through linear policy, (2) then Discretize\n",
        "\n",
        "  # HINT: current_action = ... (two lines)\n",
        "  # 2 steps - (1) Forward pass through linear policy, (2) then Discretize\n",
        "\n",
        "  # HINT: action = best_action if evaluation else current_action (one line)\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action, actor_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Check correctness of implementation (Run me) \n",
        "\n",
        "def check_random_policy_search_choose_action(choose_action):\n",
        "  key = None # not used\n",
        "  actor_state = None # not used\n",
        "\n",
        "  # obs\n",
        "  obs = jnp.array([1,1,2,4])\n",
        "\n",
        "  # eval=False checks\n",
        "  # Parameters\n",
        "  evaluation=False\n",
        "  current_params = jnp.array([-1,-1,-1,-1])\n",
        "  best_params = jnp.array([0,0,0,0])\n",
        "  # check case1 - negative dot product.\n",
        "  current_params = jnp.array([1,-2,2,-1])\n",
        "  rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "  action, actor_state = choose_action(key,rps_params,actor_state,obs,evaluation)\n",
        "  assert action == 0,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # check case2 - positive dot product\n",
        "  current_params = jnp.array([1,2,2,1])\n",
        "  rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "  action, actor_state = choose_action(key,rps_params,actor_state,obs,evaluation)\n",
        "  assert action == 1,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # check case3 - 0 dot product\n",
        "  current_params = jnp.array([0,0,0,0])\n",
        "  rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "  action, actor_state = choose_action(key,rps_params,actor_state,obs,evaluation)\n",
        "  assert action == 0,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # eval=True checks\n",
        "  evaluation=True\n",
        "  current_params = jnp.array([-1,-1,-1,-1])\n",
        "  best_params = jnp.array([0,0,0,0])\n",
        "  # check case1 - negative dot product.\n",
        "  best_params = jnp.array([1,-2,2,-1])\n",
        "  rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "  action, actor_state = choose_action(key,rps_params,actor_state,obs,evaluation)\n",
        "  assert action == 0,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # check case2 - positive dot product\n",
        "  best_params = jnp.array([1,2,2,1])\n",
        "  rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "  action, actor_state = choose_action(key,rps_params,actor_state,obs,evaluation)\n",
        "  assert action == 1,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  # check case3 - 0 dot product\n",
        "  best_params = jnp.array([0,0,0,0])\n",
        "  rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "  action, actor_state = choose_action(key,rps_params,actor_state,obs,evaluation)\n",
        "  assert action == 0,  \"Incorrect answer, your choose action function is incorrect.\"\n",
        "\n",
        "  print(\"Your function is correct!\")\n",
        "\n",
        "random_policy_search_choose_action_jit = jax.jit(random_policy_search_choose_action) # jit the function\n",
        "check_random_policy_search_choose_action(random_policy_search_choose_action_jit)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YE4xoAgyI3kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH_9qJ08IFg6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 7 solution\n",
        "def linear_policy(params,obs):\n",
        "  result = jnp.dot(params,obs)\n",
        "  return result \n",
        "\n",
        "def random_policy_search_choose_action(\n",
        "    key, \n",
        "    params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        "):\n",
        "\n",
        "  dot_product = linear_policy(params.best,obs)\n",
        "  best_action = jnp.where(dot_product <= 0,0,1)\n",
        "\n",
        "  dot_product = linear_policy(params.current,obs)\n",
        "  current_action = jnp.where(dot_product <= 0,0,1)\n",
        "\n",
        "  action = jnp.where(evaluation,best_action,current_action)\n",
        "  return action, actor_state\n",
        "\n",
        "random_policy_search_choose_action_jit = jax.jit(random_policy_search_choose_action) # jit the function\n",
        "check_random_policy_search_choose_action(random_policy_search_choose_action_jit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oXBsSa2KjWE"
      },
      "source": [
        "### RPS learn function\n",
        "Now we need to implement a `learn` function for our Random Policy Search agent. The `learn` function is quite simple. All we need to do is check if the current weights are better than the best weights. If they are better, then set the current weights to be the new best weights and randomly sample a new set of current weights. \n",
        "\n",
        "Let's assume that our learn function receives a memory from the `AverageEpisodeReturnMemory` we implemented earlier. We can use this to compare the current weights to the best weights. We will need to keep track of the best average episode return for the learn function. For that, we can use the `learn_state` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cSH4uYmRjM2"
      },
      "outputs": [],
      "source": [
        "# A NamedTuple to store the best average episode return so far\n",
        "LearnerState = collections.namedtuple(\n",
        "                                      \"LearnerState\", \n",
        "                                      [\"count\", \"best_average_episode_return\"]\n",
        "                                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Djxc9j-LzkM"
      },
      "source": [
        "**Exercise 8:** Write a function to randomly sample new weights using jax. The weights should be sampled from the interval `[-2,2]`.\n",
        "\n",
        "**Useful functions:** \n",
        "*   [Jax random uniform sample](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.uniform.html#jax.random.uniform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V2yFM2XMjGW"
      },
      "outputs": [],
      "source": [
        "def get_new_random_weights(random_key, old_weights,minval=-2.0,maxval=2.0):\n",
        "    new_weights_shape = old_weights.shape\n",
        "    new_weights_dtype = old_weights.dtype\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # you should overwrite this\n",
        "    new_weights = ...\n",
        "\n",
        "    # END YOUR CODE\n",
        "    return new_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Check correctness of implementation (Run me) \n",
        "\n",
        "def check_random_policy_search_choose_action(get_new_random_weights):\n",
        "  old_weights = jnp.array([1,1,1,1], \"float32\")\n",
        "  random_key = jax.random.PRNGKey(42)\n",
        "\n",
        "  # Case 1\n",
        "  new_weights = get_new_random_weights_jit(random_key, old_weights,minval=-2.0,maxval=2.0)\n",
        "  assert jnp.array_equal(new_weights,jnp.array([ 0.29657745,1.4265499, -1.7621555, -1.7505779 ]))\n",
        "  \n",
        "  # Case 2\n",
        "  new_weights = get_new_random_weights_jit(random_key, old_weights,minval=-0.1,maxval=0.1)\n",
        "  assert jnp.allclose(new_weights,jnp.array([0.01482888,0.0713275,-0.08810778,-0.0875289]))\n",
        "  \n",
        "  print(\"Function is correct!\")\n",
        "\n",
        "get_new_random_weights_jit = jax.jit(get_new_random_weights) # jit the function\n",
        "check_random_policy_search_choose_action(get_new_random_weights)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T8ifHTKGQlAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T05R_AWHLx_I",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 8 solution\n",
        "\n",
        "def get_new_random_weights(random_key, old_weights,minval=-2.0,maxval=2.0):\n",
        "    new_weights_shape = old_weights.shape\n",
        "    new_weights_dtype = old_weights.dtype\n",
        "    # Sample new weights\n",
        "    new_weights = jax.random.uniform(random_key,new_weights_shape,new_weights_dtype,minval=minval,\n",
        "                      maxval=maxval)\n",
        "    return new_weights\n",
        "\n",
        "get_new_random_weights_jit = jax.jit(get_new_random_weights) # jit the function\n",
        "check_random_policy_search_choose_action(get_new_random_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxZXdP8bOu6b"
      },
      "source": [
        "Now let's implement the Random Policy Search function.\n",
        "\n",
        "**Exercise 9:** Use the description of the Random Policy Search learn function at the top of this section to complete the function below. Try to use jax (remember `jnp.where()`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_policy_search_learn(key, params, learner_state, memory):\n",
        "    best_weights = params.best \n",
        "    current_weights = params.current\n",
        "\n",
        "    current_episode_return = memory.average_episode_return\n",
        "    best_average_episode_return = learner_state.best_average_episode_return\n",
        "\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # HINT: if current better than best then ...\n",
        "    best_weights = ...\n",
        "        \n",
        "    best_average_episode_return = ...\n",
        "    \n",
        "    # END YOUR CODE\n",
        "\n",
        "    # Generate new random weights\n",
        "    new_weights = get_new_random_weights_jit(key, best_weights)\n",
        "\n",
        "    # Bundle weights in RPS Params NamedTuple\n",
        "    params = RandomPolicySearchParams(current=new_weights, best=best_weights)\n",
        "\n",
        "    # Increment the learn counter by one\n",
        "    learn_count = learner_state.count + 1\n",
        "\n",
        "    return params, LearnerState(learn_count, best_average_episode_return)"
      ],
      "metadata": {
        "id": "Te_Q3qzmZcN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ElX1w1iQTrE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 9 solution\n",
        "def random_policy_search_learn(key, params, learner_state, memory):\n",
        "    best_weights = params.best \n",
        "    current_weights = params.current\n",
        "\n",
        "    current_episode_return = memory.average_episode_return\n",
        "    best_average_episode_return = learner_state.best_average_episode_return\n",
        "\n",
        "    # Update best_weights and best_average_episode_return\n",
        "    best_weights = jnp.where(current_episode_return>best_average_episode_return,current_weights,best_weights)    \n",
        "    best_average_episode_return = jnp.where(current_episode_return>best_average_episode_return,current_episode_return,best_average_episode_return)\n",
        "\n",
        "    # Generate new random weights\n",
        "    new_weights = get_new_random_weights_jit(key, best_weights)\n",
        "\n",
        "    # Bundle weights in RPS Params NamedTuple\n",
        "    params = RandomPolicySearchParams(current=new_weights, best=best_weights)\n",
        "\n",
        "    # Increment the learn counter by one\n",
        "    learn_count = learner_state.count + 1\n",
        "\n",
        "    return params, LearnerState(learn_count, best_average_episode_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ol_AxnMBdgP"
      },
      "source": [
        "Now we can put everything together using the environment loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx57tf7vRjM3"
      },
      "outputs": [],
      "source": [
        "# Jax random number generator\n",
        "initial_seed=0\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(initial_seed))\n",
        "\n",
        "initial_learner_state = LearnerState(0, -float(\"inf\"))\n",
        "\n",
        "# Jit the learn function for some extra speed\n",
        "random_policy_search_learn_jit = jax.jit(random_policy_search_learn)\n",
        "\n",
        "initial_weights = np.array([1,1,1,1], \"float32\")\n",
        "initial_params = RandomPolicySearchParams(initial_weights, initial_weights)\n",
        "\n",
        "memory = AverageEpisodeReturnBuffer(num_episodes_to_store=20)\n",
        "\n",
        "episode_return, evaluator_episode_returns = run_environment_loop(\n",
        "                                        rng, \n",
        "                                        env, \n",
        "                                        initial_params, \n",
        "                                        random_policy_search_choose_action_jit, \n",
        "                                        None, # no actor state\n",
        "                                        random_policy_search_learn_jit, \n",
        "                                        initial_learner_state, \n",
        "                                        memory, \n",
        "                                        num_episodes=2001\n",
        "                                    )\n",
        "\n",
        "# Plot graph of evaluator episode returns\n",
        "plt.plot(evaluator_episode_returns)\n",
        "plt.title(\"Random Search Evaluator Episode Return\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG10FG6uS05A"
      },
      "source": [
        "Hopefully, you found a set of optimal parameters on CartPole (if evaluator episode return eventually reaches `500`). If you haven't found optimal parameters, try running the environment loop again, with a different initial seed, you were probably just a little unlucky. That is the big limitation with Random Policy Search after all, if you are unlucky you might not (randomly) stumble on the optimal policy.\n",
        "\n",
        "So, in Random Policy Search there is very little (if any) real learning going on. We might have been able to find a reasonable policy in this case, but would this work if our policy was in higher dimensions (imagine 100s or 1000s of weights, instead of 4)?   \n",
        "\n",
        "Next, let's look to implementing a simple RL algorithm instead, that can use its experience to guide the learning process, rather than just randomly sampling new weights. Hopefully, this will help us more reliably find an optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEnSjZVESrxc"
      },
      "source": [
        "## Section 3: Policy Gradients (PG)\n",
        "As discussed, the goal in RL is to find a policy which maximise the expected cummulative reward (return) the agent receives from the environment. We can write the expected return of a policy as:\n",
        "\n",
        "$J(\\pi_\\theta)=\\mathrm{E}_{\\tau\\sim\\pi_\\theta}\\ [R(\\tau)]$,\n",
        "\n",
        "where $\\pi_\\theta$ is a policy parametrised by $\\theta$, $\\mathrm{E}$ means *expectation*, $\\tau$ is shorthand for \"*episode*\", $\\tau\\sim\\pi_\\theta$ is shorthand for \"*episodes sampled using the policy* $\\pi_\\theta$\", and $R(\\tau)$ is the return of episode $\\tau$.\n",
        "\n",
        "Then, the goal in RL is to find the parameters $\\theta$ that maximise the function $J(\\pi_\\theta)$. One way to find these parameters is to perform gradient ascent on $J(\\pi_\\theta)$ with respect to the parameters $\\theta$: \n",
        "\n",
        "$\\theta_{k+1}=\\theta_k + \\alpha \\nabla J(\\pi_\\theta)|_{\\theta_{k}}$,\n",
        "\n",
        "where $\\nabla J(\\pi_\\theta)|_{\\theta_{k}}$ is the gradient of the expected return with respect to the policy parameters $\\theta_k$ and $\\alpha$ is the step size. This quantity, $\\nabla J(\\pi_\\theta)$, is also called the **policy gradient** and is very important in RL. If we can comput the policy gradient, theat we will have a means by which to directly optimise our policy.\n",
        "\n",
        "As it turns out, there is a way for us to compute the policy gradient and the mathematical derivation can be found [here](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html). But for this tutorial we will ommit the derivation and just give you the result:\n",
        "\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) R(\\tau)]$\n",
        "\n",
        "Informaly, the policy gradient is equal to the gradient of the log of the probability of the action chosen multiplied by the return of the episode in which the action was taken.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTnTzgtSuy-y"
      },
      "source": [
        "### REINFORCE\n",
        "REINFORCE is a simple RL algorithm that uses the policy gradient to find the optimal policy by increasing the probability of choosing actions that tend to lead to high return episodes.\n",
        "\n",
        "**Exercise 10:** Implement a function that takes the probability of an action and the return of the episode the action was taken in and returns the log of the probability multiplied by the return. Make sure you use jax so that we can jit the function.\n",
        "\n",
        "**Useful functions:**\n",
        "*   [Jax numpy log](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.log.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJObUsoUrOyV"
      },
      "outputs": [],
      "source": [
        "def compute_weighted_log_prob(action_prob, episode_return):\n",
        "    weighted_log_prob = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return weighted_log_prob\n",
        "\n",
        "# TEST: the result should be -22.314354\n",
        "action_prob = 0.8\n",
        "episode_return = 100\n",
        "print(\"Weighted log prob:\", compute_weighted_log_prob(action_prob, episode_return))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x7dTAlCdrJkf"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 10 solution\n",
        "\n",
        "def compute_weighted_log_prob(action_prob, episode_return):\n",
        "    weighted_log_prob = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    weighted_log_prob = jnp.log(action_prob) * episode_return\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return weighted_log_prob\n",
        "\n",
        "# TEST\n",
        "action_prob = 0.8\n",
        "episode_return = 100\n",
        "print(\"Weighted log prob:\", compute_weighted_log_prob(action_prob, episode_return))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmgW9UJ3tIpl"
      },
      "source": [
        "### Rewards-to-go\n",
        "The gradient of the log of the actions probability, weighted by the return of the episode will tend to push up the probability of actions that were in episodes whith high return, regardless of where in the episode the action was taken. This does not really make much sense because an action near the end of an episode may be reinforced because lots of reward was collected earlier on in the episode, before the action was taken. RL agents should really only reinforce actions on the basis of their *consequences*. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come after. The cummulative rewards received after an action was taken is called the **rewards-to-go** and can be computed as:\n",
        "\n",
        "$\\hat{R}_t=\\sum_t^Tr_t$\n",
        "\n",
        "Compare the rewards-to-go with the episode return:\n",
        "\n",
        "$R(\\tau)=\\sum_{t=0}^Tr_t$\n",
        "\n",
        "Thus, the policy gradient with rewards-to-go is given by:\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\hat{R}_t]$\n",
        "\n",
        "**Exercise 11:** Implement a function that takes a list of all the rewards obtained in an episode and computes the rewards-to-go. Don't worry about using jax in this function. You can use regular Python operations like `for-loops`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV1Hww8E3dUJ"
      },
      "outputs": [],
      "source": [
        "# Implement reward to go\n",
        "def rewards_to_go(rewards):\n",
        "    \"\"\"\n",
        "    This function should take a list of rewards as input and \n",
        "    compute the rewards-to-go for each timestep.\n",
        "    \n",
        "    Arguments:\n",
        "        rewards[t] is the reward at time step t.\n",
        "\n",
        "    Returns:\n",
        "        rewards_to_go[t] should be the reward-to-go at timestep t.\n",
        "    \"\"\"\n",
        "\n",
        "    rewards_to_go = []\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return rewards_to_go\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2qy2F3xRjM3"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 11 solution\n",
        "\n",
        "def rewards_to_go(rewards):\n",
        "    rewards_to_go = []\n",
        "    for i in range(len(rewards)):\n",
        "        r2g = 0\n",
        "        for j in range(i, len(rewards)):\n",
        "            r2g += rewards[j]\n",
        "        rewards_to_go.append(r2g)\n",
        "    return rewards_to_go\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IsJg3PxVV2e"
      },
      "outputs": [],
      "source": [
        "# Faster rewards to go calculation using numpy\n",
        "def rewards_to_go(rewards):\n",
        "    return np.flip(np.cumsum(np.flip(rewards)))\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboxN9MS65i5"
      },
      "source": [
        "Next we will need to make a new agent memory to store the rewards-to-go $R_t$ along with the observation $o_t$ and action $a_t$ at every timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhS4V6auRjM3"
      },
      "outputs": [],
      "source": [
        "# Now we need a new episode memory buffer\n",
        "\n",
        "EpisodeRewardsToGoMemory = collections.namedtuple(\"AverageEpisodeReturnMemory\", [\"obs\", \"action\", \"reward_to_go\"])\n",
        "\n",
        "class EpisodeRewardsToGoBuffer:\n",
        "\n",
        "    def __init__(self, num_transitions_to_store=500, batch_size=500):\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_buffer = collections.deque(maxlen=num_transitions_to_store)\n",
        "        self.current_episode_transition_buffer = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_transition_buffer.append(transition)\n",
        "\n",
        "        if transition.done:\n",
        "\n",
        "            episode_rewards = []\n",
        "            for t in self.current_episode_transition_buffer:\n",
        "                episode_rewards.append(t.reward)\n",
        "\n",
        "            r2g = rewards_to_go(episode_rewards)\n",
        "\n",
        "            for i, t in enumerate(self.current_episode_transition_buffer):\n",
        "                memory = EpisodeRewardsToGoMemory(t.obs, t.action, r2g[i])\n",
        "                self.memory_buffer.append(memory)\n",
        "\n",
        "            # Reset episode buffer\n",
        "            self.current_episode_transition_buffer = []\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.memory_buffer) >= self.batch_size\n",
        "\n",
        "    def sample(self):\n",
        "        random_memory_sample = random.sample(self.memory_buffer, self.batch_size)\n",
        "\n",
        "        obs_batch, action_batch, reward_to_go_batch = zip(*random_memory_sample)\n",
        "\n",
        "        return EpisodeRewardsToGoMemory(\n",
        "            np.stack(obs_batch).astype(\"float32\"), \n",
        "            np.asarray(action_batch).astype(\"int32\"), \n",
        "            np.asarray(reward_to_go_batch).astype(\"int32\")\n",
        "        )\n",
        "\n",
        "\n",
        "# Instantiate Memory\n",
        "REINFORCE_memory = EpisodeRewardsToGoBuffer(num_transitions_to_store=512, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idkav_aSYXvz"
      },
      "source": [
        "### Policy neural network\n",
        "Next, we need to aproximate the policy using a simple neural network. Our policy neural network will have an input layer that takes the observation as input and passes it through two hidden layers and then outputs one scalar value for each of the possible actions. So, in CartPole the output layer will have size `2`.\n",
        "\n",
        "[Haiku](https://github.com/deepmind/dm-haiku) is a library for implementing neural networks is Jax. Below we have implemented a simple function to make the policy network for you. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2XO7VkORjM4"
      },
      "outputs": [],
      "source": [
        "def make_policy_network(num_actions: int, layers=[10, 10]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for the policy.\"\"\"\n",
        "\n",
        "  def policy_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [\n",
        "            hk.Flatten(),\n",
        "            hk.nets.MLP(layers + [num_actions])\n",
        "        ]\n",
        "    )\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(policy_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GR2y8FjaG-G"
      },
      "source": [
        "Haiku networks have two important functions you need to know about. The first is the `<network>.init(<rng>, <input>)`, which returns a set of random initial parameters. The second method is the `<network>.apply(<params>, <input>)` which passes an input through the network using the set of parameters provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJrn9o-Vatkw"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "POLICY_NETWORK = make_policy_network(num_actions=2, layers=[20,20])\n",
        "random_key = next(rng) # get next random key\n",
        "dummy_obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "REINFORCE_params = POLICY_NETWORK.init(random_key, dummy_obs)\n",
        "print(\"Initial params:\", REINFORCE_params.keys())\n",
        "\n",
        "output = POLICY_NETWORK.apply(REINFORCE_params, dummy_obs)\n",
        "print(\"Policy network output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlouUBvoeunz"
      },
      "source": [
        "The outputs of our policy network are [logits](https://qr.ae/pv4YTe). To convert this into a probability distribution over actions we pass the logits to the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function (more on this later).\n",
        "\n",
        "### Action selection\n",
        "\n",
        "**Exercise 12:** Complete the function below which takes a vector of logits and randomly samples an action. \n",
        "\n",
        "**Useful functions:**\n",
        "*   [Jax random categorical](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.categorical.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Z8DxUmeOGJ"
      },
      "outputs": [],
      "source": [
        "def sample_action(random_key, logits):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # END YOUR code\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(\"Action:\", sample_action(next(rng), np.array([0, 1], \"float32\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2huSgyukg1N4"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 12 solution\n",
        "\n",
        "def sample_action(random_key, logits):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    action = jax.random.categorical(random_key, logits)\n",
        "    # END YOUR code\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST: \n",
        "for i in range(10):\n",
        "  print(\"Action:\", sample_action(next(rng), np.array([0, 1], \"float32\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn2yzwDrhmUI"
      },
      "source": [
        "Notice in the tests that the actions are randomly sampled. Ofcourse the action with the higher probability will be chose more often because, but there is always a chance the action with the lower probability will be chosen. This is actually desirable in RL because it mean the agent will always try new things in the environment. We call this **exploring**. Exploring is important because it helps the agent discover new, possibly better, strategies in the environment. When an agent chooses the best possible action (given its current knowledge) we say the agent is being **greedy**.\n",
        "\n",
        "**Exercise 13:** Complete the function below which takes a vector of logits and returns the greedy action. \n",
        "\n",
        "**Useful functions:**\n",
        "*   [Jax numpy argmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.argmax.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG_4qq6RjLCg"
      },
      "outputs": [],
      "source": [
        "def greedy_action(logits):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "    print(\"Action:\", greedy_action(np.array([0, 1], \"float32\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r2sa7wqFjHb-"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 13 solution\n",
        "\n",
        "def greedy_action(logits):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "    action = jnp.argmax(logits)\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "    print(\"Action:\", greedy_action(np.array([0, 1], \"float32\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MffEyZUjyaF"
      },
      "source": [
        "Notice that the greed action selector always chooses the same action, namely the one with the highest probability (or equivalently the largets logit). Next, we will implement the REINFORCE `select_action` function. The function passes the observation through the policy neural network to get loggits and then uses thos to chose an action. If `evaluation` is `True`, the greedy action will be chosen. Otherwise, the action is sampled from the action probability distribution given by the logits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP5UH87VRjM4"
      },
      "outputs": [],
      "source": [
        "def REINFORCE_select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim\n",
        "    logits = POLICY_NETWORK.apply(params, obs)[0] # remove batch dim\n",
        "\n",
        "    sampled_action = sample_action(key, logits)\n",
        "\n",
        "    best_action = greedy_action(logits)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        evaluation,\n",
        "        best_action,\n",
        "        sampled_action\n",
        "    )\n",
        "    \n",
        "    return action, actor_state\n",
        "\n",
        "# TEST\n",
        "action, actor_state = REINFORCE_select_action(\n",
        "    key=next(rng),\n",
        "    params=REINFORCE_params, # we instantiated this earlier\n",
        "    actor_state=None, # not used\n",
        "    obs=np.array([1,1,1,1], \"float32\") # dummy obs\n",
        ")\n",
        "\n",
        "print(\"Action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDhTH3culwqo"
      },
      "source": [
        "Now that we finished the REINFORCE action selection function, all we have left to do is make a REINFORCE learn function. The learn function should use the `weighted_log_prob` function we made earlier to compute the policy gradient and apply the updates to our neural network.\n",
        "\n",
        "### Network Optimiser\n",
        "\n",
        "To apply updates to our neural network we will use a Jax library called [Optax](https://github.com/deepmind/optax). Optax has an implementation of the [Adam optimizer](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/) which we can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxXINlMHP5Ic"
      },
      "outputs": [],
      "source": [
        "REINFORCE_OPTIMIZER = optax.adam(1e-3)\n",
        "REINFORCE_optim_state = REINFORCE_OPTIMIZER.init(REINFORCE_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ALCJESQJ8e"
      },
      "source": [
        "### Policy gradient loss\n",
        "\n",
        "**Exercise 14:** Complete the `pg_loss` function below.\n",
        "\n",
        "**Useful methods:**\n",
        "*   [Jax softmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html)\n",
        "*   [Jax one-hot vector](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.one_hot.html)\n",
        "*   [Jax dot product](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sUKkqx0RjM4"
      },
      "outputs": [],
      "source": [
        "def pg_loss(action, logits, reward_to_go):\n",
        "    chosen_action_prob = 0.0 # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # HINT all_action_probs = ... (convert logits into probs)\n",
        "\n",
        "    # HINT extract the prob of the desired action.\n",
        "    # One way to achieve this is to use a one-hot vector and a dot product...?\n",
        "\n",
        "    # END YOUR CODE\n",
        "    weighted_log_prob = compute_weighted_log_prob(\n",
        "                            chosen_action_prob, \n",
        "                            reward_to_go\n",
        "                        )\n",
        "    \n",
        "    loss = - weighted_log_prob # negative because we want gradient `ascent`\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# TEST \n",
        "print(\"Policy gradient loss:\", pg_loss(0, np.array([0,1], \"float32\"), 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1HAxSFU0qBVo"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 14 solution\n",
        "\n",
        "def pg_loss(action, logits, reward_to_go):\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    all_action_probs = jax.nn.softmax(logits)\n",
        "    action_mask = jax.nn.one_hot(action, logits.shape[0])\n",
        "    chosen_action_prob = jnp.dot(all_action_probs, action_mask)\n",
        "\n",
        "    # END YOUR CODE\n",
        "    \n",
        "    weighted_log_prob = compute_weighted_log_prob(\n",
        "                            chosen_action_prob, \n",
        "                            reward_to_go\n",
        "                        )\n",
        "    \n",
        "    loss = - weighted_log_prob \n",
        "    \n",
        "    return loss\n",
        "\n",
        "# TEST \n",
        "print(\"Policy gradient loss:\", pg_loss(0, np.array([0,1], \"float32\"), 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzuqx1jJrwVx"
      },
      "source": [
        "Now, when we do a policy gradient update step we are going to want to do it using a batch of experience, rather than just a single experience like above. We can use Jax's [vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap) function to easily make our `pg_loss` function work on a batch of experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yq4naLURjM4"
      },
      "outputs": [],
      "source": [
        "def batched_pg_loss(params, obs_batch, action_batch, reward_to_go_batch):\n",
        "    logits_batch = POLICY_NETWORK.apply(params, obs_batch) # network we made earlier\n",
        "    pg_loss_batch = jax.vmap(pg_loss)(action_batch, logits_batch, reward_to_go_batch) # add batch\n",
        "    mean_pg_loss = jnp.mean(pg_loss_batch)\n",
        "    return mean_pg_loss\n",
        "\n",
        "# TEST\n",
        "obs_batch = np.array([[1,0,0,1],[1,0,0,1],[1,0,0,1]])\n",
        "actions_batch = np.array([1,0,0])\n",
        "rew2go_batch = np.array([2.3, 4.3, 2.1])\n",
        "\n",
        "loss = batched_pg_loss(REINFORCE_params, obs_batch, actions_batch, rew2go_batch)\n",
        "\n",
        "print(\"PG loss on batch:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViENrHOALbCw"
      },
      "source": [
        "Now we can make the REINFORCE learn function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQr2Uz5ORjM5"
      },
      "outputs": [],
      "source": [
        "REINFORCELearnerState = collections.namedtuple(\"LearnerState\", [\"optim_state\"])\n",
        "\n",
        "def REINFORCE_learn(key, params, learner_state, memory):\n",
        "    \n",
        "    #Get Policy gradient by using `jax.grad()` on `batched_pg_loss`\n",
        "    grad_loss = jax.grad(batched_pg_loss)(params, memory.obs, memory.action, memory.reward_to_go)\n",
        "\n",
        "    # Get param updates using gradient and optimizer\n",
        "    updates, new_optim_state = REINFORCE_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "    # Apply updates to params\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return params, REINFORCELearnerState(new_optim_state) # update learner state\n",
        "\n",
        "# Lets jit the learn function and the select action function for some extra speed\n",
        "REINFORCE_learn_jit = jax.jit(REINFORCE_learn)\n",
        "REINFORCE_select_action_jit = jax.jit(REINFORCE_select_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5an3U2NhRKgG"
      },
      "source": [
        "### Training\n",
        "Now we can train our REINFORCE agent by putting everything together using the environment loop. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vioIcVGsRjM5"
      },
      "outputs": [],
      "source": [
        "learner_state = REINFORCELearnerState(REINFORCE_optim_state)\n",
        "actor_state = None # not used\n",
        "\n",
        "episode_returns, evaluator_returns = run_environment_loop(rng, env, REINFORCE_params, REINFORCE_select_action_jit , actor_state, \n",
        "    REINFORCE_learn_jit, learner_state, REINFORCE_memory, num_episodes=1001)\n",
        "\n",
        "# Plot the episode returns over time\n",
        "plt.plot(episode_returns)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_HHzFTOc-Qr"
      },
      "source": [
        "## Section 4: Q-Learning\n",
        "Another common aproach to finding an optimal policy in an environment in RL is via Q-learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1nOZrUSzhE"
      },
      "source": [
        "### State-Action Value function\n",
        "In Q-learning the agent learns a function that approximates the **value** of state-action pairs. By *value* we mean the return you expect to receive if you start in a particular state $s_t$, take a particular action $a_t$, and then act according to a particular policy $\\pi$ forever after. The state-action value function of policy $\\pi$ is given by\n",
        "\n",
        "$Q_\\pi(s,a)=\\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s,\\ a_t=a\\right]$.\n",
        "\n",
        "We say that the value function $Q_\\pi(s,a)$ is the **optimal** value function if the policy $\\pi$ is an optimal policy. We denote the optimal value function as follows:\n",
        "\n",
        "$Q_\\ast(s,a)=\\max \\limits_\\pi \\  \\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s,\\ a_0=a\\right]$\n",
        "\n",
        "There is an important relationship between the optimal action $a_\\ast$ in a state $s$ and the optimal state-action value function $Q_\\ast$. Namely, the optimal action $a_\\ast$ in state $s$ is equal to the action that maximises the optimal state-action value function. This relationship naturally induces an optimal policy:\n",
        "\n",
        "$\\pi_\\ast(s)=\\arg \\max \\limits_a\\ Q_\\ast(s, a)$\n",
        "\n",
        "This kind of policy is an example of a **greedy** policy. It is an optimal action selection strategy when the Q-function is optimal. However, since we approximate the Q-function, the resulting greedy policy may not be optimal. In some environments, such a policy still yields decent behavior, however, most environments require an **exploration** strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2x2tqvZSihz"
      },
      "source": [
        "### Greedy action selection\n",
        "\n",
        "**Exercise 15:** Let's implement a function that, given a vector of Q-values, returns the action with the largest Q-value (i.e. the greedy action).\n",
        "\n",
        "**Useful methods:**\n",
        "*   [Jax argmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.argmax.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn9P1YdzTIDU"
      },
      "outputs": [],
      "source": [
        "# Implement a function takes q-values as input and returns the greedy_action\n",
        "def select_greedy_action(q_values):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBzLr_G7QKXR"
      },
      "outputs": [],
      "source": [
        "# @title Check correctness of implementation (Run me) \n",
        "\n",
        "def check_select_greedy_action(select_greedy_action):\n",
        "  q_values = jnp.array([1,1,3,4])\n",
        "  action = select_greedy_action(q_values)\n",
        "\n",
        "  assert action == 3, \"Incorrect answer, your greedy action selector looks wrong\"\n",
        "\n",
        "  print(\"Looks good.\")\n",
        "\n",
        "check_select_greedy_action(select_greedy_action)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GsPv35lRjM6"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 15 solution\n",
        "\n",
        "def select_greedy_action(q_values):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE\n",
        "    action = jnp.argmax(q_values)\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFQUlqZ4ZyLp"
      },
      "source": [
        "### Q-Network\n",
        "Unlike in the policy gradient approaches from the previous section, Q-learning and other value-based reinforcement learning approaches don't need a direct parameterisation for the policy. This is because approximated Q-function implicitly stores a policy which can be recovered using: \n",
        "\n",
        "$\\hat{\\pi}(s)=\\arg \\max \\limits_a\\ Q_{\\theta}(s, a)$\n",
        "\n",
        "As we did previously, we shall use haiku to make a neural network to approximate this Q-function. The network will take an observation as input and then output a Q-value for each of the available actions. So in the case of CartPole, the output of the network will have size $2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wU1soJYZyLp"
      },
      "outputs": [],
      "source": [
        "def build_network(num_actions: int, layers=[10, 10]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for approximating Q-values.\"\"\"\n",
        "\n",
        "  def q_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [hk.Flatten(),\n",
        "         hk.nets.MLP(layers + [num_actions])])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(q_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwG-4qTS1zx"
      },
      "source": [
        "Let's initialise our Q-network and get the initial weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uvq5n2cS9lu"
      },
      "outputs": [],
      "source": [
        "# Initialise Q-network\n",
        "Q_NETWORK = build_network(num_actions=2) # two actions\n",
        "\n",
        "dummy_obs = jnp.zeros((1,4), jnp.float32) # a dummy observation like the one in CartPole\n",
        "\n",
        "Q_NETWORK_WEIGHTS = Q_NETWORK.init(next(rng), dummy_obs) # Get initial weights\n",
        "\n",
        "print(\"Q-Learning params:\", Q_NETWORK_WEIGHTS.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we implement the loss function required for training our Q-network. Let's first discuss the intuition behind it. "
      ],
      "metadata": {
        "id": "iqCUeZfhEfyP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbLig3uSZyLp"
      },
      "source": [
        "### The Bellman Equations\n",
        "The value function can be written recursively as:\n",
        "\n",
        "$Q_{\\pi}(s, a) =\\underset{s^{\\prime} \\sim P}{\\mathrm{E}}\\left[r(s, a)+\\gamma \\underset{a^{\\prime} \\sim \\pi}{\\mathrm{E}}\\left[Q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\\right]$,\n",
        "\n",
        "where $s' \\sim P$ is shorthand for saying that the next state $s'$ is sampled from the environmentâs transition function $P(s'\\mid s,a)$. Intuitively, this equation says that the value of the state you are in is equal to the reward you expect to get from being there, plus the \"average\" value across the possible actions in the state you transition to next. The Bellman equation for the optimal value function is:\n",
        "\n",
        "$Q_{*}(s, a) =\\underset{s^{\\prime} \\sim P}{\\mathrm{E}}\\left[r(s, a)+\\gamma\\ \\underset{a^{\\prime}}{\\max}\\ Q_{*}(s^{\\prime}, a^{\\prime})\\right]$\n",
        "\n",
        "Notice that in this version of the state-action value function the \"average\" value across actions in the neighbouring state $s^{\\prime}$ is replaced by the value of the action in $s^{\\prime}$ with the largest action-value.\n",
        "\n",
        "\n",
        "For a more in-depth discussion of the Bellman Equations, see the [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsOJi5G8ZyLp"
      },
      "source": [
        "### The Bellman Backup\n",
        "To learn to approximate the optimal Q-value function, we can use the right-hand side of the Bellman equation as an update rule. In other words, suppose we have a Q-function $Q_\\theta$ approximated using parameters $\\theta$ then we can iteratively update the parameters such that\n",
        "\n",
        "$Q_\\theta(s,a)\\leftarrow r(s, a) + \\gamma \\underset{a'}{\\max}\\ Q_\\theta(s', a')$.\n",
        "\n",
        "Intuitively, this says that the approximation of the Q-value of action $a$ in state $s$ should be updated such that it is closer to being equal to the reward received from the environment $r(s, a)$ plus the value of best possible action in the next state $s'$. We can perform this optimisation by minimising the difference between the left and right-hand side, with respect to the parameters $\\theta$ using gradient descent. We can measure the difference between the two values using the [squared-error](https://en.wikipedia.org/wiki/Mean_squared_error#Loss_function).\n",
        "\n",
        "**Exercise 16:** Implement the squared-error function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTto__ohZyLp"
      },
      "outputs": [],
      "source": [
        "def compute_squared_error(pred, target):\n",
        "  # YOUR CODE\n",
        "  squared_error = ...\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms7pHyHGZyLp"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 16 solution\n",
        "def compute_squared_error(pred, target):\n",
        "    squared_error = None\n",
        "\n",
        "    # YOUR CODE\n",
        "    squared_error = jnp.square(pred - target)\n",
        "    # END YOUR CODE\n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycpZVkgdZyLp"
      },
      "source": [
        "**Exercise 17:** Implement a function that computes the **Bellman target** (right-hand side of the Bellman equation). If the episode is at the last timestep (i.e. done==1.0), then the Bellman target should be equal to the reward, with no extra value at the end.  \n",
        "\n",
        "Assume a discount factor of 1, ($\\gamma = 1$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "961_OllWZyLp"
      },
      "outputs": [],
      "source": [
        "# Bellman target\n",
        "def compute_bellman_target(reward, done, next_q_values):\n",
        "  \"\"\"A function to compute the bellman target.\n",
        "  \n",
        "  Args:\n",
        "      reward: a scalar reward.\n",
        "      done: a scalar of value either 1.0 or 0.0, indicating if the transition is a terminal one.\n",
        "      next_q_values: a vector of q_values for the next state. One for each action.\n",
        "  Returns:\n",
        "      A scalar equal to the bellman target.\n",
        "  \n",
        "  \"\"\"\n",
        "  # YOUR CODE\n",
        "  bellman_target = ...\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return bellman_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbAGg-_yZyLq"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 17 solution\n",
        "\n",
        "# Bellman target\n",
        "def compute_bellman_target(reward, done, next_q_values):\n",
        "    \"\"\"A function to compute the bellman target.\n",
        "    \n",
        "    Args:\n",
        "        reward: a scalar reward.\n",
        "        done: a scalar of value either 1.0 or 0.0, indicating if the transition is a terminal one.\n",
        "        next_q_values: a vector of q_values for the next state. One for each action.\n",
        "    Returns:\n",
        "        A scalar equal to the bellman target.\n",
        "    \n",
        "    \"\"\"\n",
        "    # YOUR CODE\n",
        "    bellman_target = reward + (1.0 - done) * jnp.max(next_q_values)\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return bellman_target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sIjrHSJZyLq"
      },
      "source": [
        "We can now combine these two functions to compute the loss for Q-learning. The Q-learning loss is equal to the squared difference between the predicted Q-value of an action and its corresponding Bellman target.\n",
        "\n",
        "**Exercise 18:** Implement the Q-learning loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJY_kpFcZyLq"
      },
      "outputs": [],
      "source": [
        "def q_learning_loss(q_values, action, reward, done, next_q_values):\n",
        "    \"\"\"Implementation of the Q-learning loss.T\n",
        "    \n",
        "    Args:\n",
        "        q_values: a vector of Q-values, one for each action.\n",
        "        action: an integer, giving the action that was chosen. q_values[action] is the value of the chose action.\n",
        "        done: is a scalar that indicates if this is a terminal transition.\n",
        "        next_q_values: a vector of Q-values in the next state.\n",
        "    Returns:\n",
        "        The squared difference between the q_value of the chosen action and the bellman target.\n",
        "    \"\"\"\n",
        "    # YOUR CODE\n",
        "    chosen_action_q_value = ...\n",
        "    bellman_target = ...\n",
        "    squared_error = ...\n",
        "    # END YOUR CODE\n",
        "    \n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_KQleTsZyLq"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 18 solution\n",
        "\n",
        "def q_learning_loss(q_values, action, reward, done, next_q_values):\n",
        "    \"\"\"Implementation of the Q-learning loss.T\n",
        "    \n",
        "    Args:\n",
        "        q_values: a vector of Q-values, one for each action.\n",
        "        action: an integer, giving the action that was chosen. q_values[action] is the value of the chose action.\n",
        "        done: is a scalar that indicates if this is a terminal transition.\n",
        "        next_q_values: a vector of Q-values in the next state.\n",
        "    Returns:\n",
        "        The squared difference between the q_value of the chosen action and the bellman target.\n",
        "    \"\"\"\n",
        "    # YOUR CODE\n",
        "    chosen_action_q_value = q_values[action]\n",
        "    bellman_target = compute_bellman_target(reward, done, next_q_values)\n",
        "    squared_error = compute_squared_error(chosen_action_q_value, bellman_target)\n",
        "    # END YOUR CODE\n",
        "    \n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4YnSUfJZyLq"
      },
      "source": [
        "### Target Q-network\n",
        "Notice that when we compute the bellman target we are using our Q-network $Q_\\theta$ to compute the value for the next state $s_t$. We are basically using our latest approximation of the Q-function to compute the target of our next approximation. Using an approximation to compute the target for your next approximation, is called bootstrapping. Unfortunately, if we naively bootstrap like this, it can make training a neural network very unstable. To mitigage this we can instead use a different set of parameters $\\hat{\\theta}$ to compute the values at state $s_{t+1}$. We will keep the parameters $\\hat{\\theta}$ fixed and only periodically update them to be equal to the latest online parameters $\\theta$ every couple of training steps *(say 100)*. This serves to keep the bellman targets fixed for a couple training steps to help reduce the instability due to bootstrapping. \n",
        "\n",
        "\n",
        "We will need to keep track of the latest (online) parameters, as well as the target networks parameters. Lets make a `namedtuple` to store these two values. We will also need to keep track of the number of learner steps we have taken, so that we know when to update the target network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvZqUKmq6L7k"
      },
      "outputs": [],
      "source": [
        "QLearnerState = collections.namedtuple(\"LearnerState\", [\"count\", \"optim_state\"])\n",
        "QLearnerParams = collections.namedtuple(\"Params\", [\"online\", \"target\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJWH2_kNZsau"
      },
      "source": [
        "We will once again be using Optax to optimize our neural network in Jax. Here we instantiate the optimizer and add the initial Q-network weights to a `QLearnerParams` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwqKTN6BaAXE"
      },
      "outputs": [],
      "source": [
        "# Initialise Q-network optimizer\n",
        "Q_LEARN_OPTIMIZER = optax.adam(1e-3) # learning rate = 0.001\n",
        "\n",
        "Q_LEARN_OPTIM_STATE = Q_LEARN_OPTIMIZER.init(Q_NETWORK_WEIGHTS) # initial optim state\n",
        "\n",
        "# Create Learn State\n",
        "Q_LEARNING_LEARN_STATE = QLearnerState(0, Q_LEARN_OPTIM_STATE) # count set to zero initially\n",
        "\n",
        "# Add initial Q-network weights to QLearnerParams object\n",
        "Q_LEARNING_PARAMS = QLearnerParams(online=Q_NETWORK_WEIGHTS, target=Q_NETWORK_WEIGHTS) # target equal to online"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj89M8LgZlFe"
      },
      "source": [
        "Now we can implement a simple function that updates the learners parameters every 100 training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKrg_3rO6cCL"
      },
      "outputs": [],
      "source": [
        "def update_target_params(learn_state, online_weights, target_weights):\n",
        "\n",
        "  target = jax.lax.cond(\n",
        "      jax.numpy.mod(learn_state.count, 100) == 0,\n",
        "      lambda x, y: x,\n",
        "      lambda x, y: y,\n",
        "      online_weights, \n",
        "      target_weights\n",
        "  )\n",
        "\n",
        "  params = QLearnerParams(online_weights, target)\n",
        "\n",
        "  return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoiaYSo9ZyLq"
      },
      "source": [
        "### Q-learning loss\n",
        "We now have everything we need to implement the `q_learn_step` function which takes some batch of transitions and does a step of Q-learning to update the network paramters. But first we use `jax.vmap` to modify the `q_learning_loss` function so that it accepts batches of transitions. In addition, we will compute the Q-values by passing the observations through the `Q_NETWORK` and the target Q-values using the target parameters of the `Q_Network`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnrsGppWZyLq"
      },
      "outputs": [],
      "source": [
        "def batched_q_learning_loss(online_params, target_params, obs, actions, rewards, next_obs, dones):\n",
        "    q_values = Q_NETWORK.apply(online_params, obs)\n",
        "    next_q_values = Q_NETWORK.apply(target_params, next_obs)\n",
        "    squared_error = jax.vmap(q_learning_loss)(q_values, actions, rewards, dones, next_q_values) # vmap\n",
        "    mean_squared_error = jnp.mean(squared_error) # mean squared error\n",
        "    return mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU8vo9ZebnEa"
      },
      "source": [
        "Now we can create the `q_learner_step` function which computes the gradient of the `batched_q_learning_loss` and then uses an Optax optimizer to update the network weights and then finally (maybe) updates the target parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BYoX2W_ZyLr"
      },
      "outputs": [],
      "source": [
        "def q_learner_step(rng, params, learner_state, memory):\n",
        "  # Compute gradients\n",
        "  grad_loss = jax.grad(batched_q_learning_loss)(params.online, params.target, memory.obs, \n",
        "                                          memory.action, memory.reward, \n",
        "                                          memory.next_obs, memory.done,\n",
        "                                          )\n",
        "\n",
        "  # Get updates\n",
        "  updates, opt_state = Q_LEARN_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "  # Apply them\n",
        "  new_weights = optax.apply_updates(params.online, updates)\n",
        "\n",
        "  # Maybe update target network\n",
        "  params = update_target_params(learner_state, new_weights, params.target)\n",
        "\n",
        "  # Increment learner step counter\n",
        "  learner_state = QLearnerState(learner_state.count + 1, opt_state)\n",
        "\n",
        "  return params, learner_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpZsKHssZyLq"
      },
      "source": [
        "### Replay Buffer\n",
        "For Q-learning we will need an agent memory that stores entire transitions: `obs`, `action`, `reward`, `next_obs`, `done`. When we retrieve transitions from the memory, they should be chosen randomly. In RL we often call such a module a **replay buffer**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tv5dUH6ZyLr"
      },
      "outputs": [],
      "source": [
        "class TransitionMemory(object):\n",
        "  \"\"\"A simple Python replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, max_size=5000, batch_size=256):\n",
        "    self.batch_size = batch_size\n",
        "    self.buffer = collections.deque(maxlen=max_size)\n",
        "\n",
        "  def push(self, transition):\n",
        "\n",
        "      self.buffer.append(\n",
        "          (transition.obs, transition.action, transition.reward, \n",
        "           transition.next_obs, transition.done)\n",
        "      )\n",
        "\n",
        "  \n",
        "  def is_ready(self):\n",
        "    return self.batch_size <= len(self.buffer)\n",
        "\n",
        "  def sample(self):\n",
        "    random_replay_sample = random.sample(self.buffer, self.batch_size)\n",
        "    obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*random_replay_sample)\n",
        "\n",
        "    return Transition(\n",
        "        np.stack(obs_batch).astype(\"float32\"), \n",
        "        np.asarray(action_batch).astype(\"int32\"), \n",
        "        np.asarray(reward_batch).astype(\"float32\"), \n",
        "        np.stack(next_obs_batch).astype(\"float32\"), \n",
        "        np.asarray(done_batch).astype(\"float32\")\n",
        "    )\n",
        "\n",
        "Q_LEARNING_MEMORY = TransitionMemory(max_size=5000, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbHk03VVUHAV"
      },
      "source": [
        "### Random exploration\n",
        "We almost have everything we need for a functioning Q-learning agent. But one problem is that if we always choose the action with the highest Q-value as our policy then the agent's policy will be completly deterministic. This means the agent will always choose the same strategy. This can pose a problem because at the start of training, the Q-network will be very inaccurate (i.e. a bad aproximation of the true Q-function). As such, the agent will consistently choose suboptimal actions. Moreover, the agent will never deviate from its suboptimal strategy and will never discover new, potentially more rewarding  actions. As a result, the Q-network remains inaccurate. Ideally, the agent should try out many different strategies so that it can observe the outcomes (rewards) of its actions in different states and so improve its approximation of the Q-function.\n",
        "\n",
        "One easy way to ensure that the agent tries out many different actions is to let it periodically choose some random actions, instead of the greedy (best) action all the time.\n",
        "\n",
        "**Exercise 19:** Implement a function that, given the number of possible (discrete) actions, returns a random action.\n",
        "\n",
        "**Useful methods:**\n",
        "\n",
        "*   [Jax random int](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.randint.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUKkpMLXUtko"
      },
      "outputs": [],
      "source": [
        "def select_random_action(key, num_actions):\n",
        "    \n",
        "    # YOUR CODE\n",
        "    action = ...\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(f\"Random action number {i}: {select_random_action(next(rng), 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrVadhcwRjM6"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 19 solution\n",
        "\n",
        "def select_random_action(key, num_actions):\n",
        "    # YOUR CODE\n",
        "    action = jax.random.randint(\n",
        "        key, \n",
        "        shape=(), \n",
        "        minval=0, \n",
        "        maxval=num_actions\n",
        "    )\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(f\"Random action number {i}: {select_random_action(next(rng), 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-kKDFT6XU6y"
      },
      "source": [
        "### $\\varepsilon$-greedy action selection\n",
        "At the start of training, when the accuracy of the Q-network is low, it is worthwhile for the agent to mostly take random actions. However, as the accuracy of the Q-network improves, the agent should start taking fewer random actions and instead start choosing the greedy actions with respect to the Q-values. Choosing actions from the current implicit or explicit policy is referred to as **exploitation.** In RL we often call the ratio of random to greedy actions **epsilon** $\\varepsilon$. Epsilon is usually a decimal value in the interval $[0,1]$, where for example $\\varepsilon=0.4$ means that the agent chooses a random action 40% of the time and the greedy action 60% of the time. It is common in RL to linearly decrease the value of epsilon over time so that the agent becomes increasingly greedy as the accuracy of its Q-network improves through learning.\n",
        "\n",
        "\n",
        "**Exercise 17:** Implement a function that takes the number of timesteps as input and returns the current epsilon value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qejnbCocurG"
      },
      "outputs": [],
      "source": [
        "EPSILON_DECAY_TIMESTEPS = 1_000\n",
        "EPSILON_MIN = 0.1 # 10% exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ujSbssCZyLs"
      },
      "outputs": [],
      "source": [
        "def get_epsilon(num_timesteps):\n",
        "  # YOUR CODE\n",
        "  epsilon = ...\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return epsilon\n",
        "\n",
        "# TEST\n",
        "\n",
        "print(\"Epsilon after 10 timesteps:\", get_epsilon(10))\n",
        "print(\"Epsilon after 10 010 timesteps:\", get_epsilon(5_010))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTpezAeUYFR7"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 20 solution\n",
        "\n",
        "def get_epsilon(num_timesteps):\n",
        "\n",
        "  # YOUR CODE\n",
        "  epsilon = 1.0 - num_timesteps / EPSILON_DECAY_TIMESTEPS\n",
        "\n",
        "  epsilon = jax.lax.select(\n",
        "      epsilon < EPSILON_MIN,\n",
        "      EPSILON_MIN,\n",
        "      epsilon\n",
        "  )\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return epsilon\n",
        "\n",
        "# TEST\n",
        "\n",
        "print(\"Epsilon after 10 timesteps:\", get_epsilon(10))\n",
        "print(\"Epsilon after 10 010 timesteps:\", get_epsilon(5_010))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t56oo58TVQ_s"
      },
      "source": [
        "**Exercise 21:** Now lets put these functions together to do epsilon-greedy action selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQx8K4vKUXj"
      },
      "outputs": [],
      "source": [
        "def select_epsilon_greedy_action(key, q_values, num_timesteps):    \n",
        "    # YOUR CODE HERE\n",
        "    action = ...\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "dummy_q_values = jnp.array([0,1], jnp.float32)\n",
        "num_timesteps = 5010 # very greedy\n",
        "print(\"Greedy actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")\n",
        "print()\n",
        "\n",
        "num_timesteps = 0 # completly random\n",
        "print(\"Random actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXDRQhORRjM6"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 21 solution\n",
        "\n",
        "# Now make a function that takes an epsilon-greedy action\n",
        "\n",
        "def select_epsilon_greedy_action(key, q_values, num_timesteps):\n",
        "\n",
        "    epsilon = get_epsilon(num_timesteps)\n",
        "\n",
        "    should_explore = jax.random.uniform(key, (1,))[0] < epsilon\n",
        "\n",
        "    num_actions = len(q_values)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        should_explore,\n",
        "        select_random_action(key, num_actions), \n",
        "        select_greedy_action(q_values)\n",
        "    )\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "dummy_q_values = jnp.array([0,1], jnp.float32)\n",
        "num_timesteps = 5010 # very greedy\n",
        "print(\"Greedy actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")\n",
        "print()\n",
        "\n",
        "num_timesteps = 0 # completly random\n",
        "print(\"Random actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5W23MnobN9x"
      },
      "source": [
        "### Q-learning select action\n",
        "\n",
        "We now have everything we need to make the `q_learning_select_action` function. We will use the `actor_state` to store a counter which keeps track of the current number of timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81TysLc0RjM6"
      },
      "outputs": [],
      "source": [
        "# Actor state stores the current number of timesteps\n",
        "QActorState = collections.namedtuple(\"ActorState\", [\"count\"])\n",
        "\n",
        "def q_learning_select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim\n",
        "    q_values = Q_NETWORK.apply(params.online, obs)[0] # remove batch dim\n",
        "\n",
        "    action = select_epsilon_greedy_action(key, q_values, actor_state.count)\n",
        "    greedy_action = select_greedy_action(q_values)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        evaluation,\n",
        "        greedy_action,\n",
        "        action\n",
        "    )\n",
        "\n",
        "    next_actor_state = QActorState(actor_state.count + 1) # increment timestep counter\n",
        "\n",
        "    return action, next_actor_state\n",
        "\n",
        "Q_LEARNING_ACTOR_STATE = QActorState(0) # counter set to zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z884-1oNRGEr"
      },
      "source": [
        "### Training\n",
        "We can now put everything together using the agent-environment loop. But first,lets jit the select action function and the learn function for some extra speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbdHDbd1RjM8"
      },
      "outputs": [],
      "source": [
        "# Jit functions\n",
        "q_learning_select_action_jit = jax.jit(q_learning_select_action)\n",
        "q_learner_step_jit = jax.jit(q_learner_step)\n",
        "\n",
        "# Initialise memory\n",
        "memory = TransitionMemory(10_000, 512) # store 10000 transitions\n",
        "\n",
        "# Run environment loop\n",
        "episode_returns, evaluator_returns = run_environment_loop(\n",
        "                                        rng, \n",
        "                                        env, \n",
        "                                        Q_LEARNING_PARAMS, \n",
        "                                        q_learning_select_action_jit, \n",
        "                                        Q_LEARNING_ACTOR_STATE,\n",
        "                                        q_learner_step_jit, \n",
        "                                        Q_LEARNING_LEARN_STATE, \n",
        "                                        memory,\n",
        "                                        num_episodes=1_001,\n",
        "                                        learn_steps_per_episode=16\n",
        "                                    )\n",
        "\n",
        "plt.plot(episode_returns)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, the approximated Q-function hopefully converged to a decent (implicit) policy for balancing the pole in the CartPole problem. \n",
        "\n",
        "This section attempts to summarise [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602), the research paper where Deep-Q Learning was first introduced. To understand the concepts covered in this section better, we recommend you give it a read."
      ],
      "metadata": {
        "id": "2k0-41wbpFDE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "In this practical we learnt the basics of reinforcement learning (RL).\n",
        "\n",
        "In the first section we learnt some basic concepts such as environment observations, action selection strategies, rewards, and episodes. We learnt about rewards and that the goal in RL is to learn a policy which maximises some notion of cummulative reward that the agent receives from the environment (return). \n",
        "\n",
        "In the second section we searched for an optimal policy in CartPole using an algorithm called RandomSearch. Basically, we tried out different policies until we happened to find one that worked well. This method did not yield consistent results and success required immense luck.\n",
        "\n",
        "In the third section we learnt about policy gradients and how we can use gradient ascent to adjust the parameters in our agents policy in the direction which maximises the expected cummulative reward (return).\n",
        "\n",
        "Finally, in the fourth section we learnt about the state-action value function and how it is related to an optimal policy. We implemented an algorithm called Q-learning to learn the optimal state-action value function in CartPole. We learnt about the importance of using a target network and epsilon-greedy exploration.\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "Now that you have successfully solved CartPole with two different RL algorithms, REINFORCE and Deep Q-Learning, we now encourage you to use what you have learnt to try and solve some more challenging environments. OpenAI Gym is a great place to find RL environments. [LunarLander](https://www.gymlibrary.ml/environments/box2d/lunar_lander/) is a great next step.\n",
        "\n",
        "In addition, there are many RL algorithms out there that make significant improvements to REINFORCE and Deep Q-Learning. See these resources:\n",
        "* [REINFORCE with baseline](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#baselines-in-policy-gradients)\n",
        "* [Double Deep Q-Network](https://arxiv.org/pdf/1509.06461.pdf)\n",
        "* [Proximal Policy Optimisation (PPO)](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "\n",
        "**Appendix:** \n",
        "\n",
        "N/a\n",
        "\n",
        "**References:** \n",
        "\n",
        "* [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)\n",
        "* [Deep Q-Network]()\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "#@title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6zu51ep7Sh0M"
      ],
      "name": "intro_to_rl.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}