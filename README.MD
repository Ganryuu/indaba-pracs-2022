# Deep Learning Indaba Practicals 2022
  
## The Practicals 
| Topic üí• | Description üìò | Colab üë©‚Äçüíª |
|---------------|----------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
[Introduction to ML using JAX](https://github.com/deep-learning-indaba/indaba-pracs-2022/blob/main/Introduction_to_ML_using_JAX.ipynb) | In this tutorial, we will learn about JAX, a new machine learning framework that has taken deep learning research by storm! JAX is praised for its speed, and we will learn how to achieve these speedups, using core concepts in JAX, such as automatic differentiation (`grad`), parallelization (`pmap`), vectorization (`vmap`), just-in-time compilation (`jit`), and more. We will then use what we have learned to implement Linear Regression effectively while learning some of the fundamentals of optimization. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/Introduction_to_ML_using_JAX.ipynb) | 
[Bayesian Deep Learning](https://github.com/deep-learning-indaba/indaba-pracs-2022/blob/main/Bayesian_Deep_Learning_Prac.ipynb) | Bayesian inference provides us with the tools to update our beliefs consistently when we observe data. Compared to the, more common, loss minimisation approach to learning, Bayesian methods offer us calibrated uncertainty estimates, resistance to overfitting, and even approaches to select hyper-parameters without a validation set. üöÄIn this prac we will learn to do all of these things!üöÄ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/Bayesian_Deep_Learning_Prac.ipynb) | 
[Transformers and Attention](https://github.com/deep-learning-indaba/indaba-pracs-2022/blob/main/Introduction_to_ML_using_JAX.ipynb) | The transformer architecture, introduced in Vaswani et al. 2017's paper [Attention is All You Need](https://arxiv.org/abs/1706.03762?amp=1), has significantly impacted the deep learning field. It has arguably become the de-facto architecture for complex Natural Language Processing (NLP) tasks. It can also be applied in various domains reaching state-of-the-art performance, including computer vision and reinforcement learning. Transformers, as the title of the original paper implies, are almost entirely based on a concept known as attention. Attention allows models to "focus" on different parts of an input; while considering the entire context of the input versus an RNN, that operates on the data sequentially. In this practical, we will introduce attention in greater detail and build the entire transformer architecture block by block to see why it is such a robust and powerful architecture | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/attention_and_transformers.ipynb) | 



This repository contains the practical notebooks for the Deep Learning Indaba
2022, held at SUP‚ÄôCOM University in Tunis, Tunisia.

See [www.deeplearningindaba.com](http://www.deeplearningindaba.com) for more details.
