{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Introduction to ML Using Jax\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png\" width=\"60%\" />\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/prac1-intro-ml-using-jax/prac1/prac_0_intro_to_jax_using_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "JAX is a language/API used for writing effective numerical transformations [[1]](https://jax.readthedocs.io/en/latest/index.html). It leverages [Autograd](https://github.com/hips/autograd) and [XLA](https://www.tensorflow.org/xla) (Accelerated Linear Algebra), to provide useful functionality such as automatic differentiation (`Grad`), parallelization (`pmap`), vectorization (`vmap`), just-in-time compilation (`JIT`) and more. \n",
        "\n",
        "JAX is **not** a replacement for Pytorch or Tensorflow, but a lower-level library that is commonly used with higher-level neural network libraries such as [Haiku](https://github.com/deepmind/dm-haiku) or [Flax](https://github.com/google/flax).  \n",
        "\n",
        "**Topics:** \n",
        "\n",
        "Content: <font color='orange'>`Supervised Learning`</font>  \n",
        "Level: <font color='grey'>`Beginner`</font>\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Learn the basics of JAX and how what differentiates it from numpy.\n",
        "- Learn how to use JAX transforms - jit, grad, vmap and pmap.  \n",
        "- Learn how build simple classifiers using jax. \n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Basic knowledge of numpy.\n",
        "\n",
        "**Outline:** \n",
        "\n",
        "[Points that link to each section.]\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell. \n",
        "#@title Install and import required packages. (Run Cell)\n",
        "%%capture\n",
        "\n",
        "# Section1\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions. (Run Cell)\n",
        "import copy\n",
        "from typing import Dict \n",
        "def plot_performance(data:Dict, title: str):\n",
        "  runs = list(data.keys())\n",
        "  time = list(data.values())\n",
        "  \n",
        "  # creating the bar plot\n",
        "  plt.bar(runs, time, width = 0.35)\n",
        "  \n",
        "  plt.xlabel(\"Framework\")\n",
        "  plt.ylabel(\"Average time taken (in s)\")\n",
        "  # plt.title(\"Average time taken per framework to run dot product\")\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "  best_perf_key = min(data, key=data.get)\n",
        "  all_runs_key = copy.copy(runs)\n",
        "\n",
        "  # all_runs_key_except_best\n",
        "  all_runs_key.remove(best_perf_key)\n",
        "\n",
        "  for k in all_runs_key:\n",
        "    print(f\"{best_perf_key} was {round((data[k]/data[best_perf_key]),2)} times faster than {k} !!!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YQe1CfDyrkdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Basics of JAX"
      ],
      "metadata": {
        "id": "Enx0WUr8tIPf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## 1.1 From Numpy ➡ Jax - <font color='blue'>`ALL`</font>\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jax and Numpy have a similiar interface 🤝\n",
        "\n"
      ],
      "metadata": {
        "id": "sMk5WFYVQ71H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot sin functions using numpy."
      ],
      "metadata": {
        "id": "KbYfoaujT2F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 100 linearly spaced numbers\n",
        "x = np.linspace(-np.pi,np.pi,100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = np.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x,y, 'b', label='y=sin(x)')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sgRLq58OTz1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now using jax.numpy - `jnp` (`import jax.numpy as jnp`)"
      ],
      "metadata": {
        "id": "XCEnlC-PU3ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 100 linearly spaced numbers\n",
        "x = jnp.linspace(-np.pi,np.pi,100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = jnp.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x,y, 'b', label='y=sin(x)')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kRQf2mNRTlt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task:** Can you plot cosine using `jnp`?"
      ],
      "metadata": {
        "id": "wuNscwHeV_dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot Cosine using jnp. (UPDATE ME)\n",
        "\n",
        "# 100 linearly spaced numbers\n",
        "x = jnp.linspace(-np.pi,np.pi,100)\n",
        "\n",
        "# UPDATE ME\n",
        "y = x  \n",
        "\n",
        "if (y == x).all():\n",
        "  raise Exception(\"Update ME!\")\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x,y, 'b', label='y=cos(x)')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5svZFPUCQNsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX arrays are immutable.\n",
        "\n",
        "JAX and Numpy arrays are often interchangeable, **but** Jax arrays are **immutable** (they can't be modified after they are created)."
      ],
      "metadata": {
        "id": "dPbOnhE4ZSTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy: mutable arrays\n",
        "x = np.arange(10)\n",
        "x[0] = 10\n",
        "print(x)"
      ],
      "metadata": {
        "id": "7r-Los6YZR-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX: immutable arrays\n",
        "x = jnp.arange(10)\n",
        "x[0] = 10"
      ],
      "metadata": {
        "id": "OxjkKpqAZxWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to use [helper functions](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html) that return an updated copy of jax array. \n",
        "\n",
        "`x[idx] = y` -> `x = x.at[idx].set(y)`"
      ],
      "metadata": {
        "id": "VoWT5RBUagW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(10)\n",
        "x = x.at[0].set(10)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "qJYxkh4qagwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Randomness in JAX.\n",
        "\n",
        "JAX is more explicit in Pseudo Random Number Generation (PRNG) and doesn't rely on a global state. "
      ],
      "metadata": {
        "id": "oAH4c_smdGQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In Numpy, PRNG is based on a global `state`."
      ],
      "metadata": {
        "id": "Q2m376Ethf8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set global state \n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "-0t3sjxzdgmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a few samples from a Gaussian Distribution:"
      ],
      "metadata": {
        "id": "nloZ9abah3J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"sample 1 = {np.random.normal()}\")\n",
        "print(f\"sample 2 = {np.random.normal()}\")\n",
        "print(f\"sample 3 = {np.random.normal()}\")"
      ],
      "metadata": {
        "id": "MaYIspBrh7tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy's global random state is updated everytime a random number is generated, so sample 1 != sample 2 != sample 3. \n",
        "\n",
        "Having the state automatically updated, makes it difficult to handle randomness in a **reproducible** way across threads, processes and devices. "
      ],
      "metadata": {
        "id": "nuHkW6V4iLa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In JAX, PRNG is more explicit.\n",
        "\n",
        "For each random number generation, you need to explicitly pass in a random key/state."
      ],
      "metadata": {
        "id": "lGDU6ckKkzqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the same state/key results in the same number being generated. This is generally undesirable."
      ],
      "metadata": {
        "id": "6oKdk5CSmD-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "print(f\"sample 2 = {random.normal(key)}\")\n",
        "print(f\"sample 3 = {random.normal(key)}\")"
      ],
      "metadata": {
        "id": "Y-6B0hjtlTmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate different and indepedent samples, you need to manually **split** the keys. "
      ],
      "metadata": {
        "id": "l0KcwEbZqIaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "\n",
        "# We split the key -> key and subkey\n",
        "key, subkey = random.split(key)\n",
        "\n",
        "# We use the subkey immediately and keep the key for future splits. It doesn't really matter which key we keep and which one we use immediately. \n",
        "print(f\"sample 2 = {random.normal(subkey)}\")\n",
        "\n",
        "# We split the new key -> key and subkey\n",
        "key, subkey = random.split(key)\n",
        "print(f\"sample 3 = {random.normal(subkey)}\")"
      ],
      "metadata": {
        "id": "v-7BhY0MmEhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we keep track of the random key/state, we can reproduce JAX random number generation in parallel across threads, processes or even devices. For more details on PRNG in JAX, you can read more [here](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html). "
      ],
      "metadata": {
        "id": "2VnTDptmuk-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Acceleration in JAX 🚀 "
      ],
      "metadata": {
        "id": "TSj972IWxTo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX is backend Agnostic - <font color='blue'>`ALL`</font>\n",
        "\n",
        "Using JAX, you can run the same code on different backends/AI accelerators (e.g. CPU/GPU/TPU), **with no changes in code** (no more `.to(device)` - from frameworks like PyTorch). This means we can easily run linear algebra operations directly on gpu/tpu."
      ],
      "metadata": {
        "id": "_bQ9QqT-yKbs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbcFsfAibBu"
      },
      "source": [
        "#### Speed Improvement - Multiplying Matrices\n",
        "\n",
        "Dot products are a common operation in numerical computing and a central part of modern deep learning. They are defined over [vectors](https://en.wikipedia.org/wiki/Coordinate_vector), which can loosely be thought of as a list of multiple scalers (single values). \n",
        "\n",
        "Formally, given two vectors $\\boldsymbol{x}$,$\\boldsymbol{y}$ $\\in R^n$, their dot product is defined as:\n",
        "\n",
        "<center>$\\boldsymbol{x}^{\\top} \\boldsymbol{y}=\\sum_{i=1}^{n} x_{i} y_{i}$</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dot Product in Numpy (will run on cpu)"
      ],
      "metadata": {
        "id": "AY1RsVkXaokP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size=1000\n",
        "x = np.random.normal(size=(size, size))\n",
        "y = np.random.normal(size=(size, size))\n",
        "numpy_time = %timeit -o -n 10 a_np = np.dot(y,x.T)"
      ],
      "metadata": {
        "id": "yj59KkD_HDOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dot Product using JAX (will run on current runtime - e.g. GPU)."
      ],
      "metadata": {
        "id": "6c_kl-u0KPVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size=1000\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key,shape=(size, size))\n",
        "y = jax.random.normal(key,shape=(size, size))\n",
        "jax_time = %timeit -o -n 10 jnp.dot(y, x.T).block_until_ready()"
      ],
      "metadata": {
        "id": "PHRcHK86KO3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How much faster was the dot product in JAX (Using GPU)?"
      ],
      "metadata": {
        "id": "S3vwh6Q724gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_average_time=np.mean(numpy_time.all_runs)\n",
        "jax_average_time=np.mean(jax_time.all_runs)\n",
        "data = {'numpy':np_average_time, 'jax':jax_average_time}\n",
        "\n",
        "plot_performance(data,title=\"Average time taken per framework to run dot product\")"
      ],
      "metadata": {
        "id": "UkASX9p34A1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOGuGWtLmP7n"
      },
      "source": [
        "### Basic JAX Transformations - `JIT` and `GRAD` - <font color='blue'>`ALL`</font>\n",
        "\n",
        "JAX transforms first convert python functions into an intermediate language called jaxpr. Transforms (e.g. jit, grad, vmap) are then applied to this jaxpr representation.\n",
        "\n",
        "JAX generates jaxpr, in a process known as *tracing*. During tracing, function inputs are wrapped by a tracer object and then JAX records all operations (including regular python code) that occur during the function call. These recorded operations are used to reconstruct the function. Any python side-effects are not recording during tracing.\n",
        "\n",
        "In this section, we will explore two basic JAX transforms: \n",
        "- JIT (Just-in-time compilation) - compile JAX Python function so that they can be run efficiently on XLA - `speed up functions`.\n",
        "- Grad - Automatically compute gradients - `automatic differentiation`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JIT\n",
        "\n",
        "Jax dispatches operations to accelerators one at a time. If we have repeated operations, we use `jit` to compile the function the first time it is called, then subsequent calls will be cached. "
      ],
      "metadata": {
        "id": "QsJE_U-ZzVol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compile [ReLU (Rectified Linear Unit)](https://arxiv.org/abs/1803.08375), a popular activation in deep learning. \n",
        "\n",
        "ReLU is defined as follows:\n",
        "<center>$f(x)=max(0,x)$</center>\n",
        "\n",
        "It can be visualized as follows:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"35%\" />\n",
        "</center>,\n",
        "\n",
        "where $x$ is the input to the function and $y$ is output of ReLU.\n"
      ],
      "metadata": {
        "id": "uIYsqIp_-Dly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$f(x)=\\max (0, x)=\\left\\{\\begin{array}{l}x_{i} \\text { if } x_{i}>0 \\\\ 0 \\text { if } x_{i}<=0\\end{array}\\right.$$"
      ],
      "metadata": {
        "id": "Vm-bN9sQETLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task:** Complete the ReLU implementation below."
      ],
      "metadata": {
        "id": "dFiuu3BFAKdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implement ReLU.\n",
        "def relu(x):\n",
        "  if x > 0:\n",
        "    return\n",
        "    # TODO Implement me! \n",
        "  else:\n",
        "    return\n",
        "    # TODO Implement me! "
      ],
      "metadata": {
        "id": "1_qMJJbs-Cbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run to test your ReLU function.\n",
        "\n",
        "def plot_relu(relu_function):\n",
        "  max_int = 5\n",
        "  # Generete 100 evenly spaced points from -max_int to max_int \n",
        "  x = np.linspace(-max_int,max_int,1000)\n",
        "  y = np.array([relu_function(xi) for xi in x])\n",
        "  plt.plot(x, y,label='ReLU')\n",
        "  plt.legend(loc=\"upper left\")\n",
        "  plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
        "  plt.show()\n",
        "\n",
        "def check_relu_function(relu_function):\n",
        "  # Generete 100 evenly spaced points from -100 to -1\n",
        "  x = np.linspace(-100,-1,100)\n",
        "  y = np.array([relu_function(xi) for xi in x])\n",
        "  assert (y == 0).all()\n",
        "\n",
        "  # Check if x == 0\n",
        "  x = 0\n",
        "  y = relu_function(x)\n",
        "  assert y == 0\n",
        "\n",
        "  # Generete 100 evenly spaced points from 0 to 100\n",
        "  x = np.linspace(0,100,100)\n",
        "  y = np.array([relu_function(xi) for xi in x])\n",
        "  assert np.allclose(x, y)\n",
        "\n",
        "  print(\"Your ReLU function is correct!\")\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ],
      "metadata": {
        "id": "zCobLakM1esy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to `jit` this function to speed up compilation and try to call it."
      ],
      "metadata": {
        "id": "2mgIAyE2Fx3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "num_random_numbers=1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "try:# Should raise an error. \n",
        "  relu_jit(x) \n",
        "except Exception as e:\n",
        "  print(\"Exception {}\".format(e))"
      ],
      "metadata": {
        "id": "4YDkiNlRF6jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why does this fail?**\n",
        "\n",
        "As mentioned above, JAX transforms first convert python functions into an intermediate language called jaxpr. Jaxpr only captures what is executed on the parameters given to it during tracing, so this means during conditional calls, jaxpr only considers the branch taken. \n",
        "\n",
        "When jit-compiling a function, we want to compile and cache a version of the function that can handle multiple different arguement types (so we don't have recompile for each function evaluation). For example, when we compile a function on an array `jnp.array([1., 2., 3.], jnp.float32)`, we would likely also want to used the compiled function for `jnp.array([4., 5., 6.], jnp.float32)`. \n",
        "\n",
        "To achieve this, JAX traces your code based on abstract values. The default abstraction level is a ShapedArray - array that has a fixed size and dtype, for example, if we trace a function using `ShapedArray((3,), jnp.float32)`,  it can be reused for any concrete array of size 3, and float32 dtype. \n",
        "\n",
        "This does come with some challenges. Tracing that relies on concrete values become tricky and sometimes results in `ConcretizationTypeError` as in the relu function above. Furtermore, when tracing function with conditional statements (\"if ...\"), JAX doesn't which branch to take when tracing and so tracing can't occur.\n",
        "\n",
        "To solve this, we have two options:\n",
        "- Use static arguements to make sure jax traces on a concrete value level - not ideal, if you need to retrace a lot. Example - bottom of this [section](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-jit).\n",
        "- Use buildin jax condition flow primitives such as [`lax.cond`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html) or [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html).  \n"
      ],
      "metadata": {
        "id": "y7q33C4pHOQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task** : Let's convert our ReLU function above to use [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) (you can also use `jnp.maximum`, if you prefer.) "
      ],
      "metadata": {
        "id": "SX8k4R7daBpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######@title Implement ReLU using jnp.where.\n",
        "def relu(x):\n",
        "  # TODO Implement ME! \n",
        "  return"
      ],
      "metadata": {
        "id": "p-4mXLwqaK-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ReLU function\n",
        "check_relu_function(relu)"
      ],
      "metadata": {
        "id": "B5fq_QRoaaG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see the performance benefit of using jit!"
      ],
      "metadata": {
        "id": "CSA0_24Nbo-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relu_jit = jax.jit(relu)\n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "num_random_numbers=1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "\n",
        "jax_time = %timeit -o -n 10 relu(x).block_until_ready()\n",
        "\n",
        "# Warm up/Compile - first run \n",
        "relu_jit(x).block_until_ready()\n",
        "jax_jit_time = %timeit -o -n 10 relu_jit(x).block_until_ready()\n",
        "\n",
        "# Let's plot the performance difference\n",
        "jax_avg_time=np.mean(jax_time.all_runs)\n",
        "jax_jit_avg_time=np.mean(jax_jit_time.all_runs)\n",
        "data = {'JAX (no jit)':jax_avg_time, 'JAX (with jit)':jax_jit_avg_time}\n",
        "\n",
        "plot_performance(data,title=\"Average time taken for ReLU function\")"
      ],
      "metadata": {
        "id": "KYogDOCLiLXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Grad"
      ],
      "metadata": {
        "id": "dxq-z-xzs40s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Advanced JAX Transforms - `VMAP` and `PMAP` - <font color='purple'>`Optional`</font> "
      ],
      "metadata": {
        "id": "vAO9dOdrtiqI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB0503xgmSFh"
      },
      "source": [
        "# Part 2 - From Linear to Non-Linear Regression\n",
        "\n",
        "[Background/content for the section.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:** \n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:** \n",
        "\n",
        "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
        "https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html\n",
        "https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "#@title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m2s4kN_QPQVe",
        "6EqhIg1odqg0",
        "Enx0WUr8tIPf",
        "-ZUp8i37dFbU",
        "sMk5WFYVQ71H",
        "dPbOnhE4ZSTi",
        "oAH4c_smdGQU",
        "Q2m376Ethf8m",
        "lGDU6ckKkzqL",
        "TSj972IWxTo2",
        "_bQ9QqT-yKbs",
        "4PbcFsfAibBu",
        "cOGuGWtLmP7n",
        "QsJE_U-ZzVol",
        "dxq-z-xzs40s",
        "vAO9dOdrtiqI",
        "aB0503xgmSFh",
        "fV3YG7QOZD-B",
        "o1ndpYE50BpG"
      ],
      "name": "prac_0_intro_to_jax_using_ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}